{"question": "ARES系统的全称是什么？", "ground_truth_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"], "ground_truth_answer": "ARES代表Automated RAG Evaluation System。", "retrieved_contexts": ["only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "erations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, Ope-\nnAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad-\nditionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-to-\nsequence model (Lewis et al., 2019). During re-\ntrieval, each RAG system only retrieves one pas-\nsage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "labeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strat-\negy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides"], "generated_answer": "ARES系统的全称在提供的上下文中没有明确提及。因此，我无法提供其完整的名称。如果你有其他相关的问题或者需要进一步的帮助，请告诉我！"}
{"question": "ARES论文的作者是谁？", "ground_truth_contexts": ["Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com"], "ground_truth_answer": "作者是Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。", "retrieved_contexts": ["only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "labeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strat-\negy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-"], "generated_answer": "根据提供的信息，ARES论文的作者包括：\n\n- Jeff Johnson\n- Matthijs Douze\n- Hervé Jégou\n\n这些作者在2019年发表了一篇相关的论文《Billion-scale similarity search with GPUs》，该论文被引用在与ARES相关的工作中。然而，请注意这可能不是完整的作者列表，因为这里只提到了与特定引用相关的一部分作者。如果你需要完整的信息，建议查阅原始的ARES论文或其官方发布的信息。"}
{"question": "ARES报告的三个评估分数是什么？", "ground_truth_contexts": ["ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response generated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question)."], "ground_truth_answer": "三个评估分数是上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "erations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, Ope-\nnAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad-\nditionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-to-\nsequence model (Lewis et al., 2019). During re-\ntrieval, each RAG system only retrieves one pas-\nsage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled"], "generated_answer": "ARES报告的三个评估分数是：\n\n1. **Context Relevance（上下文相关性）**：评估检索到的信息是否与测试问题相关。\n2. **Answer Faithfulness（答案忠实度）**：评估语言模型生成的响应是否正确基于检索到的上下文。\n3. **Answer Relevance（答案相关性）**：评估生成的答案是否与问题相关。"}
{"question": "ARES如何生成用于训练判断器的合成数据？", "ground_truth_contexts": ["Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus.", "We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query–passage–answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers)."], "ground_truth_answer": "ARES使用像FLAN-T5 XXL这样的语言模型从领域内段落生成合成问题-答案对，包括正面和负面示例，并使用检索器过滤低质量查询。", "retrieved_contexts": ["only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "labeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strat-\negy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "sequence is greater than or equal to the later value\nin the sequence. A Kendall’s τ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document prepro-\ncessing. We want to make sure that ARES has satis-\nfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall’s τ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pair-\nwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the"], "generated_answer": "ARES通过使用FLAN-T5模型生成合成查询和答案来创建其自己的合成训练数据。具体来说，它采用以下步骤：\n\n1. **提示结构**：为了生成合成问题和答案，ARES使用了一个特定的提示结构，并提供了5个少样本示例（few-shot examples）。\n   \n2. **生成正确与错误答案**：对于生成不正确或矛盾的答案，ARES简单地将少样本示例替换为不正确或矛盾的内容。\n\n3. **生成合成问题**：在生成合成问题时，ARES使用了针对FLAN-T5 XXL模型的特定提示，并同样提供了一些示例。\n\n通过这种方式，ARES能够自动生成大量用于训练轻量级语言模型判断器的数据，从而评估RAG系统的上下文相关性、回答忠实性和回答相关性等维度的表现。"}
{"question": "预测驱动推理（PPI）在ARES中的作用是什么？", "ground_truth_contexts": ["Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals for RAG scoring.", "PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our human preference validation set."], "ground_truth_answer": "PPI通过利用一小部分人工标注数据点来提高基于模型的评估准确性，为分数提供置信区间，结合标注和未标注预测。", "retrieved_contexts": ["the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-"], "generated_answer": "在ARES中，预测驱动推理（PPI）的作用是提升细调后的LLM judge的排名预测准确性。通过利用PPI，ARES能够更有效地评估和排名不同的RAG系统，即使在数据标注有限的情况下也能保持较高的准确性和数据效率。具体来说，PPI帮助ARES在进行评估时，更好地结合模型的预测与实际的人类偏好验证集，从而提高评分的可靠性和有效性。"}
{"question": "ARES在评估过程中通常需要多少人工标注？", "ground_truth_contexts": ["ARES significantly improves data efficiency during evaluation by only requiring three inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more)."], "ground_truth_answer": "ARES需要大约150个或更多人工标注数据点在人类偏好验证集中。", "retrieved_contexts": ["only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "triples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative\nexamples of query–passage–answer triples (e.g.,\nrelevant/irrelevant passages and correct/incorrect\nanswers). For generation, the LLM uses our in-\nput set of few-shot examples with in-domain pas-\nsages mapped to in-domain queries and answers;\nthe model then generates a synthetic question and", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "sequence is greater than or equal to the later value\nin the sequence. A Kendall’s τ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document prepro-\ncessing. We want to make sure that ARES has satis-\nfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall’s τ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pair-\nwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-"], "generated_answer": "根据提供的文档内容，ARES在评估过程中使用的人工标注数量相对较少。具体来说：\n\n- ARES使用了一个由大约150个或更多人工标注的数据点组成的人类偏好验证集（human preference validation set），这些数据点用于评估模型改进情况，并且用来计算其置信区间。\n- 在与传统采样注释方法的比较中，即使传统方法对每个模拟RAG系统采样了150个数据点，总计1,350个注释，ARES仍然能够以更少的注释量（减少了78%）达到更高的准确性。\n\n因此，ARES通常需要约150个人工标注的数据点来指导其评估过程中的预测和排名。这表明ARES在利用人类反馈方面是高效且精确的。"}
{"question": "ARES中使用哪些模型进行合成数据生成和判断器微调？", "ground_truth_contexts": ["For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge."], "ground_truth_answer": "FLAN-T5 XXL用于合成数据生成，DeBERTa-v3-Large用于微调LLM判断器。", "retrieved_contexts": ["erations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, Ope-\nnAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad-\nditionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-to-\nsequence model (Lewis et al., 2019). During re-\ntrieval, each RAG system only retrieves one pas-\nsage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "In Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-\npared to RAGAS, ARES is 0.16 higher for context\nrelevance and 0.15 higher for answer relevance, on\naverage. ARES also provided accurate confidence\nbounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-", "labeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strat-\negy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides"], "generated_answer": "在ARES框架中，合成数据生成和判断器微调使用的模型如下：\n\n### 合成数据生成：\n- **FLAN-T5 XXL**：用于生成合成查询和答案。通过提供少量示例（few-shot examples），该模型能够根据给定的文档生成相关的问题和答案。\n\n### 判断器微调：\n- **LLM（大型语言模型）**：针对三个不同的评估指标（上下文相关性、答案忠实性和答案相关性），分别对一个带有二分类头的LLM进行微调。这些模型被训练来区分正例和负例样本。\n  - 具体来说，在实验部分提到了使用了以下模型作为基础模型之一：\n    - **MPT-7b-Instruct** (Team, 2023)\n    - **GPT-3.5-Turbo**\n    - **GPT-4**\n\n此外，还提到使用了**Facebook RAG模型**(Lewis et al., 2020)，它结合了DPR检索器(Karpukhin et al., 2020)和BART序列到序列模型(Lewis et al., 2019)。虽然这不是直接用于合成数据生成或判断器微调的部分，但它是与ARES比较的基线系统之一。\n\n综上所述，合成数据生成主要依赖于FLAN-T5 XXL，而判断器则是通过对不同大型语言模型进行微调得到的。"}
{"question": "总结ARES管道的三个阶段。", "ground_truth_contexts": ["ARES proceeds in three stages (Figure 1). ... First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus. Second, it defines three separate judge models to perform three classification tasks... Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI)."], "ground_truth_answer": "阶段1：从语料段落生成合成查询和答案。阶段2：为上下文相关性、答案忠实度和答案相关性微调LLM判断器。阶段3：使用判断器和PPI为RAG系统评分以获得置信区间。", "retrieved_contexts": ["quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "ever, while the fine-tuned LLM judge had a higher\nKendall’s tau on average, the GPT-3.5 judge is\nmore readily deployable and does not require any\nadditional fine-tuning. The GPT-3.5 judge does\ncome with its own querying costs, which can vary\nbased on the date of querying as well as the total\ntokens used in evaluation.\nWe also wanted to better understand the impor-\ntance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "compared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the hu-\nman preference validation set with the much larger\nset of datapoints with ML predictions, PPI can de-\nvelop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confi-\ndence bounds for the success and failure rates of the\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (proba-\nbility) for our confidence interval.\nWith the accuracy confidence interval for each", "after changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction\nRetrieval-augmented generation (RAG) has be-\ncome a prominent approach for building user-\nfacing NLP applications, such as systems for ques-\ntion answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to gener-\nate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan", "Shyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262.\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Relevance-guided supervision for openqa with\ncolbert. Transactions of the association for computa-\ntional linguistics, 9:929–944.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit", "PPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-\nGLUE, LLM judges are unable to generalize when\nmaking more drastic shifts in domain, such as:\nswitching languages (e.g. English to Spanish, Ger-\nman, and other languages), switching from text to\ncode (e.g. questions + passages to coding functions\n+ documentation), and switching from retrieving\ntext to extraction of entities, webpages, or citations.\nTo test cross-lingual transfer, we used the\nXGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "we use a single linear classification layer and ap-\nply a 0.1 dropout to the input, which is the final\nhidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score context relevance:\n• Given the following question and document,\nyou must analyze the provided document and\ndetermine whether it is sufficient for answer-\ning the question. In your evaluation, you\nshould consider the content of the document\nand how it relates to the provided question.", "the correlation between a perfectly accurate pair-\nwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in\ninformation retrieval, allowing developers to eval-\nuate ranking systems empirically. Therefore, we\nbelieve Kendall’s tau and prediction accuracy pro-\nvide meaningful metrics for testing the efficacy of\nARES as a RAG evaluation system.\n5 Results & Analysis\n5.1 ARES Ranking\nTable 1 summarizes our main evaluation of ARES\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges). We compare against RAGAS (ver-\nsion 0.0.18) and a baseline few-shot prompted GPT-\n3.5 judge ( gpt-3.5-turbo-16k). For the few-shot\nGPT-3.5 judge, we provide few-shot examples for\nguiding predictions; the prompts are included in", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "from GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "logits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n7 Limitations\nARES relies on a small set of annotations in the\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\noften require an annotator familiar with the RAG\nsystem’s domain application. While these annota-\ntions can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,"], "generated_answer": "ARES管道的三个阶段如下：\n\n1. **合成数据生成**：ARES首先创建自己的合成训练数据，用于微调轻量级语言模型（LM）法官，以评估RAG组件的质量。\n\n2. **微调LM法官**：利用生成的合成数据，ARES对LM法官进行微调，使其能够准确评估RAG系统的上下文相关性、答案忠实性和答案相关性。\n\n3. **预测驱动推理（PPI）校正**：为了提高评估的准确性，ARES使用一小部分人工标注的数据点进行预测驱动推理（PPI），从而为RAG系统的性能提供置信区间，并估计LM法官的错误。"}
{"question": "比较ARES和RAGAS在上下文相关性和答案相关性方面的性能。", "ground_truth_contexts": ["ARES averages a Kendall’s τ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS. ... For context relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS while for answer relevance, our system is 14.4 percentage points higher than RAGAS."], "ground_truth_answer": "ARES在Kendall’s τ上优于RAGAS（上下文相关性0.065，答案相关性0.132），并在准确性上更高（上下文相关性59.9%点，答案相关性14.4%点）。", "retrieved_contexts": ["quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "logits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n7 Limitations\nARES relies on a small set of annotations in the\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\noften require an annotator familiar with the RAG\nsystem’s domain application. While these annota-\ntions can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "from GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "ity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets\nOur core experimental goal is to provide a rich\npicture of where ARES can be applied effectively.\nTo test across multiple types of queries, documents,\nand answers, we selected all the datasets from the\nwidely-used KILT and SuperGLUE benchmarks\nfor which RAG is appropriate.\nFrom KILT (Petroni et al., 2021), we use Natural\nQuestions (NQ), HotpotQA, FEVER, and Wizards", "after changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction\nRetrieval-augmented generation (RAG) has be-\ncome a prominent approach for building user-\nfacing NLP applications, such as systems for ques-\ntion answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to gener-\nate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how", "PPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-\nGLUE, LLM judges are unable to generalize when\nmaking more drastic shifts in domain, such as:\nswitching languages (e.g. English to Spanish, Ger-\nman, and other languages), switching from text to\ncode (e.g. questions + passages to coding functions\n+ documentation), and switching from retrieving\ntext to extraction of entities, webpages, or citations.\nTo test cross-lingual transfer, we used the\nXGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan"], "generated_answer": "根据提供的信息，ARES 在评估检索增强生成（RAG）系统方面优于现有的 RAGAS 自动评估框架。具体来说：\n\n1. **上下文相关性（Context Relevance）** 和 **答案相关性（Answer Relevance）**：\n   - ARES 能够准确地对 RAG 系统进行评分和排序，特别是在上下文相关性、答案忠实度以及答案相关性方面。\n   - 相比之下，RAGAS 框架在这些任务上的准确性较低。\n\n2. **准确性对比**：\n   - 表5展示了在真实世界 RAG 系统的排名中，ARES 在多个数据集（如 NQ、WoW、FEVER）上表现出更高的准确性。例如，在 NQ 数据集中，ARES 的上下文相关性和答案相关性的准确率分别为 85.6% 和 93.3%，而 RAGAS 的准确率则为 35.9% 和 68.2%。\n   - 类似地，在其他数据集中，ARES 的表现也显著优于 RAGAS。\n\n总结而言，ARES 在上下文相关性和答案相关性方面的性能明显优于 RAGAS。"}
{"question": "用于评估ARES的KILT和SuperGLUE数据集有哪些？", "ground_truth_contexts": ["From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW)... From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD."], "ground_truth_answer": "KILT数据集：Natural Questions (NQ)、HotpotQA、FEVER、Wizards of Wikipedia (WoW)。SuperGLUE数据集：MultiRC和ReCoRD。", "retrieved_contexts": ["the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run.\nFor PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation\nset. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nused ARES with human annotation sets ranging\nin size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).\n5.2 ARES Performance on AIS\nWoW CNN / DM\nARES Split Prediction 0.478 0.835\nCorrect Positive/Negative Split 0.458 0.859\nARES Judge Accuracy 62.5% 84.0%", "RAGAS Accuracy 31.4% 71.2% 17.2% 76.0% 36.4% 77.8% 23.7% 69.2% 16.1% 75.0% 15.0% 72.8%\nGPT-3.5 Judge Accuracy 73.8% 95.5% 75.3% 71.6% 84.3% 85.2% 60.4% 59.6% 72.4% 60.3% 81.0% 65.8%\nARES Accuracy 79.3% 97.2% 92.3% 81.3% 85.7% 96.1% 88.4% 78.5% 85.8% 82.7% 67.8% 92.3%\nTable 1: ARES Ranking with Fine-tuned LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge:\nFor scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES\nwith our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.\nFor our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels", "Accuracy on\nRAG Evaluation Sets 79.3% 96.7% 88.4% 78.3% 85.8% 82.5%\nTable 4: GPT-4 Labels vs. Human Labels : We wanted to explore the practicality of using GPT-4 generated\nlabels instead of human annotations for our human preference validation set in ARES. In the experiments, we\ngenerated 500 GPT-4 labels as replacements for human labeling using few-shot prompts (see Sections A.2, A.3,\nand A.4). While GPT-4 generated labels decreased Kendall’s tau in most settings by 0.05 to 0.30, the ability to\ncheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of\nannotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we"], "generated_answer": "根据提供的上下文，ARES在以下数据集上进行了评估：\n\n1. **KILT**：这是一个用于知识密集型语言任务的基准数据集。\n2. **SuperGLUE**：这是另一个被提及的知识密集型任务数据集。\n\n此外，还提到了**AIS**（Automated Information Selection）数据集，它也被用于ARES的评估中。 \n\n因此，在KILT、SuperGLUE和AIS数据集上对ARES进行了评估。 文中没有提到这些数据集更具体的子集或版本。"}
{"question": "ARES如何处理合成数据生成中的负面示例？", "ground_truth_contexts": ["To generate negatives for fine-tuning our LLM judges, we rely on two novel strategies... 1. Weak Negative Generation: ... 2. Strong Negative Generation: ..."], "ground_truth_answer": "ARES使用弱负面（随机采样）和强负面（从相似段落采样或生成矛盾答案）用于上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "erations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, Ope-\nnAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad-\nditionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-to-\nsequence model (Lewis et al., 2019). During re-\ntrieval, each RAG system only retrieves one pas-\nsage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "sequence is greater than or equal to the later value\nin the sequence. A Kendall’s τ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document prepro-\ncessing. We want to make sure that ARES has satis-\nfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall’s τ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pair-\nwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial"], "generated_answer": "在ARES框架中，合成数据生成的负面示例是通过以下方式处理的：\n\n1. **选择负面查询-段落匹配**：对于上下文相关性（context relevance）的评估，系统会选择一些不相关的查询和段落对作为负面示例。\n\n2. **选择负面查询-段落-答案匹配**：对于答案相关性（answer relevance）的评估，系统会构造包含不相关或错误答案的查询、段落和答案三元组作为负面示例。\n\n这些负面示例与正面示例一起用于训练和微调专门的LLM判断模型，以识别和评分RAG系统的性能。具体来说，在每个评估指标上，都会使用一个带有二分类头的独立LLM进行微调，以便能够区分正例和负例。这种做法有助于确保ARES能够在细微准确度差异的情况下有效指导RAG系统的发展和比较。"}
{"question": "ARES实验中使用的置信区间概率是多少？", "ground_truth_contexts": ["Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (probability) for our confidence interval."], "ground_truth_answer": "ARES在其实验中使用95%的置信区间概率。", "retrieved_contexts": ["the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "triples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative\nexamples of query–passage–answer triples (e.g.,\nrelevant/irrelevant passages and correct/incorrect\nanswers). For generation, the LLM uses our in-\nput set of few-shot examples with in-domain pas-\nsages mapped to in-domain queries and answers;\nthe model then generates a synthetic question and", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "sequence is greater than or equal to the later value\nin the sequence. A Kendall’s τ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document prepro-\ncessing. We want to make sure that ARES has satis-\nfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall’s τ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pair-\nwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in"], "generated_answer": "[[No]]"}
{"question": "分析ARES在跨域应用中的优势。", "ground_truth_contexts": ["In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross-domain applications. Across all settings, we found that LLM judges in ARES had strong generalizability... PPI helped mitigate the loss in accuracy and still allow ARES to be successful."], "ground_truth_answer": "ARES判断器在KILT和SuperGLUE的查询和文档类型上显示出强大的泛化能力，PPI缓解准确性损失，但在语言、代码或实体提取等剧烈转变中失败。", "retrieved_contexts": ["Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "we use a single linear classification layer and ap-\nply a 0.1 dropout to the input, which is the final\nhidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score context relevance:\n• Given the following question and document,\nyou must analyze the provided document and\ndetermine whether it is sufficient for answer-\ning the question. In your evaluation, you\nshould consider the content of the document\nand how it relates to the provided question.", "RAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.\n3.3 Ranking RAG Systems with Confidence\nIntervals\nOnce we have prepared our LLM judges, we need\nto use them to score and rank the competing RAG\nsystems. To do this, ARES samples the in-domain\nquery-document-answer triples produced by each\nRAG approach, and the judges label each triple,\npredicting their context relevance, answer faithful-\nness, and answer relevance. By averaging the in-\ndividual predicted labels for each in-domain triple,\nwe calculate the RAG system performance across\neach of the three metrics.\nIn principle, we could simply report these aver-\nage scores as quality metrics for each RAG system.\nHowever, these scores reflect entirely unlabeled", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)", "Kendall’s Tau by Dataset\nNQ MultiRC ReCoRD\nPPI Labeled\nCount C.R. A.R. C.R. A.R. C.R. A.R.\n400 1.0 1.0 0.89 0.94 0.89 0.94\n300 0.89 1.0 0.94 0.89 0.83 0.89\n200 0.83 1.0 0.83 0.94 0.83 0.83\n150 0.72 1.0 0.83 0.89 0.72 0.83\n100 0.44 1.0 0.67 0.67 0.67 0.83\n50 0.44 0.94 0.61 0.44 0.56 0.67\n25 0.44 0.89 0.56 0.44 0.44 0.56\nTable 3: Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall’s Tau: The Kendall’s tau values represent\nthe correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same\nexperimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human\npreference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan", "Shyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262.\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Relevance-guided supervision for openqa with\ncolbert. Transactions of the association for computa-\ntional linguistics, 9:929–944.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit", "from GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos", "after changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction\nRetrieval-augmented generation (RAG) has be-\ncome a prominent approach for building user-\nfacing NLP applications, such as systems for ques-\ntion answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to gener-\nate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how", "compared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the hu-\nman preference validation set with the much larger\nset of datapoints with ML predictions, PPI can de-\nvelop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confi-\ndence bounds for the success and failure rates of the\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (proba-\nbility) for our confidence interval.\nWith the accuracy confidence interval for each"], "generated_answer": "ARES（Automated RAG Evaluation System）在跨域应用中具有显著的优势，主要体现在以下几个方面：\n\n1. **适应性强**：ARES能够为RAG管道的每个组件生成定制化的大型语言模型（LLM）评委，这使得它可以根据不同的评估场景调整评判标准，从而提高评估的准确性和灵活性。\n\n2. **准确性高**：与现有的RAG评估系统相比，如RAGAS，ARES通过利用预测驱动推理（PPI）技术，在少量人工标注数据的情况下依然能保持较高的评估精度。这意味着即使是在新领域或新任务上，ARES也能提供可靠的评估结果。\n\n3. **提供置信区间**：ARES不仅报告平均得分作为质量指标，还使用PPI来估计错误率并生成置信边界，这对理解RAG系统的性能非常有帮助。这种能力对于需要量化不确定性的实际应用场景尤为重要。\n\n4. **有效应对领域转移**：研究表明，ARES评委在面对不同类型的查询和/或文档时仍然表现良好，证明了其在领域转移情况下的鲁棒性。也就是说，当从一个领域的训练数据转移到另一个领域进行测试时，ARES依旧可以维持良好的评价效果。\n\n5. **资源效率**：尽管开发过程中可能需要用到GPU等计算资源，但一旦训练完成，轻量级的语言模型就可以用于快速而有效地对RAG系统进行评分和排名，这对于资源有限的研究者来说是一个重要的优势。\n\n综上所述，ARES凭借其强大的自适应能力、高度准确性以及对领域变化的良好泛化能力，在跨域应用中展现出了明显优于传统方法的表现。"}
{"question": "论文中提到的ARES的局限性是什么？", "ground_truth_contexts": ["ARES relies on a small set of annotations... These annotations often require an annotator familiar with the RAG system’s domain... The LLMs used in ARES benefit substantially from GPU-based hardware... Additionally, all of the datasets used in our evaluation of ARES are in English..."], "ground_truth_answer": "局限性包括需要领域专家标注、需要GPU硬件，并且仅在英文数据集上评估。", "retrieved_contexts": ["only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "erations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, Ope-\nnAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad-\nditionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-to-\nsequence model (Lewis et al., 2019). During re-\ntrieval, each RAG system only retrieves one pas-\nsage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "sequence is greater than or equal to the later value\nin the sequence. A Kendall’s τ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document prepro-\ncessing. We want to make sure that ARES has satis-\nfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall’s τ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pair-\nwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in", "labeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strat-\negy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,"], "generated_answer": "根据论文内容，ARES的局限性主要体现在以下几个方面：\n\n1. **依赖于高质量的训练数据**：虽然ARES通过少量示例和领域内查询答案对进行评估，但其性能仍然依赖于这些数据的质量。如果训练数据不充分或有偏差，可能会影响模型判断的准确性。\n\n2. **跨域应用时的表现波动**：尽管实验结果显示ARES在跨域任务中具有较强的泛化能力（如从NQ到FEVER、MultiRC等），但在某些特定情况下，当查询类型、文档类型或者两者同时发生变化时，LLM judge的准确率可能会有所下降。例如，在将NQ上的训练模型应用于其他数据集时，尽管使用了PPI缓解损失，但仍存在一定程度的性能下降。\n\n3. **需要人工验证集来调整与校准**：为了确保评估的有效性和可靠性，ARES要求提供一个由大约150个以上标注数据点组成的人类偏好验证集。这虽然比传统方法减少了大量标注工作量，但对于一些资源稀缺或专业性强的领域来说，获取这样的验证集仍可能存在一定难度。\n\n4. **对极端情况处理有限**：由于ARES是基于现有RAG系统生成的数据进行评分排序，并且通过模拟创建正负样本进行训练，因此它可能无法完全覆盖所有实际应用场景中的边缘案例或异常输出，特别是在面对非常规输入时可能出现误判。\n\n综上所述，尽管ARES相比以往的方法有了显著改进，但它依然受到训练数据质量、跨域适应性以及所需验证集等因素的影响。"}
{"question": "ARES在AIS基准上的答案忠实度表现如何？", "ground_truth_contexts": ["Table 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores."], "ground_truth_answer": "ARES在AIS基准上对答案幻觉的预测在真实平均值的2.5个百分点以内。", "retrieved_contexts": ["only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "labeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strat-\negy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "sequence is greater than or equal to the later value\nin the sequence. A Kendall’s τ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document prepro-\ncessing. We want to make sure that ARES has satis-\nfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall’s τ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pair-\nwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in"], "generated_answer": "根据提供的上下文，没有直接提到ARES在AIS基准上的答案忠实度表现。文中提到了ARES在多个数据集和任务上的表现，例如在KILT和SuperGLUE数据集上对上下文相关性和答案相关性的评估准确性，以及与RAGAS等现有自动化评估方法的比较。然而，并未具体提及AIS基准或其在该基准上的答案忠实度结果。\n\n如果您需要了解ARES在特定基准（如AIS）上的详细表现，可能需要查阅论文全文或其他补充材料。"}
{"question": "真实RAG系统评估中测试了哪些检索器和生成LLM？", "ground_truth_contexts": ["For our RAG systems, we selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search, and ColBERTv2... and three different generative LLMs (MPT-7b-Instruct, GPT-3.5-Turbo, and GPT-4)."], "ground_truth_answer": "检索器：BM25、OpenAI Ada嵌入、ColBERTv2。生成LLM：MPT-7b-Instruct、GPT-3.5-Turbo、GPT-4。", "retrieved_contexts": ["Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "from GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "ity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets\nOur core experimental goal is to provide a rich\npicture of where ARES can be applied effectively.\nTo test across multiple types of queries, documents,\nand answers, we selected all the datasets from the\nwidely-used KILT and SuperGLUE benchmarks\nfor which RAG is appropriate.\nFrom KILT (Petroni et al., 2021), we use Natural\nQuestions (NQ), HotpotQA, FEVER, and Wizards", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan", "after changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction\nRetrieval-augmented generation (RAG) has be-\ncome a prominent approach for building user-\nfacing NLP applications, such as systems for ques-\ntion answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to gener-\nate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "Shyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262.\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Relevance-guided supervision for openqa with\ncolbert. Transactions of the association for computa-\ntional linguistics, 9:929–944.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "RAGAS Accuracy 31.4% 71.2% 17.2% 76.0% 36.4% 77.8% 23.7% 69.2% 16.1% 75.0% 15.0% 72.8%\nGPT-3.5 Judge Accuracy 73.8% 95.5% 75.3% 71.6% 84.3% 85.2% 60.4% 59.6% 72.4% 60.3% 81.0% 65.8%\nARES Accuracy 79.3% 97.2% 92.3% 81.3% 85.7% 96.1% 88.4% 78.5% 85.8% 82.7% 67.8% 92.3%\nTable 1: ARES Ranking with Fine-tuned LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge:\nFor scoring context relevance and answer relevance (C.R. and A.R. in the table, respectively), we compare ARES\nwith our fine-tuned LLM judges against sampled annotations benchmark, RAGAS, and a few-shot GPT-3.5 judge.\nFor our sampled annotations, we gather 150 annotated datapoints from each mock RAG system and use those labels", "the correlation between a perfectly accurate pair-\nwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in\ninformation retrieval, allowing developers to eval-\nuate ranking systems empirically. Therefore, we\nbelieve Kendall’s tau and prediction accuracy pro-\nvide meaningful metrics for testing the efficacy of\nARES as a RAG evaluation system.\n5 Results & Analysis\n5.1 ARES Ranking\nTable 1 summarizes our main evaluation of ARES\n(with DeBERTa-v3-Large as the pretrained basis\nfor the judges). We compare against RAGAS (ver-\nsion 0.0.18) and a baseline few-shot prompted GPT-\n3.5 judge ( gpt-3.5-turbo-16k). For the few-shot\nGPT-3.5 judge, we provide few-shot examples for\nguiding predictions; the prompts are included in", "ever, while the fine-tuned LLM judge had a higher\nKendall’s tau on average, the GPT-3.5 judge is\nmore readily deployable and does not require any\nadditional fine-tuning. The GPT-3.5 judge does\ncome with its own querying costs, which can vary\nbased on the date of querying as well as the total\ntokens used in evaluation.\nWe also wanted to better understand the impor-\ntance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Accuracy on\nRAG Evaluation Sets 79.3% 96.7% 88.4% 78.3% 85.8% 82.5%\nTable 4: GPT-4 Labels vs. Human Labels : We wanted to explore the practicality of using GPT-4 generated\nlabels instead of human annotations for our human preference validation set in ARES. In the experiments, we\ngenerated 500 GPT-4 labels as replacements for human labeling using few-shot prompts (see Sections A.2, A.3,\nand A.4). While GPT-4 generated labels decreased Kendall’s tau in most settings by 0.05 to 0.30, the ability to\ncheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of\nannotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled"], "generated_answer": "根据提供的上下文信息，文中并没有具体提及在真实RAG系统评估中使用了哪些特定的检索器和生成LLM（大语言模型）。文中主要讨论了ARES框架如何用于评估RAG系统的不同组件，包括上下文相关性、答案忠实性和答案相关性的评分，并比较了ARES与RAGAS及GPT-3.5 Judge的效果，但并未详细列出所使用的具体检索器和生成模型。\n\n如果需要了解具体的检索器和生成LLM，可能需要查阅论文中的实验部分或附录，或者查看作者公开的代码和数据集以获取更详细的信息。"}
{"question": "总结ARES在伪RAG系统排名上的结果。", "ground_truth_contexts": ["Table 1: ... Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets."], "ground_truth_answer": "ARES在Kendall’s τ上高于RAGAS（上下文相关性0.065，答案相关性0.132），并在使用78%更少标注的情况下优于采样标注。", "retrieved_contexts": ["the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "preprint arXiv:2302.04166.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\nElkind, and Idan Szpektor. 2023. Trueteacher: Learn-\ning factual consistency evaluation with large lan-\nguage models.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nJeremy Howard and Sebastian Ruder. 2018. Universal", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "compared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the hu-\nman preference validation set with the much larger\nset of datapoints with ML predictions, PPI can de-\nvelop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confi-\ndence bounds for the success and failure rates of the\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (proba-\nbility) for our confidence interval.\nWith the accuracy confidence interval for each", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "triples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative\nexamples of query–passage–answer triples (e.g.,\nrelevant/irrelevant passages and correct/incorrect\nanswers). For generation, the LLM uses our in-\nput set of few-shot examples with in-domain pas-\nsages mapped to in-domain queries and answers;\nthe model then generates a synthetic question and", "RAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.\n3.3 Ranking RAG Systems with Confidence\nIntervals\nOnce we have prepared our LLM judges, we need\nto use them to score and rank the competing RAG\nsystems. To do this, ARES samples the in-domain\nquery-document-answer triples produced by each\nRAG approach, and the judges label each triple,\npredicting their context relevance, answer faithful-\nness, and answer relevance. By averaging the in-\ndividual predicted labels for each in-domain triple,\nwe calculate the RAG system performance across\neach of the three metrics.\nIn principle, we could simply report these aver-\nage scores as quality metrics for each RAG system.\nHowever, these scores reflect entirely unlabeled", "Kendall’s Tau by Dataset\nNQ MultiRC ReCoRD\nPPI Labeled\nCount C.R. A.R. C.R. A.R. C.R. A.R.\n400 1.0 1.0 0.89 0.94 0.89 0.94\n300 0.89 1.0 0.94 0.89 0.83 0.89\n200 0.83 1.0 0.83 0.94 0.83 0.83\n150 0.72 1.0 0.83 0.89 0.72 0.83\n100 0.44 1.0 0.67 0.67 0.67 0.83\n50 0.44 0.94 0.61 0.44 0.56 0.67\n25 0.44 0.89 0.56 0.44 0.44 0.56\nTable 3: Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall’s Tau: The Kendall’s tau values represent\nthe correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same\nexperimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human\npreference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,"], "generated_answer": "根据提供的文档内容，ARES在伪RAG系统排名上的结果表现优异。具体来说，在不同的数据集（如NQ、MultiRC和ReCoRD）上，当使用不同数量的人类偏好验证集样本时，ARES能够通过Kendall's Tau值来衡量其排名与正确排名之间的相关性。\n\n从表3中可以看出，随着人类偏好验证集中标记数据点的增加，ARES的有效性也相应提高。例如，在拥有400个标记数据点的情况下，对于所有三个数据集，ARES在上下文相关性和答案相关性方面都达到了1.0的完美Kendall's Tau值。即使是在只有25个标记数据点的小样本情况下，ARES仍然能够在某些数据集上保持较高的准确性，尽管有所下降。\n\n总体而言，这些结果显示了ARES框架不仅能够准确地评估和排名RAG系统，而且即使在少量人工标注的数据下也能维持良好的性能，证明了其高效性和实用性。此外，它还表明ARES可以为RAG系统的评价提供可靠的置信区间，这有助于理解模型性能的真实范围。[[Yes]]"}
{"question": "ARES中有效PPI所需的最少人工标注数量是多少？", "ground_truth_contexts": ["We used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3)."], "ground_truth_answer": "ARES中有效PPI至少需要150个人工标注。", "retrieved_contexts": ["the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,", "ever, while the fine-tuned LLM judge had a higher\nKendall’s tau on average, the GPT-3.5 judge is\nmore readily deployable and does not require any\nadditional fine-tuning. The GPT-3.5 judge does\ncome with its own querying costs, which can vary\nbased on the date of querying as well as the total\ntokens used in evaluation.\nWe also wanted to better understand the impor-\ntance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run.\nFor PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation\nset. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nused ARES with human annotation sets ranging\nin size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).\n5.2 ARES Performance on AIS\nWoW CNN / DM\nARES Split Prediction 0.478 0.835\nCorrect Positive/Negative Split 0.458 0.859\nARES Judge Accuracy 62.5% 84.0%", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan", "we use a single linear classification layer and ap-\nply a 0.1 dropout to the input, which is the final\nhidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score context relevance:\n• Given the following question and document,\nyou must analyze the provided document and\ndetermine whether it is sufficient for answer-\ning the question. In your evaluation, you\nshould consider the content of the document\nand how it relates to the provided question.", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)"], "generated_answer": "根据提供的文档内容，ARES中有效PPI（Prediction-powered inference）所需的最少人工标注数量是150个。文中提到：“we conducted two sets of experiments. First, we used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3).”\n\n因此，答案是：**150**。"}
{"question": "ARES是否支持非英语语言的评估？", "ground_truth_contexts": ["While LLM judges in ARES were successful in cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages...", "To test cross-lingual transfer, we used the XGLUE datasets... a LLM judge fine-tuned on NQ achieved a Kendall’s tau of 0.33..."], "ground_truth_answer": "不，ARES无法很好地泛化到非英语语言，在XGLUE上Kendall’s τ较低（例如0.33）。", "retrieved_contexts": ["the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "Accuracy on\nRAG Evaluation Sets 79.3% 96.7% 88.4% 78.3% 85.8% 82.5%\nTable 4: GPT-4 Labels vs. Human Labels : We wanted to explore the practicality of using GPT-4 generated\nlabels instead of human annotations for our human preference validation set in ARES. In the experiments, we\ngenerated 500 GPT-4 labels as replacements for human labeling using few-shot prompts (see Sections A.2, A.3,\nand A.4). While GPT-4 generated labels decreased Kendall’s tau in most settings by 0.05 to 0.30, the ability to\ncheaply produce GPT-4 generated labels significantly reduces the cost of annotation, cutting it from hundreds of\nannotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we", "triples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative\nexamples of query–passage–answer triples (e.g.,\nrelevant/irrelevant passages and correct/incorrect\nanswers). For generation, the LLM uses our in-\nput set of few-shot examples with in-domain pas-\nsages mapped to in-domain queries and answers;\nthe model then generates a synthetic question and", "Kendall’s Tau by Dataset\nNQ MultiRC ReCoRD\nPPI Labeled\nCount C.R. A.R. C.R. A.R. C.R. A.R.\n400 1.0 1.0 0.89 0.94 0.89 0.94\n300 0.89 1.0 0.94 0.89 0.83 0.89\n200 0.83 1.0 0.83 0.94 0.83 0.83\n150 0.72 1.0 0.83 0.89 0.72 0.83\n100 0.44 1.0 0.67 0.67 0.67 0.83\n50 0.44 0.94 0.61 0.44 0.56 0.67\n25 0.44 0.89 0.56 0.44 0.44 0.56\nTable 3: Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall’s Tau: The Kendall’s tau values represent\nthe correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same\nexperimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human\npreference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "sequence is greater than or equal to the later value\nin the sequence. A Kendall’s τ greater than 0.9 is\nconsidered successful but it ranges from 0.0 to 1.0.\nIn development, researchers and engineers\nwill be comparing different RAG configurations\nthrough individual pairwise comparisons of model\nchoices, retriever selection, and document prepro-\ncessing. We want to make sure that ARES has satis-\nfactory accuracy in pairwise comparisons across a\nvariety of performance gaps between RAG systems.\nKendall’s τ is explicitly designed for measuring the\naccuracy of such pairwise comparisons, calculating\nthe correlation between a perfectly accurate pair-\nwise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in"], "generated_answer": "[[No]]"}
{"question": "SuperGrok订阅的价格是多少？", "ground_truth_contexts": [], "ground_truth_answer": "文档中没有此信息。", "retrieved_contexts": ["the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024"], "generated_answer": "[[No]]"}
