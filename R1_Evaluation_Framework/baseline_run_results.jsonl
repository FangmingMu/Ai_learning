{"question": "ARES系统的全称是什么？", "ground_truth_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"], "ground_truth_answer": "ARES代表Automated RAG Evaluation System。", "retrieved_contexts": ["seven sets of domain passages. For ReCoRD, we\nperform retrieval over its news article passages.\nThe efficacy of ARES relies on its ability to rank\ndifferent RAG systems while only using a human\npreference validation set and domain-targeted LLM\njudges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which", "trieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across", "demonstrate the ability of ARES to reliably dis-\ntinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "ARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing"], "generated_answer": "ARES系统的全称是 Automated RAG Evaluation System。"}
{"question": "ARES论文的作者是谁？", "ground_truth_contexts": ["Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com"], "ground_truth_answer": "作者是Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。", "retrieved_contexts": ["not easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences", "tance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we", "ARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "et al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-\ntems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response."], "generated_answer": "我无法提供ARES论文的作者信息，因为提供的上下文内容中没有提及作者的名字。"}
{"question": "ARES报告的三个评估分数是什么？", "ground_truth_contexts": ["ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response generated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question)."], "ground_truth_answer": "三个评估分数是上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["demonstrate the ability of ARES to reliably dis-\ntinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "systems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good", "ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses", "not easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences"], "generated_answer": "ARES报告的三个评估分数是：  \n1. **上下文相关性**（是否检索到的信息与测试问题相关）  \n2. **答案可信度**（语言模型生成的回答是否基于检索到的上下文）  \n3. **答案相关性**（回答是否也与问题相关）。"}
{"question": "ARES如何生成用于训练判断器的合成数据？", "ground_truth_contexts": ["Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus.", "We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query–passage–answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers)."], "ground_truth_answer": "ARES使用像FLAN-T5 XXL这样的语言模型从领域内段落生成合成问题-答案对，包括正面和负面示例，并使用检索器过滤低质量查询。", "retrieved_contexts": ["Figure 1: Overview of ARES: As inputs, the ARES pipeline requires an in-domain passage set, a human preference\nvalidation set of 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (five or\nmore), which are used for prompting LLMs in synthetic data generation. To prepare our LLM judges for evaluation,\nwe first generate synthetic queries and answers from the corpus passages. Using our generated training triples and a", "4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries", "ARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing"], "generated_answer": "ARES通过使用大型语言模型（LLMs）生成合成查询和答案来创建用于训练判断器的合成数据。具体来说，它利用了FLAN-T5 XXL模型来生成这些合成数据。在生成过程中，ARES会基于提供的领域内段落集、人工偏好验证集以及少量的领域内查询和答案示例进行操作。这些生成的合成数据随后被用来微调一个称为DeBERTa-v3-Large的模型，作为LLM判断器，以评估检索增强生成（RAG）系统。"}
{"question": "预测驱动推理（PPI）在ARES中的作用是什么？", "ground_truth_contexts": ["Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals for RAG scoring.", "PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our human preference validation set."], "ground_truth_answer": "PPI通过利用一小部分人工标注数据点来提高基于模型的评估准确性，为分数提供置信区间，结合标注和未标注预测。", "retrieved_contexts": ["for prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "answer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores\nthe different RAG systems being assessed using\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to improve model-based evaluation ac-\ncuracy and provide statistical confidence intervals\nfor RAG scoring. PPI utilizes a small set of human\nannotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-", "average. ARES also provided accurate confidence\nbounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.", "set of datapoints with ML predictions, PPI can de-\nvelop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confi-\ndence bounds for the success and failure rates of the\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence"], "generated_answer": "预测驱动推理（PPI）在ARES中的作用是通过使用少量的人工标注数据点来计算置信区间，从而提高基于模型的评估准确性，并为RAG系统的评分提供统计置信区间。PPI能够生成对RAG系统成功和失败率的置信边界，并估计上下文相关性、答案真实性和答案相关性的表现。"}
{"question": "ARES在评估过程中通常需要多少人工标注？", "ground_truth_contexts": ["ARES significantly improves data efficiency during evaluation by only requiring three inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more)."], "ground_truth_answer": "ARES需要大约150个或更多人工标注数据点在人类偏好验证集中。", "retrieved_contexts": ["tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries", "not easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences", "tance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we", "seven sets of domain passages. For ReCoRD, we\nperform retrieval over its news article passages.\nThe efficacy of ARES relies on its ability to rank\ndifferent RAG systems while only using a human\npreference validation set and domain-targeted LLM\njudges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which"], "generated_answer": "ARES在评估过程中需要大约150个标注的数据点，或者更多。"}
{"question": "ARES中使用哪些模型进行合成数据生成和判断器微调？", "ground_truth_contexts": ["For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge."], "ground_truth_answer": "FLAN-T5 XXL用于合成数据生成，DeBERTa-v3-Large用于微调LLM判断器。", "retrieved_contexts": ["4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-", "To evaluate whether ARES can effectively gauge\nanswer faithfulness in real RAG systems, we tested\nARES on the AIS attribution benchmark (Rashkin\net al., 2022). In AIS, we selected the Wizards\nof Wikipedia (WoW) and CNN/DM datasets; the\nother benchmark datasets involve either table rea-\nsoning (ToTTo) or focus on passage summariza-\ntion (QRECC) so we excluded them. In WoW\nand CNN/DM, each evaluation example includes\na query, a retrieved passage, and a generated an-", "ARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "Figure 1: Overview of ARES: As inputs, the ARES pipeline requires an in-domain passage set, a human preference\nvalidation set of 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (five or\nmore), which are used for prompting LLMs in synthetic data generation. To prepare our LLM judges for evaluation,\nwe first generate synthetic queries and answers from the corpus passages. Using our generated training triples and a"], "generated_answer": "在ARES中，使用FLAN-T5 XXL模型进行合成数据生成，而用于判断器微调的模型是DeBERTa-v3-Large。"}
{"question": "总结ARES管道的三个阶段。", "ground_truth_contexts": ["ARES proceeds in three stages (Figure 1). ... First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus. Second, it defines three separate judge models to perform three classification tasks... Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI)."], "ground_truth_answer": "阶段1：从语料段落生成合成查询和答案。阶段2：为上下文相关性、答案忠实度和答案相关性微调LLM判断器。阶段3：使用判断器和PPI为RAG系统评分以获得置信区间。", "retrieved_contexts": ["et al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-\ntems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries", "seven sets of domain passages. For ReCoRD, we\nperform retrieval over its news article passages.\nThe efficacy of ARES relies on its ability to rank\ndifferent RAG systems while only using a human\npreference validation set and domain-targeted LLM\njudges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which", "ARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing"], "generated_answer": "ARES管道分为三个阶段：  \n1. 生成合成查询：基于提供的输入（如领域文档集、人工偏好验证集和少量示例），生成合成查询。  \n2. 检索相关文档：在特定领域的文档集中检索相关信息。  \n3. 使用人类偏好验证集和领域针对性的LLM法官对不同的RAG系统进行排名。"}
{"question": "比较ARES和RAGAS在上下文相关性和答案相关性方面的性能。", "ground_truth_contexts": ["ARES averages a Kendall’s τ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS. ... For context relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS while for answer relevance, our system is 14.4 percentage points higher than RAGAS."], "ground_truth_answer": "ARES在Kendall’s τ上优于RAGAS（上下文相关性0.065，答案相关性0.132），并在准确性上更高（上下文相关性59.9%点，答案相关性14.4%点）。", "retrieved_contexts": ["age points higher than RAGAS . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompt-\ning and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included", "GAS. ARES averages a Kendall’sτ 0.065 higher\nfor context relevance and 0.132 higher for answer\nrelevance than RAGAS . Additionally, the LLM-\njudge is substantially more accurate than RAGAS\nat predicting context relevance and answer rele-\nvance of a query-passage-answer triple. For con-\ntext relevance, ARES with a fine-tuned LLM-judge\nis 59.9 percentage points higher than RAGASwhile\nfor answer relevance, our system is 14.4 percent-\nage points higher than RAGAS . Overall, ARES", "each evaluation domain. Overall, we found that ARES ranked RAG systems more accurately than RAGAS and\nGPT-3.5 across all the explored datasets. The Kendall’s tau for ARES was 0.065 higher on average for scoring\ncontext relevance and 0.132 higher on average for scoring answer relevance than RAGAS. Additionally, we include\nthe Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted the ranking accuracy of", "ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses"], "generated_answer": "ARES outperforms RAGAS in both context relevance and answer relevance. Specifically, ARES averages a Kendall’s τ that is 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS. Additionally, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS for context relevance and 14.4 percentage points higher for answer relevance."}
{"question": "用于评估ARES的KILT和SuperGLUE数据集有哪些？", "ground_truth_contexts": ["From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW)... From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD."], "ground_truth_answer": "KILT数据集：Natural Questions (NQ)、HotpotQA、FEVER、Wizards of Wikipedia (WoW)。SuperGLUE数据集：MultiRC和ReCoRD。", "retrieved_contexts": ["documents/answers, we hope to better gauge the\nefficacy of ARES in judging RAG outputs.\nWe do not evaluate answer faithfulness for KILT\nand SuperGLUE datasets since we do not have\nhuman-annotated hallucinated answers to use for\nevaluation. However, we do test the ARES frame-\nwork on real attribution datasets in Section 5.2.\nUsing the validation subsets for each KILT\nand SuperGLUE dataset, we create nine different\ndataset splits, ranging from 70% success rate to", "tem separately to help improve system understand-\ning and create targeted solutions, and it requires\nonly minimal human annotations. For the eight dif-\nferent datasets in KILT, SuperGLUE, and AIS re-\nquiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may", "were generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets\nOur core experimental goal is to provide a rich\npicture of where ARES can be applied effectively.\nTo test across multiple types of queries, documents,\nand answers, we selected all the datasets from the\nwidely-used KILT and SuperGLUE benchmarks\nfor which RAG is appropriate.\nFrom KILT (Petroni et al., 2021), we use Natural\nQuestions (NQ), HotpotQA, FEVER, and Wizards", "for prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction"], "generated_answer": "根据提供的信息，用于评估ARES的KILT和SuperGLUE数据集包括：\n\n- KILT数据集：\n  - Natural Questions (NQ)\n  - HotpotQA\n  - FEVER\n  - Wizards\n\n- SuperGLUE数据集（具体名称未提及）。\n\n这些数据集被选为评估ARES的基准，因为它们适用于RAG（检索增强生成）方法。"}
{"question": "ARES如何处理合成数据生成中的负面示例？", "ground_truth_contexts": ["To generate negatives for fine-tuning our LLM judges, we rely on two novel strategies... 1. Weak Negative Generation: ... 2. Strong Negative Generation: ..."], "ground_truth_answer": "ARES使用弱负面（随机采样）和强负面（从相似段落采样或生成矛盾答案）用于上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["answer from a given in-domain passage, allowing\nus to create both positive and negative training ex-\namples. We include example prompts for generat-\ning synthetic queries and answers in A.6.\nFor creating our synthetic data, we primarily use\non FLAN-T5 XXL (discussed in subsection 4.1).\nARES works well with this model (see section 5)\nbut our system can ultimately use another high-\nquality model for generating synthetic queries and\nanswers. We then filter out low-quality queries", "tance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we", "not easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences", "ARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing"], "generated_answer": "根据提供的上下文，ARES在合成数据生成中通过过滤掉低质量的查询来处理负面示例。具体来说，文中提到“我们过滤掉低质量的查询”，但没有详细说明具体的过滤方法或机制。因此，关于ARES如何处理合成数据生成中的负面示例，上下文仅提到过滤低质量查询，而没有提供更多细节。"}
{"question": "ARES实验中使用的置信区间概率是多少？", "ground_truth_contexts": ["Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (probability) for our confidence interval."], "ground_truth_answer": "ARES在其实验中使用95%的置信区间概率。", "retrieved_contexts": ["intervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (proba-\nbility) for our confidence interval.\nWith the accuracy confidence interval for each\ncomponent of the RAG, we find the midpoint of\neach confidence interval and use the midpoints to\nrank the RAG systems. With our ranking, we can\ncompare different RAG systems, as well as differ-\nent configurations of the same RAG system, to find\nthe best-performing approach for a given domain.", "average. ARES also provided accurate confidence\nbounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.", "systems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good", "tance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we"], "generated_answer": "ARES实验中使用的置信区间概率是95%。"}
{"question": "分析ARES在跨域应用中的优势。", "ground_truth_contexts": ["In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross-domain applications. Across all settings, we found that LLM judges in ARES had strong generalizability... PPI helped mitigate the loss in accuracy and still allow ARES to be successful."], "ground_truth_answer": "ARES判断器在KILT和SuperGLUE的查询和文档类型上显示出强大的泛化能力，PPI缓解准确性损失，但在语言、代码或实体提取等剧烈转变中失败。", "retrieved_contexts": ["seven sets of domain passages. For ReCoRD, we\nperform retrieval over its news article passages.\nThe efficacy of ARES relies on its ability to rank\ndifferent RAG systems while only using a human\npreference validation set and domain-targeted LLM\njudges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which", "the best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in", "ARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "not easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences"], "generated_answer": "ARES在跨域应用中的优势主要体现在以下几个方面：\n\n1. **通用性**：ARES框架的LLM法官具有一定的泛化能力，使其能够在不同领域中部署，尤其是在那些难以获取领域内查询、文档和答案的特定领域。\n\n2. **灵活性**：ARES是一个灵活的框架，可以根据需要进行调整和改进。例如，可以使用GPT-4等强大的生成式LLM来替代人工标注，从而提高效率和准确性。\n\n3. **合成数据的支持**：通过使用合成的查询-段落-答案三元组，ARES能够模拟多个RAG系统，这些系统在评估指标上的准确率差异很小。这有助于测试和优化框架的性能。\n\n4. **多语言潜力**：虽然目前所有用于评估ARES的数据集都是英文的，但未来可以通过使用不同的LLM作为ARES法官和合成数据生成工具，探索在其他语言中的应用，从而更好地理解当前框架的优势和局限性。\n\n这些优势使得ARES在跨域应用中具有较大的潜力和实用性。"}
{"question": "论文中提到的ARES的局限性是什么？", "ground_truth_contexts": ["ARES relies on a small set of annotations... These annotations often require an annotator familiar with the RAG system’s domain... The LLMs used in ARES benefit substantially from GPU-based hardware... Additionally, all of the datasets used in our evaluation of ARES are in English..."], "ground_truth_answer": "局限性包括需要领域专家标注、需要GPU硬件，并且仅在英文数据集上评估。", "retrieved_contexts": ["ARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "seven sets of domain passages. For ReCoRD, we\nperform retrieval over its news article passages.\nThe efficacy of ARES relies on its ability to rank\ndifferent RAG systems while only using a human\npreference validation set and domain-targeted LLM\njudges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which", "not easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences", "tance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we"], "generated_answer": "论文中提到的ARES的局限性包括：\n\n1. ARES依赖于人类偏好验证集和领域针对性的LLM法官来对不同的RAG系统进行排序，这可能限制了其应用范围。\n2. 一些用于评估ARES的数据集是英文的，这可能限制了其在其他语言中的应用。\n3. 生成用于微调LLM法官的合成数据集可能需要昂贵的资源，这使得这些数据集不容易被所有NLP研究人员和实践者获得。"}
{"question": "ARES在AIS基准上的答案忠实度表现如何？", "ground_truth_contexts": ["Table 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores."], "ground_truth_answer": "ARES在AIS基准上对答案幻觉的预测在真实平均值的2.5个百分点以内。", "retrieved_contexts": ["To evaluate whether ARES can effectively gauge\nanswer faithfulness in real RAG systems, we tested\nARES on the AIS attribution benchmark (Rashkin\net al., 2022). In AIS, we selected the Wizards\nof Wikipedia (WoW) and CNN/DM datasets; the\nother benchmark datasets involve either table rea-\nsoning (ToTTo) or focus on passage summariza-\ntion (QRECC) so we excluded them. In WoW\nand CNN/DM, each evaluation example includes\na query, a retrieved passage, and a generated an-", "a query, a retrieved passage, and a generated an-\nswer (which is either faithful or non-attributed to\nthe retrieved passage).\nTable 2 summarizes our AIS results. We found\nthat ARES can effectively score the AIS datasets,\ngetting within 2.5 accuracy points of the correct\nscores. Furthermore, for scoring each system,\nwe only use 200 annotated datapoints for our hu-\nman preference validation set. Our results on AIS\ndemonstrate the ability of ARES to reliably dis-", "demonstrate the ability of ARES to reliably dis-\ntinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses"], "generated_answer": "ARES在AIS基准上的答案忠实度表现良好，能够有效评分，与正确分数的差距不超过2.5个百分点。这表明ARES能够可靠地区分真实答案和幻觉答案。"}
{"question": "真实RAG系统评估中测试了哪些检索器和生成LLM？", "ground_truth_contexts": ["For our RAG systems, we selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search, and ColBERTv2... and three different generative LLMs (MPT-7b-Instruct, GPT-3.5-Turbo, and GPT-4)."], "ground_truth_answer": "检索器：BM25、OpenAI Ada嵌入、ColBERTv2。生成LLM：MPT-7b-Instruct、GPT-3.5-Turbo、GPT-4。", "retrieved_contexts": ["demonstrate the ability of ARES to reliably dis-\ntinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "tab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-", "constrastive learning framework, we fine-tune an LLM to classify query–passage–answer triples in three different\ncriteria: context relevance, answer faithfulness, and answer relevance. Finally, we use the LLM judges to score\nRAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.\n3.3 Ranking RAG Systems with Confidence\nIntervals\nOnce we have prepared our LLM judges, we need\nto use them to score and rank the competing RAG"], "generated_answer": "在真实RAG系统评估中，测试的检索器和生成LLM的具体信息没有在提供的上下文中明确提及。因此，我无法提供具体的检索器或生成LLM的名称。"}
{"question": "总结ARES在伪RAG系统排名上的结果。", "ground_truth_contexts": ["Table 1: ... Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets."], "ground_truth_answer": "ARES在Kendall’s τ上高于RAGAS（上下文相关性0.065，答案相关性0.132），并在使用78%更少标注的情况下优于采样标注。", "retrieved_contexts": ["demonstrate the ability of ARES to reliably dis-\ntinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "age points higher than RAGAS . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompt-\ning and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included", "ARES Ranking of Pseudo RAG Systems\nNQ HotpotQA WoW FEVER MultiRC ReCoRD\nC.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R.\nKendall’s Tau for\nSampled Annotations 0.83 0.89 0.78 0.78 0.78 0.83 0.89 0.89 0.83 0.83 0.72 0.94\nKendall’s Tau\nfor RAGAS 0.89 0.89 0.94 0.89 0.94 0.94 0.72 0.61 0.83 0.94 0.89 0.44\nKendall’s Tau\nfor GPT-3.5 Judge 0.89 0.94 0.67 0.94 0.94 0.89 0.78 0.78 0.83 0.89 0.83 0.94\nKendall’s Tau for\nARES LLM Judge 0.89 1.0 0.89 0.94 0.94 1.0 0.83 0.72 0.94 0.83 0.78 0.83\nKendall’s Tau", "ARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses"], "generated_answer": "ARES在伪RAG系统排名上的结果表明，它在多个数据集上表现优异。具体来说，在NQ、HotpotQA、WoW、FEVER、MultiRC和ReCoRD数据集上，ARES的准确率分别为85.6%、93.3%、84.5%、88.2%、70.4%和84.0%。此外，ARES的Kendall’s Tau值在大多数数据集上也优于其他方法，如RAGAS和GPT-3.5法官。这表明ARES能够更准确地评估和排名RAG系统。"}
{"question": "ARES中有效PPI所需的最少人工标注数量是多少？", "ground_truth_contexts": ["We used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3)."], "ground_truth_answer": "ARES中有效PPI至少需要150个人工标注。", "retrieved_contexts": ["annotations to less than ten for few-shot prompts. Additionally, the efficacy of PPI continues improving as we\ngenerate more GPT-4 generated labels. In the table, we define PPI range as the number of percentage points from\nthe lower number to the upper number of the PPI confidence bounding. Additionally, we use the fine-tuned LLM\njudge (DeBERTa-v3-Large) for evaluation.", "logits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n7 Limitations\nARES relies on a small set of annotations in the\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\noften require an annotator familiar with the RAG\nsystem’s domain application. While these annota-\ntions can be easy to generate for general-domain\napplications, more specialized domains, such as", "NQ ReCoRD MultiRC\nContext\nRelevance\nAnswer\nRelevance\nContext\nRelevance\nAnswer\nRelevance\nContext\nRelevance\nAnswer\nRelevance\nKendall’s Tau 0.78 1.0 0.78 0.72 0.89 0.78\nKendall’s Tau of\nHuman Labeled Approach 0.94 1.0 0.83 0.89 0.94 0.89\nAverage PPI Range 9.2% 6.8% 8.2% 9.0% 7.7% 8.3%\nAccuracy on\nRAG Evaluation Sets 79.3% 96.7% 88.4% 78.3% 85.8% 82.5%\nTable 4: GPT-4 Labels vs. Human Labels : We wanted to explore the practicality of using GPT-4 generated", "costs required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of"], "generated_answer": "根据提供的信息，ARES中有效PPI所需的最少人工标注数量大约是150-300个数据点。"}
{"question": "ARES是否支持非英语语言的评估？", "ground_truth_contexts": ["While LLM judges in ARES were successful in cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages...", "To test cross-lingual transfer, we used the XGLUE datasets... a LLM judge fine-tuned on NQ achieved a Kendall’s tau of 0.33..."], "ground_truth_answer": "不，ARES无法很好地泛化到非英语语言，在XGLUE上Kendall’s τ较低（例如0.33）。", "retrieved_contexts": ["not easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences", "seven sets of domain passages. For ReCoRD, we\nperform retrieval over its news article passages.\nThe efficacy of ARES relies on its ability to rank\ndifferent RAG systems while only using a human\npreference validation set and domain-targeted LLM\njudges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which", "demonstrate the ability of ARES to reliably dis-\ntinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "ARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing"], "generated_answer": "根据提供的上下文，ARES目前的评估是在英语数据集上进行的。文中提到：“All of the datasets used in our evaluation of ARES are in English”，这意味着当前的评估主要针对英语语言。不过，文中也提到未来的工作可以探索如何在其他语言中使用ARES，通过利用不同的LLM（大语言模型）作为ARES的判断器和合成数据生成工具。因此，虽然目前ARES可能不直接支持非英语语言的评估，但未来有可能扩展其支持范围。"}
{"question": "SuperGrok订阅的价格是多少？", "ground_truth_contexts": [], "ground_truth_answer": "文档中没有此信息。", "retrieved_contexts": ["Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.", "guage generation models.\nJon Saad-Falcon, Omar Khattab, Keshav Santhanam,\nRadu Florian, Martin Franz, Salim Roukos, Avirup\nSil, Md Arafat Sultan, and Christopher Potts. 2023.", "arXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge"], "generated_answer": "我无法提供SuperGrok订阅的价格信息，因为这不在提供的上下文中。"}
