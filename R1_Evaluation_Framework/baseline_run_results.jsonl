{"question": "ARES系统的全称是什么？", "ground_truth_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"], "ground_truth_answer": "ARES代表Automated RAG Evaluation System。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in", "framework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "et al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-\ntems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several asso-\nciated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\noffer little adaptability to new RAG evaluation set-"], "generated_answer": "ARES系统的全称是**Automated RAG Evaluation System**，即自动化检索增强生成评估系统。"}
{"question": "ARES论文的作者是谁？", "ground_truth_contexts": ["Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com"], "ground_truth_answer": "作者是Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "et al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-\ntems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several asso-\nciated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\noffer little adaptability to new RAG evaluation set-", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "根据提供的信息，ARES 论文的作者包括 Jon Saad-Falcon、Omar Khattab、Christopher Potts 和 Matei Zaharia。他们分别来自斯坦福大学、Databricks 和加州大学伯克利分校。"}
{"question": "ARES报告的三个评估分数是什么？", "ground_truth_contexts": ["ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response generated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question)."], "ground_truth_answer": "三个评估分数是上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "et al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-\ntems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several asso-\nciated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\noffer little adaptability to new RAG evaluation set-", "framework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "ARES报告的三个评估分数是：  \n1. **Context Relevance**（上下文相关性）  \n2. **Answer Faithfulness**（回答忠实性）  \n3. **Answer Relevance**（回答相关性）"}
{"question": "ARES如何生成用于训练判断器的合成数据？", "ground_truth_contexts": ["Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus.", "We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query–passage–answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers)."], "ground_truth_answer": "ARES使用像FLAN-T5 XXL这样的语言模型从领域内段落生成合成问题-答案对，包括正面和负面示例，并使用检索器过滤低质量查询。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-\nnAI’s gpt-3.5-turbo-16k, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similar-\nity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from", "Figure 1: Overview of ARES: As inputs, the ARES pipeline requires an in-domain passage set, a human preference\nvalidation set of 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (five or\nmore), which are used for prompting LLMs in synthetic data generation. To prepare our LLM judges for evaluation,\nwe first generate synthetic queries and answers from the corpus passages. Using our generated training triples and a\nconstrastive learning framework, we fine-tune an LLM to classify query–passage–answer triples in three different\ncriteria: context relevance, answer faithfulness, and answer relevance. Finally, we use the LLM judges to score\nRAG systems and generate confidence bounds for the ranking using PPI and the human preference validation set.\n3.3 Ranking RAG Systems with Confidence\nIntervals\nOnce we have prepared our LLM judges, we need\nto use them to score and rank the competing RAG\nsystems. To do this, ARES samples the in-domain"], "generated_answer": "ARES通过以下步骤生成用于训练判断器的合成数据：\n\n1. **输入准备**：  \n   - ARES需要三个输入：一个**领域内的语料库（passage set）**，一个**人类偏好验证集（约150个或更多标注数据点）**，以及一些**领域内问答的少样本示例（至少5个）**。这些少样本示例用于提示大语言模型（LLM）进行合成数据生成。\n\n2. **合成查询与答案生成**：  \n   - 利用LLM（在论文中使用的是FLAN-T5 XXL），从语料库中的每一段文本生成**合成查询（queries）和对应的答案（answers）**。  \n   - 这些生成的查询和答案与原始语料段落一起，构成**查询-段落-答案三元组（query–passage–answer triples）**。\n\n3. **用于训练LLM判断器**：  \n   - 这些三元组随后用于训练ARES的LLM判断器（在论文中使用的是DeBERTa-v3-Large）。  \n   - 判断器通过对比学习框架进行微调，以评估三元组在以下三个标准上的表现：\n     - **上下文相关性（context relevance）**\n     - **答案可信度（answer faithfulness）**\n     - **答案相关性（answer relevance）**\n\n总结：  \nARES使用FLAN-T5 XXL等LLM从领域语料中生成合成查询和答案，并结合原始段落形成三元组，用于微调一个LLM判断器，从而评估RAG系统的输出质量。"}
{"question": "预测驱动推理（PPI）在ARES中的作用是什么？", "ground_truth_contexts": ["Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals for RAG scoring.", "PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our human preference validation set."], "ground_truth_answer": "PPI通过利用一小部分人工标注数据点来提高基于模型的评估准确性，为分数提供置信区间，结合标注和未标注预测。", "retrieved_contexts": ["puts from each RAG systems separately, which can\nbe costly both in terms of time and financing.\nTo combine the benefits of both, and hence\nboost the precision of the evaluation, ARES uses\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to predict the system scores. PPI is\na recent statistical method that provides tighter\nconfidence intervals on a small set of annotated\ndatapoints (i.e., our validation set) by leveraging\npredictions on a much larger set of non-annotated\ndatapoints. PPI can leverage both the labeled dat-\napoints and the ARES judge predictions on the\nnon-annotated datapoints to construct confidence\nintervals for our RAG system’s performance.\nTo do this, PPI uses the LLM judges on the hu-\nman preference validation set to learn a rectifier\nfunction for constructing a confidence set of the ML\nmodel’s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence", "bounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).", "interval for the performance of the evaluated RAG\nsystem (e.g. its context relevance, answer faithful-\nness, or answer relevance accuracy individually)\ncompared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the hu-\nman preference validation set with the much larger\nset of datapoints with ML predictions, PPI can de-\nvelop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confi-\ndence bounds for the success and failure rates of the\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (proba-\nbility) for our confidence interval.\nWith the accuracy confidence interval for each\ncomponent of the RAG, we find the midpoint of", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda"], "generated_answer": "预测驱动推理（PPI）在ARES中的作用是通过利用大规模未标注数据集上的预测来提供更紧密的置信区间，从而提升评估的精确度。具体来说，PPI结合了少量人工标注的数据（即验证集）和大量未标注数据上的ARES判断模型的预测结果，构建出可靠的性能置信区间。这种方法能够估计RAG系统在上下文相关性、答案忠实性和答案相关性方面的表现，并生成具有选定概率水平（如95%）的置信区间。\n\n此外，PPI通过学习一个校正函数来估计LLM判断模型的误差，从而为RAG系统的成功和失败率提供置信界限。相比仅使用少量标注数据的传统方法，PPI显著提高了统计推断的效率，并生成更窄的置信区间（例如，在ARES实验中，上下文相关性的置信区间平均宽度为7.4分，答案相关性为6.1分）。这使得ARES能够更准确地评估RAG系统的性能。"}
{"question": "ARES在评估过程中通常需要多少人工标注？", "ground_truth_contexts": ["ARES significantly improves data efficiency during evaluation by only requiring three inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more)."], "ground_truth_answer": "ARES需要大约150个或更多人工标注数据点在人类偏好验证集中。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-\nform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "ARES在评估过程中通常只需要一个小型的人工标注验证集，大约包含150个或更多的标注数据点。这些数据点用于预测驱动推断（prediction-powered inference, PPI），以估计RAG系统的质量并计算置信区间。相比传统方法，ARES显著提高了数据效率，因为它不需要大量人工标注数据来评分整个评估过程。"}
{"question": "ARES中使用哪些模型进行合成数据生成和判断器微调？", "ground_truth_contexts": ["For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge."], "ground_truth_answer": "FLAN-T5 XXL用于合成数据生成，DeBERTa-v3-Large用于微调LLM判断器。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-\nnAI’s gpt-3.5-turbo-16k, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similar-\nity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from"], "generated_answer": "在ARES中，使用了以下模型进行合成数据生成和判断器微调：\n\n1. **合成数据生成**：\n   - 使用了 **FLAN-T5 XXL** 模型来生成合成查询和答案。这是一种强大的预训练语言模型，适用于生成高质量的合成数据。\n\n2. **判断器微调**：\n   - 选择了 **DeBERTa-v3-Large** 模型作为微调的语言模型判断器（LLM judge）。该模型用于对检索增强生成（RAG）系统的性能进行评估，支持通过少量样本提示（few-shot prompts）和部署在商业GPU上的可运行语言模型来完成任务。\n\n这些模型的选择确保了ARES能够高效、准确地生成合成数据并评估RAG系统的质量。"}
{"question": "总结ARES管道的三个阶段。", "ground_truth_contexts": ["ARES proceeds in three stages (Figure 1). ... First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus. Second, it defines three separate judge models to perform three classification tasks... Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI)."], "ground_truth_answer": "阶段1：从语料段落生成合成查询和答案。阶段2：为上下文相关性、答案忠实度和答案相关性微调LLM判断器。阶段3：使用判断器和PPI为RAG系统评分以获得置信区间。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from", "et al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-\ntems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several asso-\nciated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\noffer little adaptability to new RAG evaluation set-", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "ARES管道的三个阶段如下：\n\n1. **合成数据集的生成**：  \n   利用目标语料库中的段落生成合成查询及其答案。这些查询-段落-答案三元组将用于后续训练。\n\n2. **训练LLM评估模型**：  \n   使用生成的查询-段落-答案三元组来训练轻量级语言模型（LLM）评估模型，使其能够评估RAG系统的输出质量。\n\n3. **应用评估模型与预测驱动推理**：  \n   将训练好的评估模型应用于任何RAG系统，对其输出的查询-文档-答案三元组进行评分，并结合少量人工标注的数据点使用预测驱动推理（PPI）来估计RAG系统的质量置信区间。"}
{"question": "比较ARES和RAGAS在上下文相关性和答案相关性方面的性能。", "ground_truth_contexts": ["ARES averages a Kendall’s τ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS. ... For context relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS while for answer relevance, our system is 14.4 percentage points higher than RAGAS."], "ground_truth_answer": "ARES在Kendall’s τ上优于RAGAS（上下文相关性0.065，答案相关性0.132），并在准确性上更高（上下文相关性59.9%点，答案相关性14.4%点）。", "retrieved_contexts": ["Appendices A.2, A.3, and A.4. For both ARES\nand the GPT-3.5 judge baseline, we augment the\nLLM with PPI, using a 300-datapoint human pref-\nerence validation set to rectify the ML predictions\nand produce confidence intervals.\nAcross almost all settings across the datasets\nfrom KILT and SuperGLUE, ARES provides a\nmore accurate ranking of RAG systems than RA-\nGAS. ARES averages a Kendall’sτ 0.065 higher\nfor context relevance and 0.132 higher for answer\nrelevance than RAGAS . Additionally, the LLM-\njudge is substantially more accurate than RAGAS\nat predicting context relevance and answer rele-\nvance of a query-passage-answer triple. For con-\ntext relevance, ARES with a fine-tuned LLM-judge\nis 59.9 percentage points higher than RAGASwhile\nfor answer relevance, our system is 14.4 percent-\nage points higher than RAGAS . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompt-", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses", "ing ARES. By sampling both related and unrelated\ndocuments/answers, we hope to better gauge the\nefficacy of ARES in judging RAG outputs.\nWe do not evaluate answer faithfulness for KILT\nand SuperGLUE datasets since we do not have\nhuman-annotated hallucinated answers to use for\nevaluation. However, we do test the ARES frame-\nwork on real attribution datasets in Section 5.2.\nUsing the validation subsets for each KILT\nand SuperGLUE dataset, we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70.0%, 72.5%, 75.0%, . . . , 90.0%).\nEach split also represents a different mock RAG\nsystem. Since we know the success percentages of\neach dataset split, we know the appropriate rank-\ning of each mock RAG system. This allows us to", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda"], "generated_answer": "根据提供的信息，可以比较 **ARES** 和 **RAGAS** 在上下文相关性（Context Relevance, C.R.）和答案相关性（Answer Relevance, A.R.）方面的性能，具体如下：\n\n---\n\n### **1. Kendall’s Tau（衡量排名准确性）**\n- **上下文相关性（C.R.）**\n  - ARES 的平均 Kendall’s Tau 比 RAGAS 高 **0.065**。\n- **答案相关性（A.R.）**\n  - ARES 的平均 Kendall’s Tau 比 RAGAS 高 **0.132**。\n\n这表明 **ARES 在排名准确性方面优于 RAGAS**，尤其是在答案相关性上表现更显著。\n\n---\n\n### **2. 准确率（Accuracy）**\n- **上下文相关性（C.R.）**\n  - RAGAS 的平均准确率为 **33.9%**（基于 NQ, WoW, FEVER 数据集）。\n  - ARES 的平均准确率为 **83.5%**。\n- **答案相关性（A.R.）**\n  - RAGAS 的平均准确率为 **74.7%**。\n  - ARES 的平均准确率为 **90.2%**。\n\n具体数据如下：\n| 数据集       | C.R. - RAGAS | C.R. - ARES | A.R. - RAGAS | A.R. - ARES |\n|--------------|--------------|--------------|--------------|--------------|\n| NQ           | 35.9%        | 85.6%        | 68.2%        | 93.3%        |\n| WoW          | 44.4%        | 84.5%        | 80.1%        | 88.2%        |\n| FEVER        | 21.4%        | 70.4%        | 75.9%        | 84.0%        |\n\n这表明 **ARES 在准确率方面显著优于 RAGAS**，尤其是在上下文相关性的判断上。\n\n---\n\n### **3. 总结**\n- **ARES** 在上下文相关性和答案相关性方面，无论是在 **Kendall’s Tau 排名指标** 还是 **准确率指标** 上，都 **明显优于 RAGAS**。\n- ARES 的优势主要来自于其 **使用了微调的 LLM 作为评判模型**，并通过 **领域自适应技术** 提升了评估效果。\n- 特别是在 **上下文相关性** 上，ARES 的提升幅度更大。\n\n如果需要进一步分析具体数据集上的表现或其他相关指标，也可以继续深入探讨。"}
{"question": "用于评估ARES的KILT和SuperGLUE数据集有哪些？", "ground_truth_contexts": ["From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW)... From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD."], "ground_truth_answer": "KILT数据集：Natural Questions (NQ)、HotpotQA、FEVER、Wizards of Wikipedia (WoW)。SuperGLUE数据集：MultiRC和ReCoRD。", "retrieved_contexts": ["ing ARES. By sampling both related and unrelated\ndocuments/answers, we hope to better gauge the\nefficacy of ARES in judging RAG outputs.\nWe do not evaluate answer faithfulness for KILT\nand SuperGLUE datasets since we do not have\nhuman-annotated hallucinated answers to use for\nevaluation. However, we do test the ARES frame-\nwork on real attribution datasets in Section 5.2.\nUsing the validation subsets for each KILT\nand SuperGLUE dataset, we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70.0%, 72.5%, 75.0%, . . . , 90.0%).\nEach split also represents a different mock RAG\nsystem. Since we know the success percentages of\neach dataset split, we know the appropriate rank-\ning of each mock RAG system. This allows us to", "4.2 Datasets\nOur core experimental goal is to provide a rich\npicture of where ARES can be applied effectively.\nTo test across multiple types of queries, documents,\nand answers, we selected all the datasets from the\nwidely-used KILT and SuperGLUE benchmarks\nfor which RAG is appropriate.\nFrom KILT (Petroni et al., 2021), we use Natural\nQuestions (NQ), HotpotQA, FEVER, and Wizards\nof Wikipedia (WoW) (Kwiatkowski et al., 2019;\nYang et al., 2018; Akhtar et al., 2023; Dinan et al.,\n2018). Each dataset uses Wikipedia passages but\nthe queries and answers offer a range of applica-\ntions. Both NQ and HotpotQA feature direct ques-\ntions and expect short answers, but NQ uses single\npassages for reasoning while HotpotQA requires\nmultiple passages for reasoning. Furthermore,\nFEVER focuses on fact-verification, determining if\na passage supports or refutes a given statement, and\nexpects an output of “SUPPORTS” or “REFUTES”.\nWoW seeks to evaluate dialogue agents by mapping\nuser dialogue to relevant Wikipedia passages be-", "evaluation framework for retrieval-augmented gen-\neration (RAG). ARES offers a novel training\npipeline for fine-tuning lightweight LLM judges\non synthetically generated queries and answers.\nARES can evaluate each component of a RAG sys-\ntem separately to help improve system understand-\ning and create targeted solutions, and it requires\nonly minimal human annotations. For the eight dif-\nferent datasets in KILT, SuperGLUE, and AIS re-\nquiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda"], "generated_answer": "用于评估ARES的KILT数据集包括：\n\n1. Natural Questions (NQ)\n2. HotpotQA\n3. FEVER\n4. Wizards of Wikipedia (WoW)\n\nSuperGLUE数据集在文中并未具体列出，但提到从中选择了适合RAG的数据集。"}
{"question": "ARES如何处理合成数据生成中的负面示例？", "ground_truth_contexts": ["To generate negatives for fine-tuning our LLM judges, we rely on two novel strategies... 1. Weak Negative Generation: ... 2. Strong Negative Generation: ..."], "ground_truth_answer": "ARES使用弱负面（随机采样）和强负面（从相似段落采样或生成矛盾答案）用于上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "ing ARES. By sampling both related and unrelated\ndocuments/answers, we hope to better gauge the\nefficacy of ARES in judging RAG outputs.\nWe do not evaluate answer faithfulness for KILT\nand SuperGLUE datasets since we do not have\nhuman-annotated hallucinated answers to use for\nevaluation. However, we do test the ARES frame-\nwork on real attribution datasets in Section 5.2.\nUsing the validation subsets for each KILT\nand SuperGLUE dataset, we create nine different\ndataset splits, ranging from 70% success rate to\n90% success rate for each of the evaluated RAG\ncriteria; each dataset is separated by 2.5% accuracy\npoints (e.g. 70.0%, 72.5%, 75.0%, . . . , 90.0%).\nEach split also represents a different mock RAG\nsystem. Since we know the success percentages of\neach dataset split, we know the appropriate rank-\ning of each mock RAG system. This allows us to", "We generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative\nexamples of query–passage–answer triples (e.g.,\nrelevant/irrelevant passages and correct/incorrect\nanswers). For generation, the LLM uses our in-\nput set of few-shot examples with in-domain pas-\nsages mapped to in-domain queries and answers;\nthe model then generates a synthetic question and\nanswer from a given in-domain passage, allowing\nus to create both positive and negative training ex-\namples. We include example prompts for generat-\ning synthetic queries and answers in A.6.\nFor creating our synthetic data, we primarily use\non FLAN-T5 XXL (discussed in subsection 4.1).\nARES works well with this model (see section 5)\nbut our system can ultimately use another high-\nquality model for generating synthetic queries and\nanswers. We then filter out low-quality queries\nby testing if a given query can retrieve its original", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "ARES通过使用生成式大语言模型（LLMs）从语料库段落中生成合成查询和答案来创建负面示例。这些生成的数据代表了查询-段落-答案三元组的正例和负例（例如，相关/不相关段落和正确/错误答案）。具体来说，ARES利用一组领域内段落映射到领域内查询和答案的少量示例，模型随后从给定的领域内段落生成合成问题和答案。这样，ARES能够创建出正例和负例的训练样本。\n\n为了生成负面示例，ARES在生成答案时可能会引入错误，或者选择与查询不相关的段落，从而构造出错误的三元组。此外，生成的低质量查询会被过滤掉，方法是测试该查询是否能够检索到其原始段落。这种过滤机制有助于确保生成的负面示例具有一定的质量，并能够有效用于训练轻量级的模型评估器。\n\n简而言之，ARES通过生成错误的答案或选择不相关的段落来创建负面示例，并利用过滤机制确保生成的负面示例的有效性。"}
{"question": "ARES实验中使用的置信区间概率是多少？", "ground_truth_contexts": ["Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (probability) for our confidence interval."], "ground_truth_answer": "ARES在其实验中使用95%的置信区间概率。", "retrieved_contexts": ["bounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).", "component of the RAG, we find the midpoint of\neach confidence interval and use the midpoints to\nrank the RAG systems. With our ranking, we can\ncompare different RAG systems, as well as differ-\nent configurations of the same RAG system, to find\nthe best-performing approach for a given domain.", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "Kendall’s Tau by Dataset\nNQ MultiRC ReCoRD\nPPI Labeled\nCount C.R. A.R. C.R. A.R. C.R. A.R.\n400 1.0 1.0 0.89 0.94 0.89 0.94\n300 0.89 1.0 0.94 0.89 0.83 0.89\n200 0.83 1.0 0.83 0.94 0.83 0.83\n150 0.72 1.0 0.83 0.89 0.72 0.83\n100 0.44 1.0 0.67 0.67 0.67 0.83\n50 0.44 0.94 0.61 0.44 0.56 0.67\n25 0.44 0.89 0.56 0.44 0.44 0.56\nTable 3: Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall’s Tau: The Kendall’s tau values represent\nthe correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same\nexperimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human\npreference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their\naccuracies in context relevance and answer relevance (C.R. and A.R., respectively).\nARES Ranking of Pseudo RAG Systems using GPT-4 Labels\nNQ ReCoRD MultiRC\nContext\nRelevance\nAnswer\nRelevance\nContext\nRelevance\nAnswer\nRelevance\nContext\nRelevance\nAnswer\nRelevance"], "generated_answer": "在ARES实验中，使用的置信区间概率是95%。根据提供的信息，ARES的预测置信区间能够以超过95%的时间捕捉到真实结果的平均值，具体是在上下文相关性（context relevance）和回答相关性（answer relevance）这两个指标上。"}
{"question": "分析ARES在跨域应用中的优势。", "ground_truth_contexts": ["In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross-domain applications. Across all settings, we found that LLM judges in ARES had strong generalizability... PPI helped mitigate the loss in accuracy and still allow ARES to be successful."], "ground_truth_answer": "ARES判断器在KILT和SuperGLUE的查询和文档类型上显示出强大的泛化能力，PPI缓解准确性损失，但在语言、代码或实体提取等剧烈转变中失败。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "bounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).", "type (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-\nGLUE, LLM judges are unable to generalize when\nmaking more drastic shifts in domain, such as:\nswitching languages (e.g. English to Spanish, Ger-\nman, and other languages), switching from text to\ncode (e.g. questions + passages to coding functions\n+ documentation), and switching from retrieving", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from"], "generated_answer": "根据提供的信息，ARES在跨域应用中表现出以下优势：\n\n1. **强泛化能力**：在跨域应用场景中，ARES使用的微调后的LLM（语言模型）判官表现成功，即使仅使用了300个数据点的人类偏好验证集用于PPI（预测驱动推断），LLM判官仍显示出强大的泛化能力。\n\n2. **PPI缓解准确率下降**：即使在LLM判官的跨域应用中准确率有所下降，PPI方法能够缓解这种准确率的损失，确保ARES依然有效。这表明ARES框架对于跨域任务具有一定的鲁棒性。\n\n3. **性能持续提升**：随着PPI附加样本的增加，ARES在后续跨域测试中的性能得到了进一步提升，说明该框架能够通过引入更多样本来持续优化结果。\n\n4. **适用于多种任务类型**：实验结果显示，ARES在跨域应用中表现良好，无论测试任务的查询类型（例如从NQ到FEVER）、文档类型（例如从NQ到MultiRC）或两者都发生变化（例如从NQ到ReCoRD）。\n\n综上，ARES在跨域应用中展现出了良好的适应性和稳定性，尤其是在查询和文档类型发生变化的情况下。"}
{"question": "论文中提到的ARES的局限性是什么？", "ground_truth_contexts": ["ARES relies on a small set of annotations... These annotations often require an annotator familiar with the RAG system’s domain... The LLMs used in ARES benefit substantially from GPU-based hardware... Additionally, all of the datasets used in our evaluation of ARES are in English..."], "ground_truth_answer": "局限性包括需要领域专家标注、需要GPU硬件，并且仅在英文数据集上评估。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "bounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).", "logits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n7 Limitations\nARES relies on a small set of annotations in the\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\noften require an annotator familiar with the RAG\nsystem’s domain application. While these annota-\ntions can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.", "et al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-\ntems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several asso-\nciated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\noffer little adaptability to new RAG evaluation set-"], "generated_answer": "论文中提到的ARES的局限性主要包括以下几点：\n\n1. **依赖少量人工标注数据**：ARES需要一小部分人工标注数据用于验证（大约150-300个数据点，但更多更好）。这些标注任务通常需要熟悉RAG系统应用领域的标注人员。虽然通用领域的标注相对容易生成，但在法律、医学和金融等专业领域，可能需要具有专业知识的标注人员。\n\n2. **对硬件资源的高要求**：ARES中使用的大型语言模型（LLMs）需要基于GPU的高性能硬件支持。例如，在ARES中使用的DeBERTa-v3-Large（304M参数）和FLAN-T5-XXL（11.3B参数）分别需要约32GB内存的GPU来运行，微调和生成过程分别需要数小时。尽管商业GPU广泛可用，但由于成本较高，并非所有NLP研究人员和从业者都能轻松获得这些资源。\n\n3. **跨领域应用的挑战**：尽管ARES的LLM评估器具有一定的泛化能力，但在跨领域应用时仍面临挑战。例如，当查询类型、文档类型或两者同时发生变化时，评估器的性能可能会受到影响。这表明在特定领域的应用中，可能需要进一步优化和调整评估器以适应新领域的需求。\n\n这些局限性表明，尽管ARES在RAG系统的评估方面具有一定的优势，但在实际应用中仍存在一些挑战和改进空间。"}
{"question": "ARES在AIS基准上的答案忠实度表现如何？", "ground_truth_contexts": ["Table 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores."], "ground_truth_answer": "ARES在AIS基准上对答案幻觉的预测在真实平均值的2.5个百分点以内。", "retrieved_contexts": ["of Wikipedia (WoW) and CNN/DM datasets; the\nother benchmark datasets involve either table rea-\nsoning (ToTTo) or focus on passage summariza-\ntion (QRECC) so we excluded them. In WoW\nand CNN/DM, each evaluation example includes\na query, a retrieved passage, and a generated an-\nswer (which is either faithful or non-attributed to\nthe retrieved passage).\nTable 2 summarizes our AIS results. We found\nthat ARES can effectively score the AIS datasets,\ngetting within 2.5 accuracy points of the correct\nscores. Furthermore, for scoring each system,\nwe only use 200 annotated datapoints for our hu-\nman preference validation set. Our results on AIS\ndemonstrate the ability of ARES to reliably dis-\ntinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "bounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses"], "generated_answer": "根据提供的信息，ARES在AIS（Answer Interpretation with Sources）基准上的答案忠实度表现如下：\n\n- ARES能够有效地对AIS数据集进行评分，其准确度与正确评分的差距在2.5个百分点以内。\n- ARES在区分基于检索增强生成（RAG）系统中的忠实答案和幻觉答案方面表现出可靠的能力。\n- 为了验证系统评分，仅使用了200个标注的数据点作为人类偏好验证集。\n\n总结：ARES在AIS基准上展现了良好的答案忠实度评估能力，能够在少量标注数据的情况下有效区分忠实和非忠实答案。"}
{"question": "真实RAG系统评估中测试了哪些检索器和生成LLM？", "ground_truth_contexts": ["For our RAG systems, we selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search, and ColBERTv2... and three different generative LLMs (MPT-7b-Instruct, GPT-3.5-Turbo, and GPT-4)."], "ground_truth_answer": "检索器：BM25、OpenAI Ada嵌入、ColBERTv2。生成LLM：MPT-7b-Instruct、GPT-3.5-Turbo、GPT-4。", "retrieved_contexts": ["framework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "vide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strat-\negy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM", "4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-\nnAI’s gpt-3.5-turbo-16k, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similar-\nity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses"], "generated_answer": "在真实RAG系统评估中，测试了以下检索器和生成LLM：\n\n### 检索器：\n1. **BM25**：这是一种传统的信息检索方法，基于词频和文档长度的统计模型。\n2. **DPR**（Dense Passage Retriever）：一种基于深度学习的密集检索模型，通过编码器将查询和文档映射到向量空间中进行匹配。\n3. **ColBERT**：一种高效的基于上下文的检索模型，通过细粒度的向量匹配提高检索质量。\n4. **混合检索器**（BM25+DPR）：结合BM25和DPR的优势，利用传统和深度学习方法进行检索。\n\n### 生成LLM：\n1. **Llama-2-7B**：这是Meta推出的Llama系列模型中的一个版本，具有70亿参数的中等规模模型。\n2. **Llama-2-13B**：Llama-2系列中更大版本，具有130亿参数，提供更强的生成能力。\n3. **Falcon-7B**：由TII开发的高性能开源语言模型，参数规模为70亿。\n4. **ChatGPT**：OpenAI开发的著名对话式AI模型，能够生成高质量的自然语言回复。\n\n这些模型的组合被用于测试不同的RAG系统设计，以评估其在不同场景下的性能表现。"}
{"question": "总结ARES在伪RAG系统排名上的结果。", "ground_truth_contexts": ["Table 1: ... Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets."], "ground_truth_answer": "ARES在Kendall’s τ上高于RAGAS（上下文相关性0.065，答案相关性0.132），并在使用78%更少标注的情况下优于采样标注。", "retrieved_contexts": ["ARES Ranking of Pseudo RAG Systems\nNQ HotpotQA WoW FEVER MultiRC ReCoRD\nC.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R. C.R A.R.\nKendall’s Tau for\nSampled Annotations 0.83 0.89 0.78 0.78 0.78 0.83 0.89 0.89 0.83 0.83 0.72 0.94\nKendall’s Tau\nfor RAGAS 0.89 0.89 0.94 0.89 0.94 0.94 0.72 0.61 0.83 0.94 0.89 0.44\nKendall’s Tau\nfor GPT-3.5 Judge 0.89 0.94 0.67 0.94 0.94 0.89 0.78 0.78 0.83 0.89 0.83 0.94\nKendall’s Tau for\nARES LLM Judge 0.89 1.0 0.89 0.94 0.94 1.0 0.83 0.72 0.94 0.83 0.78 0.83\nKendall’s Tau\nfor ARES 0.94 1.0 0.94 0.94 1.0 1.0 0.89 0.78 0.94 0.89 0.83 0.89\nRAGAS Accuracy 31.4% 71.2% 17.2% 76.0% 36.4% 77.8% 23.7% 69.2% 16.1% 75.0% 15.0% 72.8%\nGPT-3.5 Judge Accuracy 73.8% 95.5% 75.3% 71.6% 84.3% 85.2% 60.4% 59.6% 72.4% 60.3% 81.0% 65.8%\nARES Accuracy 79.3% 97.2% 92.3% 81.3% 85.7% 96.1% 88.4% 78.5% 85.8% 82.7% 67.8% 92.3%\nTable 1: ARES Ranking with Fine-tuned LLM Judges vs. Sampled Annotations, RAGAS and GPT-3.5 Judge:", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "Figure 2: RAG Systems Evaluation on NQ - Context Relevance\nFigure 3: RAG Systems Evaluation on NQ - Answer Relevance"], "generated_answer": "根据表格1中关于ARES在伪RAG系统排名上的结果，可以总结以下几点：\n\n1. **Kendall’s Tau相关性**：\n   - ARES表现优异，尤其是在与ARES LLM Judge结合时，许多数据集上的Kendall’s Tau达到了最高值（例如，NQ的Context Relevance为0.89，Answer Relevance为1.0）。\n   - ARES在多数数据集上的Kendall’s Tau值高于RAGAS和GPT-3.5 Judge，表明其排名与人工标注基准的排序更一致。\n\n2. **不同数据集的表现**：\n   - 在NQ数据集上，ARES在Context Relevance和Answer Relevance上的表现均优于其他方法。\n   - 在FEVER数据集上，ARES在Context Relevance上表现最好，而Answer Relevance表现略逊于某些方法。\n   - 在MultiRC和ReCoRD数据集上，ARES表现稳定，但略低于某些特定指标。\n\n3. **准确性（Accuracy）**：\n   - ARES在多数数据集上的准确性显著高于RAGAS和GPT-3.5 Judge。例如，在NQ数据集上，ARES的Context Relevance准确率达到79.3%，Answer Relevance达到97.2%。\n\n总体来说，ARES在伪RAG系统的排名任务中表现突出，尤其在与ARES LLM Judge结合使用时，能够更好地捕捉与人工标注一致的排序，并且在多个数据集上展现了更高的准确性。"}
{"question": "ARES中有效PPI所需的最少人工标注数量是多少？", "ground_truth_contexts": ["We used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3)."], "ground_truth_answer": "ARES中有效PPI至少需要150个人工标注。", "retrieved_contexts": ["Kendall’s Tau by Dataset\nNQ MultiRC ReCoRD\nPPI Labeled\nCount C.R. A.R. C.R. A.R. C.R. A.R.\n400 1.0 1.0 0.89 0.94 0.89 0.94\n300 0.89 1.0 0.94 0.89 0.83 0.89\n200 0.83 1.0 0.83 0.94 0.83 0.83\n150 0.72 1.0 0.83 0.89 0.72 0.83\n100 0.44 1.0 0.67 0.67 0.67 0.83\n50 0.44 0.94 0.61 0.44 0.56 0.67\n25 0.44 0.89 0.56 0.44 0.44 0.56\nTable 3: Analysis of PPI Labeled Count vs. ARES Efficacy by Kendall’s Tau: The Kendall’s tau values represent\nthe correlation between the correct ranking and the ARES ranking of the pseudo RAG systems. We use the same\nexperimental set-up as described in subsection 4.2. We find that below about 100-150 datapoints in the human\npreference validation set, ARES cannot meaningfully distinguish between the alternate RAG systems based on their\naccuracies in context relevance and answer relevance (C.R. and A.R., respectively).\nARES Ranking of Pseudo RAG Systems using GPT-4 Labels\nNQ ReCoRD MultiRC\nContext\nRelevance\nAnswer\nRelevance\nContext\nRelevance\nAnswer\nRelevance\nContext\nRelevance\nAnswer\nRelevance", "logits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n7 Limitations\nARES relies on a small set of annotations in the\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\noften require an annotator familiar with the RAG\nsystem’s domain application. While these annota-\ntions can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.", "puts from each RAG systems separately, which can\nbe costly both in terms of time and financing.\nTo combine the benefits of both, and hence\nboost the precision of the evaluation, ARES uses\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to predict the system scores. PPI is\na recent statistical method that provides tighter\nconfidence intervals on a small set of annotated\ndatapoints (i.e., our validation set) by leveraging\npredictions on a much larger set of non-annotated\ndatapoints. PPI can leverage both the labeled dat-\napoints and the ARES judge predictions on the\nnon-annotated datapoints to construct confidence\nintervals for our RAG system’s performance.\nTo do this, PPI uses the LLM judges on the hu-\nman preference validation set to learn a rectifier\nfunction for constructing a confidence set of the ML\nmodel’s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence", "For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation\nset. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nused ARES with human annotation sets ranging\nin size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).\n5.2 ARES Performance on AIS\nWoW CNN / DM\nARES Split Prediction 0.478 0.835\nCorrect Positive/Negative Split 0.458 0.859\nARES Judge Accuracy 62.5% 84.0%\nEvaluation Set Size 707 510\nHuman Preference Data Size 200 200\nTable 2: ARES Results on the AIS benchmark\nTo evaluate whether ARES can effectively gauge\nanswer faithfulness in real RAG systems, we tested\nARES on the AIS attribution benchmark (Rashkin\net al., 2022). In AIS, we selected the Wizards\nof Wikipedia (WoW) and CNN/DM datasets; the"], "generated_answer": "根据提供的材料，在讨论ARES中使用PPI（Prediction-Powered Inference）所需的最少人工标注数量时，明确提到以下信息：\n\n1. **实验结果**：通过使用不同大小的人工标注集（从25到400不等），研究发现**150个标注数据是最低要求**，以确保ARES能够有效地评估伪RAG系统的性能（见Table 3）。\n\n2. **进一步说明**：在100-150个数据点以下时，ARES无法有意义地区分不同RAG系统在上下文相关性和答案相关性方面的表现。\n\n因此，**ARES中有效使用PPI所需的最少人工标注数量是150**。"}
{"question": "ARES是否支持非英语语言的评估？", "ground_truth_contexts": ["While LLM judges in ARES were successful in cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages...", "To test cross-lingual transfer, we used the XGLUE datasets... a LLM judge fine-tuned on NQ achieved a Kendall’s tau of 0.33..."], "ground_truth_answer": "不，ARES无法很好地泛化到非英语语言，在XGLUE上Kendall’s τ较低（例如0.33）。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "et al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-\ntems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several asso-\nciated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\noffer little adaptability to new RAG evaluation set-", "framework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "根据提供的信息，目前 ARES 的评估仅限于英语数据集，因为其依赖于英语的 LLM（大语言模型）进行综合数据生成和评估。未来的工作计划探索如何在其他语言中使用 ARES，这需要利用针对这些语言的不同 LLM。因此，目前 ARES 不支持非英语语言的评估，但这是未来研究的一个方向。"}
{"question": "SuperGrok订阅的价格是多少？", "ground_truth_contexts": [], "ground_truth_answer": "文档中没有此信息。", "retrieved_contexts": ["uation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "preet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\nXiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su,\nand Huan Sun. 2023. Automatic evaluation of attri-\nbution by large language models.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. arXiv\npreprint arXiv:1810.12885.", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and", "For generating our synthetic answers, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nQuery: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nExample #2\nQuery: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nExample #3\nQuery: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nExample #4\nQuery: <synthetic query here>\nDocument: <in-domain passage here>\nAnswer:"], "generated_answer": "我不知道SuperGrok订阅的价格，因为这个信息没有在提供的上下文中出现。你可能需要访问相关的官方网站或联系客服获取最新的价格信息。"}
