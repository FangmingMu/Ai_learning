{"question": "ARES系统的全称是什么？", "ground_truth_contexts": ["我们引入了ARES，一个自动化RAG 评估系统，用于从上下文相关性、答案忠实度和答案相关性等维度评估RAG 系统。"], "ground_truth_answer": "ARES代表自动化RAG评估系统。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "性能开发出可靠的置信区间，这些置信区间优于以前的经典推断方法。 \n4. 实验 \n4.1 模型 \n对于我们的微调“裁判”，ARES 依赖于使用LLM 生成廉价但高质量的合成查询和答案。我们使用\nFLAN-T5 XXL 生成合成数据集，并选择DeBERTa-v3-Large 作为微调的LLM“裁判”。对于我\n们的上下文学习基线，我们使用OpenAI 的gpt-3.5-turbo-16k。 \n4.2 数据集 \n我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "为了快速准确地评估RAG 系统，我们提出了ARES，即自动化 RAG 评估系统。ARES 是首个为\nRAG 流程的每个组件生成定制化LLM“裁判”的自动化评估系统，与RAGAS 等现有方法相比，在\n评估精度和准确性上取得了显著提升。此外，与现有的RAG 评估系统不同，ARES 利用预测驱动\n的推断（PPI）为其评分提供置信区间。给定一个文档语料库和一个RAG 系统，ARES 会报告三\n个评估分数：上下文相关性（检索到的信息是否与测试问题相关）、答案忠实度（语言模型生成\n的响应是否恰当地基于检索到的上下文）以及答案相关性（响应是否也与问题相关）。一个好的", "建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下\n文相关性、答案忠实度和答案相关性）。这些“裁判”是针对对比学习目标进行微调的轻量级模\n型。第三，ARES 使用预测驱动的推断（PPI）对被评估的不同 RAG 系统进行评分，以提高基于\n模型的评估准确性，并为 RAG 评分提供统计置信区间。PPI 利用一小部分人工标注的数据点来\n计算其置信区间；我们将这个标注集指定为我们的人工偏好验证集，它由大约150 个或更多的标\n注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例\n子。"], "generated_answer": "ARES系统的全称是“Automated RAG Evaluation System”，即自动化RAG评估系统。"}
{"question": "ARES论文的作者是谁？", "ground_truth_contexts": ["作者信息： ● Jon Saad-Falcon, 斯坦福大学 (jonsaadfalcon@stanford.edu) ● Omar Khattab, 斯坦福大学 (okhattab@stanford.edu) ● Christopher Potts, 斯坦福大学 (cgpotts@stanford.edu) ● Matei Zaharia, Databricks and 加州大学伯克利分校 (matei@databricks.com)"], "ground_truth_answer": "作者是Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "性能开发出可靠的置信区间，这些置信区间优于以前的经典推断方法。 \n4. 实验 \n4.1 模型 \n对于我们的微调“裁判”，ARES 依赖于使用LLM 生成廉价但高质量的合成查询和答案。我们使用\nFLAN-T5 XXL 生成合成数据集，并选择DeBERTa-v3-Large 作为微调的LLM“裁判”。对于我\n们的上下文学习基线，我们使用OpenAI 的gpt-3.5-turbo-16k。 \n4.2 数据集 \n我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下\n文相关性、答案忠实度和答案相关性）。这些“裁判”是针对对比学习目标进行微调的轻量级模\n型。第三，ARES 使用预测驱动的推断（PPI）对被评估的不同 RAG 系统进行评分，以提高基于\n模型的评估准确性，并为 RAG 评分提供统计置信区间。PPI 利用一小部分人工标注的数据点来\n计算其置信区间；我们将这个标注集指定为我们的人工偏好验证集，它由大约150 个或更多的标\n注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例\n子。", "用来源提供一定程度的透明度或可解释性。 \n多种基于LLM 的评估技术已经出现，用于衡量LLM 系统的性能。这对于在难以从头开始构建传\n统基准数据集的新环境中快速部署至关重要。早期的尝试是直接使用开箱即用的LLM，如MT-\nBench 和Chatbot Arena。AutoCalibrate 试图将LLM“裁判”与人类偏好对齐，利用自我修正的\n提示来迭代改进LLM“裁判”。然而，AutoCalibrate 对其预测的准确性不提供任何统计保证。其\n他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。"], "generated_answer": "根据提供的上下文信息，无法确定ARES论文的具体作者名单。文中提到了ARES系统的设计和实现细节，但没有明确列出作者的名字。如果您需要了解具体的作者信息，建议查阅原始的ARES论文或相关的学术出版物。"}
{"question": "ARES报告的三个评估分数是什么？", "ground_truth_contexts": ["ARES 会报告三个评估分数：上下文相关性（检索到的信息是否与测试问题相关）、答案忠实度（语言模型生成的响应是否恰当地基于检索到的上下文）以及答案相关性（响应是否也与问题相关）。"], "ground_truth_answer": "三个评估分数是上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "为了快速准确地评估RAG 系统，我们提出了ARES，即自动化 RAG 评估系统。ARES 是首个为\nRAG 流程的每个组件生成定制化LLM“裁判”的自动化评估系统，与RAGAS 等现有方法相比，在\n评估精度和准确性上取得了显著提升。此外，与现有的RAG 评估系统不同，ARES 利用预测驱动\n的推断（PPI）为其评分提供置信区间。给定一个文档语料库和一个RAG 系统，ARES 会报告三\n个评估分数：上下文相关性（检索到的信息是否与测试问题相关）、答案忠实度（语言模型生成\n的响应是否恰当地基于检索到的上下文）以及答案相关性（响应是否也与问题相关）。一个好的", "集型任务中，ARES 在评估过程中仅使用数百个人工标注，便能准确地评估RAG 系统。此外，即\n使在被评估的RAG 系统中所使用的查询和/或文档类型发生变化，ARES 的“裁判”模型在领域迁\n移后依然保持有效和准确。我们的代码和数据集已在Github 上公开。 \n1. 引言 \n对于不同的数据领域、语料库大小以及成本/延迟预算，RAG 系统的最佳设计并非普遍适用。为\n了调整自己的RAG 系统，从业者传统上需要针对其目标领域手动标注测试问题、待检索的段落\n（用于评估检索器）以及待生成的响应。或者，他们也可以通过收集用户偏好来在生产环境中评", "性能开发出可靠的置信区间，这些置信区间优于以前的经典推断方法。 \n4. 实验 \n4.1 模型 \n对于我们的微调“裁判”，ARES 依赖于使用LLM 生成廉价但高质量的合成查询和答案。我们使用\nFLAN-T5 XXL 生成合成数据集，并选择DeBERTa-v3-Large 作为微调的LLM“裁判”。对于我\n们的上下文学习基线，我们使用OpenAI 的gpt-3.5-turbo-16k。 \n4.2 数据集 \n我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档"], "generated_answer": "ARES报告的三个评估分数是：\n\n1. **上下文相关性**（Context Relevance）：检索到的信息是否与测试问题相关。\n2. **答案忠实度**（Answer Faithfulness）：语言模型生成的响应是否恰当地基于检索到的上下文。\n3. **答案相关性**（Answer Relevance）：响应是否也与问题相关。"}
{"question": "ARES如何生成用于训练判断器的合成数据？", "ground_truth_contexts": ["给定领域内段落的语料库，ARES 分三个阶段进行。首先，它利用一个LM 从语料库的段落中构建一个合成的问答对数据集。", "我们使用生成式LLM 从语料库段落中生成合成的查询和答案。生成的数据代表了查询-段落-答案三元组的正面和负面示例（例如，相关/不相关的段落和正确/不正确的答案）。", "为了创建我们的合成数据，我们主要使用FLAN-T5 XXL。然后，我们通过测试给定查询是否能使用其检索器将其原始段落作为首要结果检索出来，来过滤掉低质量的查询。"], "ground_truth_answer": "ARES使用像FLAN-T5 XXL这样的语言模型从领域内段落生成合成问题-答案对，包括正面和负面示例，并使用检索器过滤低质量查询。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "性能开发出可靠的置信区间，这些置信区间优于以前的经典推断方法。 \n4. 实验 \n4.1 模型 \n对于我们的微调“裁判”，ARES 依赖于使用LLM 生成廉价但高质量的合成查询和答案。我们使用\nFLAN-T5 XXL 生成合成数据集，并选择DeBERTa-v3-Large 作为微调的LLM“裁判”。对于我\n们的上下文学习基线，我们使用OpenAI 的gpt-3.5-turbo-16k。 \n4.2 数据集 \n我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档", "区间。 \n3.1 LLM 生成合成数据集 \n我们使用生成式LLM 从语料库段落中生成合成的查询和答案。生成的数据代表了查询-段落-答案\n三元组的正面和负面示例（例如，相关/不相关的段落和正确/不正确的答案）。在生成过程中，\nLLM 使用我们输入的少样本示例集，其中领域内段落映射到领域内查询和答案；然后模型从给定\n的领域内段落生成一个合成的问题和答案，使我们能够创建正面和负面的训练示例。 \n为了创建我们的合成数据，我们主要使用FLAN-T5 XXL。ARES 与这个模型配合得很好，但我们\n的系统最终也可以使用其他高质量模型来生成合成的查询和答案。然后，我们通过测试给定查询", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "ARES 对每个RAG 方法产生的领域内查询-文档-答案三元组进行抽样，然后由“裁判”对每个三元\n组进行标注，预测其上下文相关性、答案忠实度和答案相关性。通过平均每个领域内三元组的单\n个预测标签，我们计算出RAG 系统在三个指标上的性能。 \n原则上，我们可以简单地将这些平均分数作为每个RAG 系统的质量指标报告。然而，这些分数\n反映的是完全未标注的数据，预测来自一个合成训练的LLM“裁判”，因此它们可能不完全准确。\n作为一种极端的替代方案，我们可以仅使用之前讨论的小型人工偏好验证集进行评估。然而，基\n于标注的评估方法需要对每个RAG 系统分别标注更多的生成输出，这在时间和资金上都可能成"], "generated_answer": "ARES 通过以下步骤生成用于训练判断器的合成数据：\n\n1. **使用生成式大语言模型（LLM）**：  \n   ARES 主要使用 **FLAN-T5 XXL** 模型来生成合成的查询和答案。该模型根据输入的段落自动生成与之对应的查询和答案。\n\n2. **生成正负样本三元组**：  \n   合成数据包括查询-段落-答案三元组的**正样本**和**负样本**：\n   - **正样本**：与查询相关的段落以及正确的答案。\n   - **负样本**：\n     - 对于**上下文相关性**，从与黄金段落相同的文档中随机抽取其他段落作为负样本。\n     - 对于**答案忠实度**和**答案相关性**，通过提示 FLAN-T5 XXL 生成一个与正确答案矛盾的答案作为负样本。\n\n3. **少样本示例引导生成过程**：  \n   在生成过程中，LLM 接收一组**少样本示例**作为输入，这些示例展示了如何将领域内的段落映射到领域内的查询和答案。基于这些示例，模型学会从给定段落中生成相应的合成问题和答案。\n\n4. **构建训练数据集**：  \n   利用上述方法，ARES 能够创建大量高质量的正负样本，从而形成一个可用于微调判断器的合成数据集。\n\n总结来说，ARES 依靠 FLAN-T5 XXL 和少样本提示策略来自动生成多样化的查询-段落-答案三元组，并明确区分正负样本，以此构建用于训练判断器的合成数据集。"}
{"question": "预测驱动推理（PPI）在ARES中的作用是什么？", "ground_truth_contexts": ["第三，ARES 使用预测驱动的推断（PPI）对被评估的不同RAG 系统进行评分，以提高基于模型的评估准确性，并为RAG 评分提供统计置信区间。PPI 利用一小部分人工标注的数据点来计算其置信区间；我们将这个标注集指定为我们的人工偏好验证集，它由大约150 个或更多的标注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例子。"], "ground_truth_answer": "PPI通过利用一小部分人工标注数据点来提高基于模型的评估准确性，为分数提供置信区间，结合标注和未标注预测。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "性能开发出可靠的置信区间，这些置信区间优于以前的经典推断方法。 \n4. 实验 \n4.1 模型 \n对于我们的微调“裁判”，ARES 依赖于使用LLM 生成廉价但高质量的合成查询和答案。我们使用\nFLAN-T5 XXL 生成合成数据集，并选择DeBERTa-v3-Large 作为微调的LLM“裁判”。对于我\n们的上下文学习基线，我们使用OpenAI 的gpt-3.5-turbo-16k。 \n4.2 数据集 \n我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档", "集型任务中，ARES 在评估过程中仅使用数百个人工标注，便能准确地评估RAG 系统。此外，即\n使在被评估的RAG 系统中所使用的查询和/或文档类型发生变化，ARES 的“裁判”模型在领域迁\n移后依然保持有效和准确。我们的代码和数据集已在Github 上公开。 \n1. 引言 \n对于不同的数据领域、语料库大小以及成本/延迟预算，RAG 系统的最佳设计并非普遍适用。为\n了调整自己的RAG 系统，从业者传统上需要针对其目标领域手动标注测试问题、待检索的段落\n（用于评估检索器）以及待生成的响应。或者，他们也可以通过收集用户偏好来在生产环境中评"], "generated_answer": "根据提供的上下文信息，没有直接提到\"预测驱动推理（PPI）\"在ARES中的具体作用。因此，我无法基于现有信息准确回答这个问题。\n\n如果您有更多关于PPI在ARES中作用的具体信息或上下文，欢迎提供，我将很乐意为您解答。"}
{"question": "ARES在评估过程中通常需要多少人工标注？", "ground_truth_contexts": ["ARES 通过仅需三个输入显著提高了评估时的数据效率：一个领域内段落集，一个约150 个或更多已标注数据点的人工偏好验证集，以及少量领域内查询和答案的少样本示例（例如五个或更多示例），这些示例用于在合成数据生成中提示LLM。"], "ground_truth_answer": "ARES需要大约150个或更多人工标注数据点在人类偏好验证集中。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "ARES：一种用于检索增强生成系统的自动\n化评估框架 \n作者信息： \n● Jon Saad-Falcon, 斯坦福大学 (jonsaadfalcon@stanford.edu) \n● Omar Khattab, 斯坦福大学 (okhattab@stanford.edu) \n● Christopher Potts, 斯坦福大学 (cgpotts@stanford.edu) \n● Matei Zaharia, Databricks and 加州大学伯克利分校 (matei@databricks.com) \n摘要 \n评估检索增强生成（RAG）系统传统上依赖于对输入查询、待检索段落和待生成响应的手动标", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "集型任务中，ARES 在评估过程中仅使用数百个人工标注，便能准确地评估RAG 系统。此外，即\n使在被评估的RAG 系统中所使用的查询和/或文档类型发生变化，ARES 的“裁判”模型在领域迁\n移后依然保持有效和准确。我们的代码和数据集已在Github 上公开。 \n1. 引言 \n对于不同的数据领域、语料库大小以及成本/延迟预算，RAG 系统的最佳设计并非普遍适用。为\n了调整自己的RAG 系统，从业者传统上需要针对其目标领域手动标注测试问题、待检索的段落\n（用于评估检索器）以及待生成的响应。或者，他们也可以通过收集用户偏好来在生产环境中评", "ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。"], "generated_answer": "根据提供的文本，ARES在评估过程中通常只需要**数百个人工标注**。具体来说，文本中提到：“ARES 在评估过程中仅使用数百个人工标注，便能准确地评估RAG 系统。” 这表明其评估过程相对高效，不需要大量的手动标注数据。"}
{"question": "ARES中使用哪些模型进行合成数据生成和判断器微调？", "ground_truth_contexts": ["为了创建我们的合成数据，我们主要使用FLAN-T5 XXL。", "为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁判”，以评估三种不同的能力。"], "ground_truth_answer": "FLAN-T5 XXL用于合成数据生成，DeBERTa-v3-Large用于微调LLM判断器。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "性能开发出可靠的置信区间，这些置信区间优于以前的经典推断方法。 \n4. 实验 \n4.1 模型 \n对于我们的微调“裁判”，ARES 依赖于使用LLM 生成廉价但高质量的合成查询和答案。我们使用\nFLAN-T5 XXL 生成合成数据集，并选择DeBERTa-v3-Large 作为微调的LLM“裁判”。对于我\n们的上下文学习基线，我们使用OpenAI 的gpt-3.5-turbo-16k。 \n4.2 数据集 \n我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下\n文相关性、答案忠实度和答案相关性）。这些“裁判”是针对对比学习目标进行微调的轻量级模\n型。第三，ARES 使用预测驱动的推断（PPI）对被评估的不同 RAG 系统进行评分，以提高基于\n模型的评估准确性，并为 RAG 评分提供统计置信区间。PPI 利用一小部分人工标注的数据点来\n计算其置信区间；我们将这个标注集指定为我们的人工偏好验证集，它由大约150 个或更多的标\n注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例\n子。", "为了快速准确地评估RAG 系统，我们提出了ARES，即自动化 RAG 评估系统。ARES 是首个为\nRAG 流程的每个组件生成定制化LLM“裁判”的自动化评估系统，与RAGAS 等现有方法相比，在\n评估精度和准确性上取得了显著提升。此外，与现有的RAG 评估系统不同，ARES 利用预测驱动\n的推断（PPI）为其评分提供置信区间。给定一个文档语料库和一个RAG 系统，ARES 会报告三\n个评估分数：上下文相关性（检索到的信息是否与测试问题相关）、答案忠实度（语言模型生成\n的响应是否恰当地基于检索到的上下文）以及答案相关性（响应是否也与问题相关）。一个好的"], "generated_answer": "在ARES中：\n\n- **合成数据生成**：使用 **FLAN-T5 XXL** 模型来生成合成的问答对数据集。\n- **判断器微调**：使用 **DeBERTa-v3-Large** 模型作为基础模型，对其进行微调以构建用于评估的“裁判”模型。"}
{"question": "总结ARES管道的三个阶段。", "ground_truth_contexts": ["ARES 分三个阶段进行（见图1）。... 首先，它利用一个LM 从语料库的段落中构建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下文相关性、答案忠实度和答案相关性）。第三，ARES 使用预测驱动的推断（PPI）对被评估的不同RAG 系统进行评分。"], "ground_truth_answer": "阶段1：从语料段落生成合成查询和答案。阶段2：为上下文相关性、答案忠实度和答案相关性微调LLM判断器。阶段3：使用判断器和PPI为RAG系统评分以获得置信区间。", "retrieved_contexts": ["ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "与基于标注的评估方法相比，ARES 的准确性和效率显著更高，比基线方法所需的标注量减少了\n78%。我们还发现，ARES 能够持续地区分那些在真实指标上仅相差几个百分点的有竞争力的\nRAG 系统。这种精确性使ARES 能够指导有竞争力的方案和配置的开发与比较。 \n我们的ARES 代码和数据集已在Github 上公开。 \n2. 相关工作 \n检索增强生成（RAG）现在是通过将大语言模型（LLMs）与检索系统相结合来增强其能力的常用\n策略。通过检索，RAG 帮助 LM 系统收集特定领域的知识，将生成内容基于事实信息，并通过引\n用来源提供一定程度的透明度或可解释性。", "用来源提供一定程度的透明度或可解释性。 \n多种基于LLM 的评估技术已经出现，用于衡量LLM 系统的性能。这对于在难以从头开始构建传\n统基准数据集的新环境中快速部署至关重要。早期的尝试是直接使用开箱即用的LLM，如MT-\nBench 和Chatbot Arena。AutoCalibrate 试图将LLM“裁判”与人类偏好对齐，利用自我修正的\n提示来迭代改进LLM“裁判”。然而，AutoCalibrate 对其预测的准确性不提供任何统计保证。其\n他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "ARES：一种用于检索增强生成系统的自动\n化评估框架 \n作者信息： \n● Jon Saad-Falcon, 斯坦福大学 (jonsaadfalcon@stanford.edu) \n● Omar Khattab, 斯坦福大学 (okhattab@stanford.edu) \n● Christopher Potts, 斯坦福大学 (cgpotts@stanford.edu) \n● Matei Zaharia, Databricks and 加州大学伯克利分校 (matei@databricks.com) \n摘要 \n评估检索增强生成（RAG）系统传统上依赖于对输入查询、待检索段落和待生成响应的手动标"], "generated_answer": "根据提供的文本，我无法准确总结ARES管道的三个具体阶段。文中提到了ARES是一个用于检索增强生成系统的自动化评估框架，并讨论了其优势（如减少78%的标注需求、能够区分竞争力强的RAG系统等），但没有详细描述该框架的具体三个阶段。\n\n如果您需要了解ARES管道的具体阶段，建议查阅论文的完整内容或相关技术文档。"}
{"question": "比较ARES和RAGAS在上下文相关性和答案相关性方面的性能。", "ground_truth_contexts": ["在几乎所有来自KILT 和SuperGLUE 数据集的设置中，ARES 提供的RAG 系统排名都比RAGAS更准确。ARES 的肯德尔τ 在上下文相关性上平均高出0.065，在答案相关性上平均高出0.132。与RAGAS 相比，ARES 的LLM“裁判”在预测上下文相关性和答案相关性方面也明显更准确，准确率分别高出59.9 和14.4 个百分点。"], "ground_truth_answer": "ARES在Kendall’s τ上优于RAGAS（上下文相关性0.065，答案相关性0.132），并在准确性上更高（上下文相关性59.9%点，答案相关性14.4%点）。", "retrieved_contexts": ["ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "与基于标注的评估方法相比，ARES 的准确性和效率显著更高，比基线方法所需的标注量减少了\n78%。我们还发现，ARES 能够持续地区分那些在真实指标上仅相差几个百分点的有竞争力的\nRAG 系统。这种精确性使ARES 能够指导有竞争力的方案和配置的开发与比较。 \n我们的ARES 代码和数据集已在Github 上公开。 \n2. 相关工作 \n检索增强生成（RAG）现在是通过将大语言模型（LLMs）与检索系统相结合来增强其能力的常用\n策略。通过检索，RAG 帮助 LM 系统收集特定领域的知识，将生成内容基于事实信息，并通过引\n用来源提供一定程度的透明度或可解释性。", "用来源提供一定程度的透明度或可解释性。 \n多种基于LLM 的评估技术已经出现，用于衡量LLM 系统的性能。这对于在难以从头开始构建传\n统基准数据集的新环境中快速部署至关重要。早期的尝试是直接使用开箱即用的LLM，如MT-\nBench 和Chatbot Arena。AutoCalibrate 试图将LLM“裁判”与人类偏好对齐，利用自我修正的\n提示来迭代改进LLM“裁判”。然而，AutoCalibrate 对其预测的准确性不提供任何统计保证。其\n他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。", "建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下\n文相关性、答案忠实度和答案相关性）。这些“裁判”是针对对比学习目标进行微调的轻量级模\n型。第三，ARES 使用预测驱动的推断（PPI）对被评估的不同 RAG 系统进行评分，以提高基于\n模型的评估准确性，并为 RAG 评分提供统计置信区间。PPI 利用一小部分人工标注的数据点来\n计算其置信区间；我们将这个标注集指定为我们的人工偏好验证集，它由大约150 个或更多的标\n注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例\n子。", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。"], "generated_answer": "根据提供的上下文信息，没有直接提到 RAGAS 或其与 ARES 在上下文相关性和答案相关性方面的性能比较。因此，无法基于现有信息对这两种方法进行比较。\n\n如果你有更多具体的信息或者需要了解 ARES 的性能细节，可以参考以下总结：\n\n- **ARES** 是一种用于评估检索增强生成（RAG）系统的框架。\n- 它使用少量人工标注数据来训练轻量级“裁判”模型，这些模型专门负责评估：\n  - 上下文相关性（Context Relevance）\n  - 答案忠实度（Answer Faithfulness）\n  - 答案相关性（Answer Relevance）\n- ARES 使用预测驱动的推断（PPI）提升评估准确性，并能为评分提供统计置信区间。\n- 实验表明，ARES 比传统基线方法更高效和准确，所需的人工标注量减少 78%。\n\n如果想比较 ARES 和 RAGAS，建议查阅 RAGAS 的相关文献或实验报告以获取其在相同指标上的表现数据。"}
{"question": "用于评估ARES的KILT和SuperGLUE数据集有哪些？", "ground_truth_contexts": ["我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档和答案，我们从广泛使用的KILT 和SuperGLUE 基准测试中选择了所有适合RAG 的数据集。 ● KILT: Natural Questions (NQ), HotpotQA, FEVER, 和 Wizards of Wikipedia (WoW)。 ● SuperGLUE: MultiRC 和 ReCoRD。"], "ground_truth_answer": "KILT数据集：Natural Questions (NQ)、HotpotQA、FEVER、Wizards of Wikipedia (WoW)。SuperGLUE数据集：MultiRC和ReCoRD。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "ARES：一种用于检索增强生成系统的自动\n化评估框架 \n作者信息： \n● Jon Saad-Falcon, 斯坦福大学 (jonsaadfalcon@stanford.edu) \n● Omar Khattab, 斯坦福大学 (okhattab@stanford.edu) \n● Christopher Potts, 斯坦福大学 (cgpotts@stanford.edu) \n● Matei Zaharia, Databricks and 加州大学伯克利分校 (matei@databricks.com) \n摘要 \n评估检索增强生成（RAG）系统传统上依赖于对输入查询、待检索段落和待生成响应的手动标", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "集型任务中，ARES 在评估过程中仅使用数百个人工标注，便能准确地评估RAG 系统。此外，即\n使在被评估的RAG 系统中所使用的查询和/或文档类型发生变化，ARES 的“裁判”模型在领域迁\n移后依然保持有效和准确。我们的代码和数据集已在Github 上公开。 \n1. 引言 \n对于不同的数据领域、语料库大小以及成本/延迟预算，RAG 系统的最佳设计并非普遍适用。为\n了调整自己的RAG 系统，从业者传统上需要针对其目标领域手动标注测试问题、待检索的段落\n（用于评估检索器）以及待生成的响应。或者，他们也可以通过收集用户偏好来在生产环境中评"], "generated_answer": "根据提供的上下文信息，文中提到了KILT和SuperGLUE数据集是用于评估ARES框架的一部分。然而，具体说明这些数据集中包含哪些子任务或领域的细节并未在所提供的文本中给出。\n\n通常情况下：\n\n- **KILT**（Knowledge Intensive Language Tasks）是一个基准测试套件，旨在评估需要外部知识的语言任务。它包括多个数据集，如TriviaQA、Natural Questions、ELI5等，它们都要求模型访问外部知识源以生成答案。\n  \n- **SuperGLUE** 是一个用于评估自然语言理解系统的 benchmark，包含了多种复杂的语言理解任务，比如BoolQ、CB、COPA、MultiRC、ReCoRD、RTE、WiC 和 WSC。\n\n但请注意，上述关于KILT和SuperGLUE具体内容的描述基于一般认知，并非直接来源于您提供的文档片段。如果想要了解该研究中如何具体使用这两个数据集进行ARES评估，则需查阅原文献或更多详细资料。"}
{"question": "ARES如何处理合成数据生成中的负面示例？", "ground_truth_contexts": ["为了生成用于微调我们LLM“裁判”的负样本，我们依赖两种新颖的策略，每种策略生成相同数量的负样本： ● 弱负样本生成：对于上下文相关性的负样本，我们随机抽样与给定合成查询无关的领域内段落。对于答案忠实度和答案相关性的负样本，我们从其他段落中随机抽样合成生成的答案。 ● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的答案。"], "ground_truth_answer": "ARES使用弱负面（随机采样）和强负面（从相似段落采样或生成矛盾答案）用于上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "性能开发出可靠的置信区间，这些置信区间优于以前的经典推断方法。 \n4. 实验 \n4.1 模型 \n对于我们的微调“裁判”，ARES 依赖于使用LLM 生成廉价但高质量的合成查询和答案。我们使用\nFLAN-T5 XXL 生成合成数据集，并选择DeBERTa-v3-Large 作为微调的LLM“裁判”。对于我\n们的上下文学习基线，我们使用OpenAI 的gpt-3.5-turbo-16k。 \n4.2 数据集 \n我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "RAG 系统能够找到相关的上下文，并生成既忠实又相关的答案。 \n许多现有的RAG 评估框架需要大量的人工标注来进行评分。ARES 通过仅需三个输入显著提高了\n评估时的数据效率：一个领域内段落集，一个约150 个或更多已标注数据点的人工偏好验证集，\n以及少量领域内查询和答案的少样本示例（例如五个或更多示例），这些示例用于在合成数据生\n成中提示LLM。 \n给定领域内段落的语料库，ARES 分三个阶段进行。首先，它利用一个LM 从语料库的段落中构\n建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下", "注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例\n子。 \n我们进行了广泛的实证评估，证明ARES 在 KILT 和 SuperGLUE 的六个知识密集型数据集上准确\n地对 RAG 系统进行评分，在上下文相关性和答案相关性评估准确性上，平均分别比现有的自动\n化评估方法（如RAGAS）高出 59.3 和14.4 个百分点。此外，ARES 在 AIS 归因数据集中准确计\n算了答案幻觉的出现次数，预测的答案幻觉平均值与真实值的差距在2.5 个百分点以内。 \n与基于标注的评估方法相比，ARES 的准确性和效率显著更高，比基线方法所需的标注量减少了"], "generated_answer": "在ARES中，合成数据生成中的负面示例（负样本）是通过以下方式处理的：\n\n1. **上下文相关性的负样本**：\n   - 对于评估上下文相关性的负样本，ARES从与黄金段落相同的文档中随机抽取领域内的其他段落作为负样本。这些段落虽然来自同一文档，但与查询不直接相关，从而构成负例。\n\n2. **答案忠实度和答案相关性的负样本**：\n   - 对于评估答案忠实度和答案相关性的负样本，ARES使用FLAN-T5 XXL模型生成一个与正确答案相矛盾的答案。这种生成的矛盾答案被用作负样本，以帮助训练模型识别不忠实或不相关的答案。\n\n总结来说，ARES通过两种主要策略生成负样本：一种是通过从相同文档中采样非相关段落来构建上下文相关性的负样本；另一种是通过提示大模型（如FLAN-T5 XXL）生成矛盾答案来构建答案忠实度和答案相关性的负样本。这样可以确保生成的合成数据集中包含具有挑战性和多样性的负样本，有助于提高评估模型的鲁棒性。"}
{"question": "ARES实验中使用的置信区间概率是多少？", "ground_truth_contexts": ["此外，PPI 允许我们以选定的概率水平估计置信区间；在我们的实验中，我们使用标准的95% alpha（概率）作为我们的置信区间。"], "ground_truth_answer": "ARES在其实验中使用95%的置信区间概率。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "集型任务中，ARES 在评估过程中仅使用数百个人工标注，便能准确地评估RAG 系统。此外，即\n使在被评估的RAG 系统中所使用的查询和/或文档类型发生变化，ARES 的“裁判”模型在领域迁\n移后依然保持有效和准确。我们的代码和数据集已在Github 上公开。 \n1. 引言 \n对于不同的数据领域、语料库大小以及成本/延迟预算，RAG 系统的最佳设计并非普遍适用。为\n了调整自己的RAG 系统，从业者传统上需要针对其目标领域手动标注测试问题、待检索的段落\n（用于评估检索器）以及待生成的响应。或者，他们也可以通过收集用户偏好来在生产环境中评", "ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "用来源提供一定程度的透明度或可解释性。 \n多种基于LLM 的评估技术已经出现，用于衡量LLM 系统的性能。这对于在难以从头开始构建传\n统基准数据集的新环境中快速部署至关重要。早期的尝试是直接使用开箱即用的LLM，如MT-\nBench 和Chatbot Arena。AutoCalibrate 试图将LLM“裁判”与人类偏好对齐，利用自我修正的\n提示来迭代改进LLM“裁判”。然而，AutoCalibrate 对其预测的准确性不提供任何统计保证。其\n他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。"], "generated_answer": "根据提供的上下文信息，没有明确提到ARES实验中使用的置信区间概率具体是多少。因此，我无法给出确切的答案。如果你需要这个具体的数值，建议查阅原始论文或相关技术文档以获取更详细的信息。"}
{"question": "分析ARES在跨域应用中的优势。", "ground_truth_contexts": ["ARES 中使用的LLM“裁判”在跨领域应用中表现出强大的泛化能力，即使在查询类型、文档类型或两者都发生变化时也是如此。然而，当领域发生更剧烈的变化时，例如语言切换（如英语到西班牙语）、从文本到代码的转换，或从检索文本到提取实体，LLM“裁判”的泛化能力会下降。"], "ground_truth_answer": "ARES判断器在KILT和SuperGLUE的查询和文档类型上显示出强大的泛化能力，PPI缓解准确性损失，但在语言、代码或实体提取等剧烈转变中失败。", "retrieved_contexts": ["ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "用来源提供一定程度的透明度或可解释性。 \n多种基于LLM 的评估技术已经出现，用于衡量LLM 系统的性能。这对于在难以从头开始构建传\n统基准数据集的新环境中快速部署至关重要。早期的尝试是直接使用开箱即用的LLM，如MT-\nBench 和Chatbot Arena。AutoCalibrate 试图将LLM“裁判”与人类偏好对齐，利用自我修正的\n提示来迭代改进LLM“裁判”。然而，AutoCalibrate 对其预测的准确性不提供任何统计保证。其\n他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "与基于标注的评估方法相比，ARES 的准确性和效率显著更高，比基线方法所需的标注量减少了\n78%。我们还发现，ARES 能够持续地区分那些在真实指标上仅相差几个百分点的有竞争力的\nRAG 系统。这种精确性使ARES 能够指导有竞争力的方案和配置的开发与比较。 \n我们的ARES 代码和数据集已在Github 上公开。 \n2. 相关工作 \n检索增强生成（RAG）现在是通过将大语言模型（LLMs）与检索系统相结合来增强其能力的常用\n策略。通过检索，RAG 帮助 LM 系统收集特定领域的知识，将生成内容基于事实信息，并通过引\n用来源提供一定程度的透明度或可解释性。", "建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下\n文相关性、答案忠实度和答案相关性）。这些“裁判”是针对对比学习目标进行微调的轻量级模\n型。第三，ARES 使用预测驱动的推断（PPI）对被评估的不同 RAG 系统进行评分，以提高基于\n模型的评估准确性，并为 RAG 评分提供统计置信区间。PPI 利用一小部分人工标注的数据点来\n计算其置信区间；我们将这个标注集指定为我们的人工偏好验证集，它由大约150 个或更多的标\n注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例\n子。"], "generated_answer": "根据提供的文本，ARES在跨域应用中具有以下潜在优势：\n\n1. **减少人工标注依赖**：ARES只需要一小部分人工标注数据（约150个或更多）来构建人工偏好验证集，相比传统方法可以减少78%的标注需求。这使得在新领域部署时，能够降低获取领域专家进行标注的成本和时间。\n\n2. **快速部署能力**：由于ARES利用合成数据和LLM进行评估，它可以在难以从头开始构建传统基准数据集的新环境（包括新的领域）中快速部署。\n\n3. **提供统计保证**：ARES使用预测驱动的推断(PPI)方法为RAG评分提供统计置信区间，这增加了评估结果的可信度和可解释性。\n\n4. **高精度区分能力**：ARES能够持续区分在真实指标上仅有微小差异的竞争性RAG系统，这表明其评估具有较高的敏感性和准确性，有助于在不同领域中选择最优配置。\n\n不过需要注意的是，文本也提到了ARES的一些限制，比如依赖于具备专业知识的标注员以及需要大量GPU资源的LLM，这些因素可能会对其在某些特定跨域场景下的应用造成挑战。"}
{"question": "论文中提到的ARES的局限性是什么？", "ground_truth_contexts": ["ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。"], "ground_truth_answer": "局限性包括需要领域专家标注、需要GPU硬件，并且仅在英文数据集上评估。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "性能开发出可靠的置信区间，这些置信区间优于以前的经典推断方法。 \n4. 实验 \n4.1 模型 \n对于我们的微调“裁判”，ARES 依赖于使用LLM 生成廉价但高质量的合成查询和答案。我们使用\nFLAN-T5 XXL 生成合成数据集，并选择DeBERTa-v3-Large 作为微调的LLM“裁判”。对于我\n们的上下文学习基线，我们使用OpenAI 的gpt-3.5-turbo-16k。 \n4.2 数据集 \n我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档", "RAG 系统能够找到相关的上下文，并生成既忠实又相关的答案。 \n许多现有的RAG 评估框架需要大量的人工标注来进行评分。ARES 通过仅需三个输入显著提高了\n评估时的数据效率：一个领域内段落集，一个约150 个或更多已标注数据点的人工偏好验证集，\n以及少量领域内查询和答案的少样本示例（例如五个或更多示例），这些示例用于在合成数据生\n成中提示LLM。 \n给定领域内段落的语料库，ARES 分三个阶段进行。首先，它利用一个LM 从语料库的段落中构\n建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下", "用来源提供一定程度的透明度或可解释性。 \n多种基于LLM 的评估技术已经出现，用于衡量LLM 系统的性能。这对于在难以从头开始构建传\n统基准数据集的新环境中快速部署至关重要。早期的尝试是直接使用开箱即用的LLM，如MT-\nBench 和Chatbot Arena。AutoCalibrate 试图将LLM“裁判”与人类偏好对齐，利用自我修正的\n提示来迭代改进LLM“裁判”。然而，AutoCalibrate 对其预测的准确性不提供任何统计保证。其\n他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。"], "generated_answer": "根据提供的上下文信息，论文中并没有直接提及ARES的具体局限性。不过，我们可以推断一些可能的局限性，基于ARES的设计和实现方式：\n\n1. **依赖高质量的合成数据**：ARES的有效性部分取决于合成数据的质量。如果合成数据不能很好地代表真实世界的数据分布，那么训练出来的“裁判”模型可能无法准确评估实际应用中的RAG系统。\n\n2. **需要人工偏好的验证集**：虽然ARES减少了对大规模人工标注的需求，但它仍然需要一个小的人工偏好验证集（大约150个或更多的已标注数据点）来确保其评估的准确性。这表明它并非完全不需要人工干预。\n\n3. **提示工程的重要性**：ARES使用少量示例来指导LLM生成合成数据。这意味着提示工程的质量会直接影响到生成数据的质量，进而影响最终评估的效果。\n\n4. **模型选择的影响**：ARES选择了特定的模型（如FLAN-T5 XXL 和 DeBERTa-v3-Large）来进行数据生成和微调。不同的基础模型可能会导致不同的结果，因此模型的选择也可能是其局限性之一。\n\n请注意，以上内容是基于现有信息做出的合理推测，并不代表论文明确指出的局限性。要了解具体的局限性，最好查阅完整的论文内容。"}
{"question": "ARES在AIS基准上的答案忠实度表现如何？", "ground_truth_contexts": ["为了评估ARES 是否能有效衡量真实RAG 系统中的答案忠实度，我们在AIS 归因基准上测试了ARES。结果表明，ARES 能够有效评分AIS 数据集，与正确分数的差距在2.5 个准确率点以内，证明了其区分忠实答案和幻觉答案的能力。"], "ground_truth_answer": "ARES在AIS基准上对答案幻觉的预测在真实平均值的2.5个百分点以内。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "性能开发出可靠的置信区间，这些置信区间优于以前的经典推断方法。 \n4. 实验 \n4.1 模型 \n对于我们的微调“裁判”，ARES 依赖于使用LLM 生成廉价但高质量的合成查询和答案。我们使用\nFLAN-T5 XXL 生成合成数据集，并选择DeBERTa-v3-Large 作为微调的LLM“裁判”。对于我\n们的上下文学习基线，我们使用OpenAI 的gpt-3.5-turbo-16k。 \n4.2 数据集 \n我们的核心实验目标是全面展示ARES 可以有效应用的场景。为了测试跨多种类型的查询、文档", "建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下\n文相关性、答案忠实度和答案相关性）。这些“裁判”是针对对比学习目标进行微调的轻量级模\n型。第三，ARES 使用预测驱动的推断（PPI）对被评估的不同 RAG 系统进行评分，以提高基于\n模型的评估准确性，并为 RAG 评分提供统计置信区间。PPI 利用一小部分人工标注的数据点来\n计算其置信区间；我们将这个标注集指定为我们的人工偏好验证集，它由大约150 个或更多的标\n注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例\n子。", "ARES：一种用于检索增强生成系统的自动\n化评估框架 \n作者信息： \n● Jon Saad-Falcon, 斯坦福大学 (jonsaadfalcon@stanford.edu) \n● Omar Khattab, 斯坦福大学 (okhattab@stanford.edu) \n● Christopher Potts, 斯坦福大学 (cgpotts@stanford.edu) \n● Matei Zaharia, Databricks and 加州大学伯克利分校 (matei@databricks.com) \n摘要 \n评估检索增强生成（RAG）系统传统上依赖于对输入查询、待检索段落和待生成响应的手动标"], "generated_answer": "根据提供的上下文，没有具体提到ARES在AIS基准上的答案忠实度表现。文中提到了ARES框架使用合成数据集来微调DeBERTa-v3-Large“裁判”模型以评估答案忠实度等任务，但并未给出具体的评估结果或性能指标。因此，无法直接回答ARES在AIS基准上的答案忠实度表现如何。如果需要这方面的详细信息，可能需要查阅论文的实验部分或者相关的基准测试报告。"}
{"question": "真实RAG系统评估中测试了哪些检索器和生成LLM？", "ground_truth_contexts": ["对于我们的RAG 系统，我们选择了三个不同的检索器（BM25、OpenAI Ada 嵌入与余弦相似性搜索，以及ColBERTv2...）和三个不同的生成LLM（MPT-7b-Instruct、GPT-3.5-Turbo 和 GPT-4）。"], "ground_truth_answer": "检索器：BM25、OpenAI Ada嵌入、ColBERTv2。生成LLM：MPT-7b-Instruct、GPT-3.5-Turbo、GPT-4。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "ARES：一种用于检索增强生成系统的自动\n化评估框架 \n作者信息： \n● Jon Saad-Falcon, 斯坦福大学 (jonsaadfalcon@stanford.edu) \n● Omar Khattab, 斯坦福大学 (okhattab@stanford.edu) \n● Christopher Potts, 斯坦福大学 (cgpotts@stanford.edu) \n● Matei Zaharia, Databricks and 加州大学伯克利分校 (matei@databricks.com) \n摘要 \n评估检索增强生成（RAG）系统传统上依赖于对输入查询、待检索段落和待生成响应的手动标", "与ARES 最相关的两个项目是EXAM 和 RAGAS。为了评估RAG 系统，EXAM 度量标准估计一个\n读者（模拟为问答系统）能根据生成的回答正确回答多少考试问题。这需要一组查询，每个查询\n都附有几个相关的子问题，这增加了ARES 所没有的负担。RAGAS 基于少数启发式手写的提\n示。这些提示对新的RAG 评估环境（例如，新的语料库）的适应性很小，并且正如我们在评估\n中所示，其性能远不如ARES。 \n3. ARES 框架 \nARES 分三个阶段进行（见图1）。需要三个输入：一个领域内段落集，一个约150 个或更多已", "证明了其区分忠实答案和幻觉答案的能力。 \n5.3 ARES 对现有 RAG 系统的排名 \n我们还评估了ARES 是否能对现有的RAG 系统进行评分和排名。结果发现，ARES 能够可靠地对\n真实世界的RAG 系统进行评分和排名，上下文相关性的肯德尔τ 平均为 0.91，答案相关性为\n0.97，显著优于RAGAS。 \n5.4 跨领域应用的优势与局限 \nARES 中使用的LLM“裁判”在跨领域应用中表现出强大的泛化能力，即使在查询类型、文档类型\n或两者都发生变化时也是如此。然而，当领域发生更剧烈的变化时，例如语言切换（如英语到西", "区间。 \n3.1 LLM 生成合成数据集 \n我们使用生成式LLM 从语料库段落中生成合成的查询和答案。生成的数据代表了查询-段落-答案\n三元组的正面和负面示例（例如，相关/不相关的段落和正确/不正确的答案）。在生成过程中，\nLLM 使用我们输入的少样本示例集，其中领域内段落映射到领域内查询和答案；然后模型从给定\n的领域内段落生成一个合成的问题和答案，使我们能够创建正面和负面的训练示例。 \n为了创建我们的合成数据，我们主要使用FLAN-T5 XXL。ARES 与这个模型配合得很好，但我们\n的系统最终也可以使用其他高质量模型来生成合成的查询和答案。然后，我们通过测试给定查询"], "generated_answer": "根据提供的上下文，文中没有具体提及在真实RAG系统评估中测试了哪些检索器和生成LLM。文中主要强调了ARES框架在评估RAG系统方面的有效性和优势，但未详细列出所测试的具体组件。如果您需要这方面的具体信息，可能需要查阅论文的实验部分或相关章节以获取详细的实验设置和测试对象列表。"}
{"question": "总结ARES在伪RAG系统排名上的结果。", "ground_truth_contexts": ["我们通过人工创建模拟的RAG 系统来测试ARES，这些系统在我们的评估指标上具有已知的准确率，范围从70%到90%不等，间隔为2.5%。", "在几乎所有来自KILT 和SuperGLUE 数据集的设置中，ARES 提供的RAG 系统排名都比RAGAS更准确。... 与基于抽样标注的方法相比，ARES 在准确性上更高，同时使用的标注量减少了78%..."], "ground_truth_answer": "ARES在Kendall’s τ上高于RAGAS（上下文相关性0.065，答案相关性0.132），并在使用78%更少标注的情况下优于采样标注。", "retrieved_contexts": ["ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "用来源提供一定程度的透明度或可解释性。 \n多种基于LLM 的评估技术已经出现，用于衡量LLM 系统的性能。这对于在难以从头开始构建传\n统基准数据集的新环境中快速部署至关重要。早期的尝试是直接使用开箱即用的LLM，如MT-\nBench 和Chatbot Arena。AutoCalibrate 试图将LLM“裁判”与人类偏好对齐，利用自我修正的\n提示来迭代改进LLM“裁判”。然而，AutoCalibrate 对其预测的准确性不提供任何统计保证。其\n他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。", "与基于标注的评估方法相比，ARES 的准确性和效率显著更高，比基线方法所需的标注量减少了\n78%。我们还发现，ARES 能够持续地区分那些在真实指标上仅相差几个百分点的有竞争力的\nRAG 系统。这种精确性使ARES 能够指导有竞争力的方案和配置的开发与比较。 \n我们的ARES 代码和数据集已在Github 上公开。 \n2. 相关工作 \n检索增强生成（RAG）现在是通过将大语言模型（LLMs）与检索系统相结合来增强其能力的常用\n策略。通过检索，RAG 帮助 LM 系统收集特定领域的知识，将生成内容基于事实信息，并通过引\n用来源提供一定程度的透明度或可解释性。", "建一个合成的问答对数据集。其次，它定义了三个独立的“裁判”模型来执行三个分类任务（上下\n文相关性、答案忠实度和答案相关性）。这些“裁判”是针对对比学习目标进行微调的轻量级模\n型。第三，ARES 使用预测驱动的推断（PPI）对被评估的不同 RAG 系统进行评分，以提高基于\n模型的评估准确性，并为 RAG 评分提供统计置信区间。PPI 利用一小部分人工标注的数据点来\n计算其置信区间；我们将这个标注集指定为我们的人工偏好验证集，它由大约150 个或更多的标\n注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例\n子。"], "generated_answer": "根据提供的上下文，ARES在伪RAG系统排名上的结果可以总结如下：\n\n1. **高准确性和效率**：与传统的基于人工标注的评估方法相比，ARES展示了更高的准确性和效率。\n2. **减少标注需求**：ARES显著减少了对人工标注数据的需求，比基线方法减少了78%的标注量。\n3. **区分能力**：ARES能够持续地区分那些在真实指标上表现仅有几个百分点差异的竞争性RAG系统。\n4. **指导开发和比较**：由于其精确性，ARES能够有效地指导竞争性方案和配置的开发与比较。\n5. **统计置信区间**：通过使用预测驱动的推断(PPI)，ARES不仅提高了基于模型的评估准确性，还为RAG评分提供了统计置信区间，增强了评估的可靠性。\n\n综上所述，ARES在伪RAG系统的排名中表现出色，不仅能有效减少对人工标注的依赖，还能保持甚至提高评估的准确性和可靠性，从而更好地支持RAG系统的优化和发展。"}
{"question": "ARES中有效PPI所需的最少人工标注数量是多少？", "ground_truth_contexts": ["我们使用了ARES，并使用了大小从25到400不等的人工标注集，并发现150是所需的最小数量（表3）。"], "ground_truth_answer": "ARES中有效PPI至少需要150个人工标注。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "ARES：一种用于检索增强生成系统的自动\n化评估框架 \n作者信息： \n● Jon Saad-Falcon, 斯坦福大学 (jonsaadfalcon@stanford.edu) \n● Omar Khattab, 斯坦福大学 (okhattab@stanford.edu) \n● Christopher Potts, 斯坦福大学 (cgpotts@stanford.edu) \n● Matei Zaharia, Databricks and 加州大学伯克利分校 (matei@databricks.com) \n摘要 \n评估检索增强生成（RAG）系统传统上依赖于对输入查询、待检索段落和待生成响应的手动标", "与ARES 最相关的两个项目是EXAM 和 RAGAS。为了评估RAG 系统，EXAM 度量标准估计一个\n读者（模拟为问答系统）能根据生成的回答正确回答多少考试问题。这需要一组查询，每个查询\n都附有几个相关的子问题，这增加了ARES 所没有的负担。RAGAS 基于少数启发式手写的提\n示。这些提示对新的RAG 评估环境（例如，新的语料库）的适应性很小，并且正如我们在评估\n中所示，其性能远不如ARES。 \n3. ARES 框架 \nARES 分三个阶段进行（见图1）。需要三个输入：一个领域内段落集，一个约150 个或更多已", "ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "集型任务中，ARES 在评估过程中仅使用数百个人工标注，便能准确地评估RAG 系统。此外，即\n使在被评估的RAG 系统中所使用的查询和/或文档类型发生变化，ARES 的“裁判”模型在领域迁\n移后依然保持有效和准确。我们的代码和数据集已在Github 上公开。 \n1. 引言 \n对于不同的数据领域、语料库大小以及成本/延迟预算，RAG 系统的最佳设计并非普遍适用。为\n了调整自己的RAG 系统，从业者传统上需要针对其目标领域手动标注测试问题、待检索的段落\n（用于评估检索器）以及待生成的响应。或者，他们也可以通过收集用户偏好来在生产环境中评"], "generated_answer": "根据提供的文本，ARES 在评估过程中仅使用**数百个人工标注**（hundreds of human annotations）便能有效地评估RAG系统。然而，文中并未明确指出实现有效PPI（ presumably \"Performance Prediction Index\" or a similar metric related to evaluation effectiveness）所需的**最少**人工标注数量具体是多少。因此，虽然我们知道几百个就足够有效，但无法从文本中得出一个确切的最小值。"}
{"question": "ARES是否支持非英语语言的评估？", "ground_truth_contexts": ["然而，当领域发生更剧烈的变化时，例如语言切换（如英语到西班牙语）... LLM“裁判”的泛化能力会下降。", "为了测试跨语言转移，我们使用了XGLUE 数据集... 在NQ 上微调的LLM“裁判”在... 达到了0.33 的肯德尔τ..."], "ground_truth_answer": "不，ARES无法很好地泛化到非英语语言，在XGLUE上Kendall’s τ较低（例如0.33）。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "集型任务中，ARES 在评估过程中仅使用数百个人工标注，便能准确地评估RAG 系统。此外，即\n使在被评估的RAG 系统中所使用的查询和/或文档类型发生变化，ARES 的“裁判”模型在领域迁\n移后依然保持有效和准确。我们的代码和数据集已在Github 上公开。 \n1. 引言 \n对于不同的数据领域、语料库大小以及成本/延迟预算，RAG 系统的最佳设计并非普遍适用。为\n了调整自己的RAG 系统，从业者传统上需要针对其目标领域手动标注测试问题、待检索的段落\n（用于评估检索器）以及待生成的响应。或者，他们也可以通过收集用户偏好来在生产环境中评", "ARES：一种用于检索增强生成系统的自动\n化评估框架 \n作者信息： \n● Jon Saad-Falcon, 斯坦福大学 (jonsaadfalcon@stanford.edu) \n● Omar Khattab, 斯坦福大学 (okhattab@stanford.edu) \n● Christopher Potts, 斯坦福大学 (cgpotts@stanford.edu) \n● Matei Zaharia, Databricks and 加州大学伯克利分校 (matei@databricks.com) \n摘要 \n评估检索增强生成（RAG）系统传统上依赖于对输入查询、待检索段落和待生成响应的手动标", "ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。"], "generated_answer": "根据提供的文本，ARES 的评估主要集中在英语上，未来的工作应该探索 ARES 在其他语言中的应用。这意味着目前 ARES 可能不直接支持非英语语言的评估，但有计划在未来进行扩展。"}
{"question": "SuperGrok订阅的价格是多少？", "ground_truth_contexts": [], "ground_truth_answer": "文档中没有此信息。", "retrieved_contexts": ["● 强负样本生成：对于上下文相关性的负样本，我们从与黄金段落相同的文档中随机抽样领域\n内段落。对于答案忠实度和答案相关性的负样本，我们提示FLAN-T5 XXL 生成一个矛盾的\n答案。 \n总的来说，为评估上下文相关性和答案相关性生成的负样本数量等于生成的正样本数量。 \n3.2 准备 LLM“裁判” \n为了准备我们的RAG 评估“裁判”，我们使用我们的合成数据集来微调DeBERTa-v3-Large“裁\n判”，以评估三种不同的能力： \n● 上下文相关性：返回的段落是否与回答给定查询相关？", "集型任务中，ARES 在评估过程中仅使用数百个人工标注，便能准确地评估RAG 系统。此外，即\n使在被评估的RAG 系统中所使用的查询和/或文档类型发生变化，ARES 的“裁判”模型在领域迁\n移后依然保持有效和准确。我们的代码和数据集已在Github 上公开。 \n1. 引言 \n对于不同的数据领域、语料库大小以及成本/延迟预算，RAG 系统的最佳设计并非普遍适用。为\n了调整自己的RAG 系统，从业者传统上需要针对其目标领域手动标注测试问题、待检索的段落\n（用于评估检索器）以及待生成的响应。或者，他们也可以通过收集用户偏好来在生产环境中评", "他工作已使用LLM 提示来评估自然语言生成任务中的系统质量，例如翻译、摘要和对话。 \n在知识密集型NLP 任务的背景下，LLM 已被探索用于评估LLM 的归因和事实性。新的指南如\nLongEval 和数据集如Hagrid 和 ALCE 为分析知识密集型LLM 流程提供了资源。", "ARES 依赖于一小组人工标注，这在专业领域可能需要具备专业知识的标注员。此外，ARES 中\n使用的LLM 需要具有大量存储空间的GPU 硬件，这可能不是所有研究人员和从业者都能轻易获\n得的。最后，我们的评估主要集中在英语上，未来的工作应该探索ARES 在其他语言中的应用。", "注数据点组成，这些数据点为上下文相关性、答案忠实度和答案相关性指定了正面和负面的例\n子。 \n我们进行了广泛的实证评估，证明ARES 在 KILT 和 SuperGLUE 的六个知识密集型数据集上准确\n地对 RAG 系统进行评分，在上下文相关性和答案相关性评估准确性上，平均分别比现有的自动\n化评估方法（如RAGAS）高出 59.3 和14.4 个百分点。此外，ARES 在 AIS 归因数据集中准确计\n算了答案幻觉的出现次数，预测的答案幻觉平均值与真实值的差距在2.5 个百分点以内。 \n与基于标注的评估方法相比，ARES 的准确性和效率显著更高，比基线方法所需的标注量减少了"], "generated_answer": "我不知道SuperGrok订阅的具体价格信息。您可能需要查阅相关的官方渠道或最新的资料来获取准确的价格详情。如果您有其他问题，欢迎继续提问！"}
