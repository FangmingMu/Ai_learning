{"question": "ARES系统的全称是什么？", "ground_truth_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"], "ground_truth_answer": "ARES代表Automated RAG Evaluation System。", "retrieved_contexts": ["Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.", "hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "in size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4)."], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES论文的作者是谁？", "ground_truth_contexts": ["Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com"], "ground_truth_answer": "作者是Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。", "retrieved_contexts": ["Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:", "(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES报告的三个评估分数是什么？", "ground_truth_contexts": ["ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response generated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question)."], "ground_truth_answer": "三个评估分数是上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane", "hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "out of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES如何生成用于训练判断器的合成数据？", "ground_truth_contexts": ["Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus.", "We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query–passage–answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers)."], "ground_truth_answer": "ARES使用像FLAN-T5 XXL这样的语言模型从领域内段落生成合成问题-答案对，包括正面和负面示例，并使用检索器过滤低质量查询。", "retrieved_contexts": ["preprint arXiv:2302.04166.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\nElkind, and Idan Szpektor. 2023. Trueteacher: Learn-", "like LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-", "lected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "预测驱动推理（PPI）在ARES中的作用是什么？", "ground_truth_contexts": ["Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals for RAG scoring.", "PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our human preference validation set."], "ground_truth_answer": "PPI通过利用一小部分人工标注数据点来提高基于模型的评估准确性，为分数提供置信区间，结合标注和未标注预测。", "retrieved_contexts": ["in size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).", "hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "In Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-"], "generated_answer": "根据提供的资料，预测驱动推理（PPI）在ARES中用于评估模型的泛化能力，特别是在使用较少的数据点（如300个数据点）的情况下，仍然能够表现出较强的跨领域应用能力。"}
{"question": "ARES在评估过程中通常需要多少人工标注？", "ground_truth_contexts": ["ARES significantly improves data efficiency during evaluation by only requiring three inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more)."], "ground_truth_answer": "ARES需要大约150个或更多人工标注数据点在人类偏好验证集中。", "retrieved_contexts": ["hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES中使用哪些模型进行合成数据生成和判断器微调？", "ground_truth_contexts": ["For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge."], "ground_truth_answer": "FLAN-T5 XXL用于合成数据生成，DeBERTa-v3-Large用于微调LLM判断器。", "retrieved_contexts": ["Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.", "in size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).", "preprint arXiv:2302.04166.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\nElkind, and Idan Szpektor. 2023. Trueteacher: Learn-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "总结ARES管道的三个阶段。", "ground_truth_contexts": ["ARES proceeds in three stages (Figure 1). ... First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus. Second, it defines three separate judge models to perform three classification tasks... Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI)."], "ground_truth_answer": "阶段1：从语料段落生成合成查询和答案。阶段2：为上下文相关性、答案忠实度和答案相关性微调LLM判断器。阶段3：使用判断器和PPI为RAG系统评分以获得置信区间。", "retrieved_contexts": ["answers. We then filter out low-quality queries\nby testing if a given query can retrieve its original\npassage as the top result using its retriever. This\nfiltering approach has been used in previous work\nto isolate high-quality synthetic queries (Dai et al.,\n2022; Saad-Falcon et al., 2023).", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia", "hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "比较ARES和RAGAS在上下文相关性和答案相关性方面的性能。", "ground_truth_contexts": ["ARES averages a Kendall’s τ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS. ... For context relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS while for answer relevance, our system is 14.4 percentage points higher than RAGAS."], "ground_truth_answer": "ARES在Kendall’s τ上优于RAGAS（上下文相关性0.065，答案相关性0.132），并在准确性上更高（上下文相关性59.9%点，答案相关性14.4%点）。", "retrieved_contexts": ["Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:", "in size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4)."], "generated_answer": "根据提供的资料无法回答。"}
{"question": "用于评估ARES的KILT和SuperGLUE数据集有哪些？", "ground_truth_contexts": ["From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW)... From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD."], "ground_truth_answer": "KILT数据集：Natural Questions (NQ)、HotpotQA、FEVER、Wizards of Wikipedia (WoW)。SuperGLUE数据集：MultiRC和ReCoRD。", "retrieved_contexts": ["hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.", "ing automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-\nrences in the AIS attribution dataset (Rashkin et al.,"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES如何处理合成数据生成中的负面示例？", "ground_truth_contexts": ["To generate negatives for fine-tuning our LLM judges, we rely on two novel strategies... 1. Weak Negative Generation: ... 2. Strong Negative Generation: ..."], "ground_truth_answer": "ARES使用弱负面（随机采样）和强负面（从相似段落采样或生成矛盾答案）用于上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["like LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.", "with our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative"], "generated_answer": "根据提供的资料，ARES通过生成合成查询和答案来代表正例和负例，但具体如何处理负面示例并未详细说明。因此，根据提供的资料无法回答。"}
{"question": "ARES实验中使用的置信区间概率是多少？", "ground_truth_contexts": ["Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (probability) for our confidence interval."], "ground_truth_answer": "ARES在其实验中使用95%的置信区间概率。", "retrieved_contexts": ["with our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative", "hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "like LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "分析ARES在跨域应用中的优势。", "ground_truth_contexts": ["In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross-domain applications. Across all settings, we found that LLM judges in ARES had strong generalizability... PPI helped mitigate the loss in accuracy and still allow ARES to be successful."], "ground_truth_answer": "ARES判断器在KILT和SuperGLUE的查询和文档类型上显示出强大的泛化能力，PPI缓解准确性损失，但在语言、代码或实体提取等剧烈转变中失败。", "retrieved_contexts": ["with our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative", "like LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-", "wise ranking and an experimental pairwise ranking.\nThus, it is a popular and widespread metric used in\ninformation retrieval, allowing developers to eval-\nuate ranking systems empirically. Therefore, we\nbelieve Kendall’s tau and prediction accuracy pro-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "论文中提到的ARES的局限性是什么？", "ground_truth_contexts": ["ARES relies on a small set of annotations... These annotations often require an annotator familiar with the RAG system’s domain... The LLMs used in ARES benefit substantially from GPU-based hardware... Additionally, all of the datasets used in our evaluation of ARES are in English..."], "ground_truth_answer": "局限性包括需要领域专家标注、需要GPU硬件，并且仅在英文数据集上评估。", "retrieved_contexts": ["hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146."], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES在AIS基准上的答案忠实度表现如何？", "ground_truth_contexts": ["Table 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores."], "ground_truth_answer": "ARES在AIS基准上对答案幻觉的预测在真实平均值的2.5个百分点以内。", "retrieved_contexts": ["hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "out of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "真实RAG系统评估中测试了哪些检索器和生成LLM？", "ground_truth_contexts": ["For our RAG systems, we selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search, and ColBERTv2... and three different generative LLMs (MPT-7b-Instruct, GPT-3.5-Turbo, and GPT-4)."], "ground_truth_answer": "检索器：BM25、OpenAI Ada嵌入、ColBERTv2。生成LLM：MPT-7b-Instruct、GPT-3.5-Turbo、GPT-4。", "retrieved_contexts": ["with our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative", "like LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-", "lected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "总结ARES在伪RAG系统排名上的结果。", "ground_truth_contexts": ["Table 1: ... Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets."], "ground_truth_answer": "ARES在Kendall’s τ上高于RAGAS（上下文相关性0.065，答案相关性0.132），并在使用78%更少标注的情况下优于采样标注。", "retrieved_contexts": ["hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane", "Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES中有效PPI所需的最少人工标注数量是多少？", "ground_truth_contexts": ["We used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3)."], "ground_truth_answer": "ARES中有效PPI至少需要150个人工标注。", "retrieved_contexts": ["hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146."], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES是否支持非英语语言的评估？", "ground_truth_contexts": ["While LLM judges in ARES were successful in cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages...", "To test cross-lingual transfer, we used the XGLUE datasets... a LLM judge fine-tuned on NQ achieved a Kendall’s tau of 0.33..."], "ground_truth_answer": "不，ARES无法很好地泛化到非英语语言，在XGLUE上Kendall’s τ较低（例如0.33）。", "retrieved_contexts": ["Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.", "in size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).", "with our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from\nthe corpus passages using generative LLMs. The\ngenerated data represent both positive and negative"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "SuperGrok订阅的价格是多少？", "ground_truth_contexts": [], "ground_truth_answer": "文档中没有此信息。", "retrieved_contexts": ["hidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.", "(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and"], "generated_answer": "根据提供的资料无法回答。"}
