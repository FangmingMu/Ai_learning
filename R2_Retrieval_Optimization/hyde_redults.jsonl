{"question": "ARES系统的全称是什么？", "ground_truth_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"], "ground_truth_answer": "ARES代表Automated RAG Evaluation System。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses"], "generated_answer": "ARES系统的全称是\"An Automated Evaluation Framework for Retrieval-Augmented Generation Systems\"。"}
{"question": "ARES论文的作者是谁？", "ground_truth_contexts": ["Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com"], "ground_truth_answer": "作者是Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-\nnAI’s gpt-3.5-turbo-16k, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similar-\nity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "ARES论文的作者包括Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。"}
{"question": "ARES报告的三个评估分数是什么？", "ground_truth_contexts": ["ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response generated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question)."], "ground_truth_answer": "三个评估分数是上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "framework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "et al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-\ntems, the EXAM metric estimates how many exam\nquestions a reader (simulated as a QA system) can\nanswer correctly based on the generated response.\nThis requires a set of queries with several asso-\nciated sub-questions each, which adds a burden\nthat ARES does not bring. RAGAS is based on a\nhandful of heuristic hand-written prompts. These\noffer little adaptability to new RAG evaluation set-"], "generated_answer": "ARES报告的三个评估分数是：context relevance。"}
{"question": "ARES如何生成用于训练判断器的合成数据？", "ground_truth_contexts": ["Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus.", "We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query–passage–answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers)."], "ground_truth_answer": "ARES使用像FLAN-T5 XXL这样的语言模型从领域内段落生成合成问题-答案对，包括正面和负面示例，并使用检索器过滤低质量查询。", "retrieved_contexts": ["4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-\nnAI’s gpt-3.5-turbo-16k, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similar-\nity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "ARES使用FLAN-T5 XXL生成廉价但高质量的合成查询和答案，以用于训练判断器。"}
{"question": "预测驱动推理（PPI）在ARES中的作用是什么？", "ground_truth_contexts": ["Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals for RAG scoring.", "PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our human preference validation set."], "ground_truth_answer": "PPI通过利用一小部分人工标注数据点来提高基于模型的评估准确性，为分数提供置信区间，结合标注和未标注预测。", "retrieved_contexts": ["puts from each RAG systems separately, which can\nbe costly both in terms of time and financing.\nTo combine the benefits of both, and hence\nboost the precision of the evaluation, ARES uses\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to predict the system scores. PPI is\na recent statistical method that provides tighter\nconfidence intervals on a small set of annotated\ndatapoints (i.e., our validation set) by leveraging\npredictions on a much larger set of non-annotated\ndatapoints. PPI can leverage both the labeled dat-\napoints and the ARES judge predictions on the\nnon-annotated datapoints to construct confidence\nintervals for our RAG system’s performance.\nTo do this, PPI uses the LLM judges on the hu-\nman preference validation set to learn a rectifier\nfunction for constructing a confidence set of the ML\nmodel’s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence", "interval for the performance of the evaluated RAG\nsystem (e.g. its context relevance, answer faithful-\nness, or answer relevance accuracy individually)\ncompared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the hu-\nman preference validation set with the much larger\nset of datapoints with ML predictions, PPI can de-\nvelop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confi-\ndence bounds for the success and failure rates of the\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (proba-\nbility) for our confidence interval.\nWith the accuracy confidence interval for each\ncomponent of the RAG, we find the midpoint of", "to FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval."], "generated_answer": "预测驱动推理（PPI）在ARES中的作用是通过利用大量未标注数据的预测结果，结合少量标注数据的验证集，提供更紧致的置信区间，从而提升对RAG系统性能评估的精确性。PPI能够估计LLM裁判的误差，并生成RAG系统成功和失败率的置信区间，用于评估上下文相关性、答案忠实性和答案相关性等性能指标。"}
{"question": "ARES在评估过程中通常需要多少人工标注？", "ground_truth_contexts": ["ARES significantly improves data efficiency during evaluation by only requiring three inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more)."], "ground_truth_answer": "ARES需要大约150个或更多人工标注数据点在人类偏好验证集中。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-\nform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "ARES在评估过程中通常需要大约150个标注的数据点。"}
{"question": "ARES中使用哪些模型进行合成数据生成和判断器微调？", "ground_truth_contexts": ["For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge."], "ground_truth_answer": "FLAN-T5 XXL用于合成数据生成，DeBERTa-v3-Large用于微调LLM判断器。", "retrieved_contexts": ["4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-\nnAI’s gpt-3.5-turbo-16k, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similar-\nity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from"], "generated_answer": "ARES中使用FLAN-T5 XXL进行合成数据生成，使用DeBERTa-v3-Large进行判断器的微调。"}
{"question": "总结ARES管道的三个阶段。", "ground_truth_contexts": ["ARES proceeds in three stages (Figure 1). ... First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus. Second, it defines three separate judge models to perform three classification tasks... Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI)."], "ground_truth_answer": "阶段1：从语料段落生成合成查询和答案。阶段2：为上下文相关性、答案忠实度和答案相关性微调LLM判断器。阶段3：使用判断器和PPI为RAG系统评分以获得置信区间。", "retrieved_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-\nform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores"], "generated_answer": "ARES管道的三个阶段是：  \n1. 利用语言模型从领域内段落中构建合成的问答数据集。  \n2. 定义三个独立的判断模型，分别执行三个分类任务（上下文相关性、答案忠实性和答案相关性）。  \n3. 对RAG系统进行评分。"}
{"question": "比较ARES和RAGAS在上下文相关性和答案相关性方面的性能。", "ground_truth_contexts": ["ARES averages a Kendall’s τ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS. ... For context relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS while for answer relevance, our system is 14.4 percentage points higher than RAGAS."], "ground_truth_answer": "ARES在Kendall’s τ上优于RAGAS（上下文相关性0.065，答案相关性0.132），并在准确性上更高（上下文相关性59.9%点，答案相关性14.4%点）。", "retrieved_contexts": ["Appendices A.2, A.3, and A.4. For both ARES\nand the GPT-3.5 judge baseline, we augment the\nLLM with PPI, using a 300-datapoint human pref-\nerence validation set to rectify the ML predictions\nand produce confidence intervals.\nAcross almost all settings across the datasets\nfrom KILT and SuperGLUE, ARES provides a\nmore accurate ranking of RAG systems than RA-\nGAS. ARES averages a Kendall’sτ 0.065 higher\nfor context relevance and 0.132 higher for answer\nrelevance than RAGAS . Additionally, the LLM-\njudge is substantially more accurate than RAGAS\nat predicting context relevance and answer rele-\nvance of a query-passage-answer triple. For con-\ntext relevance, ARES with a fine-tuned LLM-judge\nis 59.9 percentage points higher than RAGASwhile\nfor answer relevance, our system is 14.4 percent-\nage points higher than RAGAS . Overall, ARES\nprovides a more accurate system for automatically\nevaluating RAG configurations than RAGAS by\nleveraging domain-adaptive techniques for prompt-", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-\nform three classification tasks (context relevance,\nanswer faithfulness, and answer relevance). These\njudges are lightweight models fine-tuned against a\ncontrastive learning objective. Third, ARES scores", "evaluation framework for retrieval-augmented gen-\neration (RAG). ARES offers a novel training\npipeline for fine-tuning lightweight LLM judges\non synthetically generated queries and answers.\nARES can evaluate each component of a RAG sys-\ntem separately to help improve system understand-\ning and create targeted solutions, and it requires\nonly minimal human annotations. For the eight dif-\nferent datasets in KILT, SuperGLUE, and AIS re-\nquiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing"], "generated_answer": "根据提供的资料，ARES在上下文相关性和答案相关性方面优于RAGAS。具体来说，ARES在上下文相关性上的Kendall’sτ平均比RAGAS高0.065，在答案相关性上平均比RAGAS高0.132。此外，ARES在预测上下文相关性和答案相关性时也比RAGAS更准确，尤其是在上下文相关性方面，ARES比RAGAS高出59.9个百分点，而在答案相关性方面，ARES比RAGAS高出14.4个百分点。"}
{"question": "用于评估ARES的KILT和SuperGLUE数据集有哪些？", "ground_truth_contexts": ["From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW)... From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD."], "ground_truth_answer": "KILT数据集：Natural Questions (NQ)、HotpotQA、FEVER、Wizards of Wikipedia (WoW)。SuperGLUE数据集：MultiRC和ReCoRD。", "retrieved_contexts": ["evaluation framework for retrieval-augmented gen-\neration (RAG). ARES offers a novel training\npipeline for fine-tuning lightweight LLM judges\non synthetically generated queries and answers.\nARES can evaluate each component of a RAG sys-\ntem separately to help improve system understand-\ning and create targeted solutions, and it requires\nonly minimal human annotations. For the eight dif-\nferent datasets in KILT, SuperGLUE, and AIS re-\nquiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-\nnAI’s gpt-3.5-turbo-16k, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similar-\nity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "根据提供的资料，用于评估ARES的KILT和SuperGLUE数据集包括八种不同的数据集。"}
{"question": "ARES如何处理合成数据生成中的负面示例？", "ground_truth_contexts": ["To generate negatives for fine-tuning our LLM judges, we rely on two novel strategies... 1. Weak Negative Generation: ... 2. Strong Negative Generation: ..."], "ground_truth_answer": "ARES使用弱负面（随机采样）和强负面（从相似段落采样或生成矛盾答案）用于上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in", "tings (e.g., new corpora) and, as we show in our\nevaluation, substantially underperform ARES.\n3 ARES\nARES proceeds in three stages (Figure 1). There\nare three required inputs: an in-domain passage set,\na human preference validation set of approximately\n150 annotated datapoints (or more), and few-shot\nexamples of in-domain queries and answers (five\nor more examples), which are used for prompting\nLLMs in synthetic data generation. With our inputs\nprepared, we begin by generating synthetic queries\n(and their answers) from the passages in the target\ncorpus. We then use these query–passage–answer\ntriples to train LLM judges. Subsequently, we ap-\nply these judges to any RAG system, scoring a\nsample of its in-domain query-document-answer\ntriples, and use prediction-powered inference (PPI)\nwith our human preference validation set to esti-\nmate a confidence interval for the quality of each\nRAG system.\n3.1 LLM Generation of Synthetic Dataset\nWe generate synthetic queries and answers from"], "generated_answer": "根据提供的资料，没有明确说明ARES如何处理合成数据生成中的负面示例。因此，无法从上下文中得出具体答案。"}
{"question": "ARES实验中使用的置信区间概率是多少？", "ground_truth_contexts": ["Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (probability) for our confidence interval."], "ground_truth_answer": "ARES在其实验中使用95%的置信区间概率。", "retrieved_contexts": ["component of the RAG, we find the midpoint of\neach confidence interval and use the midpoints to\nrank the RAG systems. With our ranking, we can\ncompare different RAG systems, as well as differ-\nent configurations of the same RAG system, to find\nthe best-performing approach for a given domain.", "bounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).", "to FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval."], "generated_answer": "根据提供的资料，ARES实验中使用的置信区间概率是超过95%。"}
{"question": "分析ARES在跨域应用中的优势。", "ground_truth_contexts": ["In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross-domain applications. Across all settings, we found that LLM judges in ARES had strong generalizability... PPI helped mitigate the loss in accuracy and still allow ARES to be successful."], "ground_truth_answer": "ARES判断器在KILT和SuperGLUE的查询和文档类型上显示出强大的泛化能力，PPI缓解准确性损失，但在语言、代码或实体提取等剧烈转变中失败。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "bounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in"], "generated_answer": "ARES在跨域应用中的优势在于其LLM judge的泛化能力，这使其能够在专业领域中部署，尤其是在难以获取领域内查询、文档和答案的情况下。"}
{"question": "论文中提到的ARES的局限性是什么？", "ground_truth_contexts": ["ARES relies on a small set of annotations... These annotations often require an annotator familiar with the RAG system’s domain... The LLMs used in ARES benefit substantially from GPU-based hardware... Additionally, all of the datasets used in our evaluation of ARES are in English..."], "ground_truth_answer": "局限性包括需要领域专家标注、需要GPU硬件，并且仅在英文数据集上评估。", "retrieved_contexts": ["practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "bounds for its predictions, capturing the ground\ntruth average outcomes for context relevance and\nanswer relevance more than 95% of the time; on av-\nerage, the PPI confidence intervals were 7.4 points\nwide for context relevance and 6.1 points wide for\nanswer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).", "accurately calculates answer hallucination occur-\nrences in the AIS attribution dataset (Rashkin et al.,\n2022), predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach. We also find that ARES consistently\ndistinguishes competitive RAG systems that are\nonly a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-"], "generated_answer": "根据提供的资料，论文中提到的ARES的局限性包括：所有用于评估ARES的数据集都是英文的，这是一个资源丰富的语言，拥有大量的注释。未来的工作应探索如何在其他语言中使用ARES，通过使用不同的LLM作为ARES的评判者和生成合成数据。这可以帮助更好地理解当前ARES框架的优势和劣势。"}
{"question": "ARES在AIS基准上的答案忠实度表现如何？", "ground_truth_contexts": ["Table 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores."], "ground_truth_answer": "ARES在AIS基准上对答案幻觉的预测在真实平均值的2.5个百分点以内。", "retrieved_contexts": ["accurately calculates answer hallucination occur-\nrences in the AIS attribution dataset (Rashkin et al.,\n2022), predicting within 2.5 percentage points of\nthe ground truth average for answer hallucinations.\nCompared to annotation-based evaluation methods,\nARES is substantially more accurate and efficient,\nrequiring 78% less annotations than the baseline\napproach. We also find that ARES consistently\ndistinguishes competitive RAG systems that are\nonly a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "of Wikipedia (WoW) and CNN/DM datasets; the\nother benchmark datasets involve either table rea-\nsoning (ToTTo) or focus on passage summariza-\ntion (QRECC) so we excluded them. In WoW\nand CNN/DM, each evaluation example includes\na query, a retrieved passage, and a generated an-\nswer (which is either faithful or non-attributed to\nthe retrieved passage).\nTable 2 summarizes our AIS results. We found\nthat ARES can effectively score the AIS datasets,\ngetting within 2.5 accuracy points of the correct\nscores. Furthermore, for scoring each system,\nwe only use 200 annotated datapoints for our hu-\nman preference validation set. Our results on AIS\ndemonstrate the ability of ARES to reliably dis-\ntinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER"], "generated_answer": "ARES在AIS基准上的答案忠实度表现能够准确计算答案幻觉的发生情况，预测结果与真实平均值的差距在2.5个百分点以内。"}
{"question": "真实RAG系统评估中测试了哪些检索器和生成LLM？", "ground_truth_contexts": ["For our RAG systems, we selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search, and ColBERTv2... and three different generative LLMs (MPT-7b-Instruct, GPT-3.5-Turbo, and GPT-4)."], "ground_truth_answer": "检索器：BM25、OpenAI Ada嵌入、ColBERTv2。生成LLM：MPT-7b-Instruct、GPT-3.5-Turbo、GPT-4。", "retrieved_contexts": ["4 Experiments\n4.1 Models\nFor our fine-tuned judges, ARES relies on generat-\ning cheap but quality synthetic queries and answers\nusing LLMs. For generating our synthetic datasets,\nwe use FLAN-T5 XXL (Chung et al., 2022). We se-\nlected DeBERTa-v3-Large (He et al., 2021) for our\nfine-tuned LLM judge. Our fine-tuned LLM judges\nallow us to rank RAG systems without relying on\nexternal APIs, solely using few-shot prompts and\ndeployable LLMs on commercial GPUs.\nFor our in-context learning baseline, we use Ope-\nnAI’s gpt-3.5-turbo-16k, version 10/23, (Brown\net al., 2020) in a zero/few-shot setting. For similar-\nity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets", "erations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, Ope-\nnAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad-\nditionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-to-\nsequence model (Lewis et al., 2019). During re-\ntrieval, each RAG system only retrieves one pas-\nsage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-\npared to RAGAS, ARES is 0.16 higher for context\nrelevance and 0.15 higher for answer relevance, on\naverage. ARES also provided accurate confidence\nbounds for its predictions, capturing the ground", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and"], "generated_answer": "在真实RAG系统评估中，测试了以下检索器和生成LLM：\n\n**检索器：**\n- BM25\n- OpenAI Ada embeddings with cosine similarity search\n- ColBERTv2 (Santhanam et al., 2022)\n\n**生成LLM：**\n- MPT-7b-Instruct (Team, 2023)\n- GPT-3.5-Turbo\n- GPT-4\n- Facebook RAG模型（使用DPR retriever和BART sequence-to-sequence模型）"}
{"question": "总结ARES在伪RAG系统排名上的结果。", "ground_truth_contexts": ["Table 1: ... Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets."], "ground_truth_answer": "ARES在Kendall’s τ上高于RAGAS（上下文相关性0.065，答案相关性0.132），并在使用78%更少标注的情况下优于采样标注。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses", "ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in", "evaluation framework for retrieval-augmented gen-\neration (RAG). ARES offers a novel training\npipeline for fine-tuning lightweight LLM judges\non synthetically generated queries and answers.\nARES can evaluate each component of a RAG sys-\ntem separately to help improve system understand-\ning and create targeted solutions, and it requires\nonly minimal human annotations. For the eight dif-\nferent datasets in KILT, SuperGLUE, and AIS re-\nquiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing"], "generated_answer": "根据提供的资料，ARES在伪RAG系统排名上的结果如下：ARES在多个指标上表现优异，包括Kendall’s Tau和Accuracy。例如，在Context Relevance (C.R.)和Answer Relevance (A.R.)方面，ARES的Kendall’s Tau值分别为1.0和1.0，显著高于其他方法如RAGAS、GPT-3.5 Judge等。此外，ARES的Accuracy也优于其他方法，显示出其在评估和排名RAG系统方面的有效性。"}
{"question": "ARES中有效PPI所需的最少人工标注数量是多少？", "ground_truth_contexts": ["We used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3)."], "ground_truth_answer": "ARES中有效PPI至少需要150个人工标注。", "retrieved_contexts": ["puts from each RAG systems separately, which can\nbe costly both in terms of time and financing.\nTo combine the benefits of both, and hence\nboost the precision of the evaluation, ARES uses\nprediction-powered inference (PPI; Angelopoulos\net al. 2023) to predict the system scores. PPI is\na recent statistical method that provides tighter\nconfidence intervals on a small set of annotated\ndatapoints (i.e., our validation set) by leveraging\npredictions on a much larger set of non-annotated\ndatapoints. PPI can leverage both the labeled dat-\napoints and the ARES judge predictions on the\nnon-annotated datapoints to construct confidence\nintervals for our RAG system’s performance.\nTo do this, PPI uses the LLM judges on the hu-\nman preference validation set to learn a rectifier\nfunction for constructing a confidence set of the ML\nmodel’s performance, using each ML prediction in\nthe larger non-annotated dataset. The confidence\nset can then be used to create a tighter confidence", "For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation\nset. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nused ARES with human annotation sets ranging\nin size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).\n5.2 ARES Performance on AIS\nWoW CNN / DM\nARES Split Prediction 0.478 0.835\nCorrect Positive/Negative Split 0.458 0.859\nARES Judge Accuracy 62.5% 84.0%\nEvaluation Set Size 707 510\nHuman Preference Data Size 200 200\nTable 2: ARES Results on the AIS benchmark\nTo evaluate whether ARES can effectively gauge\nanswer faithfulness in real RAG systems, we tested\nARES on the AIS attribution benchmark (Rashkin\net al., 2022). In AIS, we selected the Wizards\nof Wikipedia (WoW) and CNN/DM datasets; the", "leveraging domain-adaptive techniques for prompt-\ning and training as well as utilizing PPI to bolster\nmodel predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides\na more accurate ranking of the RAG systems than\nthe GPT-3.5 judge, averaging a Kendall’s tau 0.06"], "generated_answer": "根据提供的资料，ARES中有效PPI所需的最少人工标注数量是150。"}
{"question": "ARES是否支持非英语语言的评估？", "ground_truth_contexts": ["While LLM judges in ARES were successful in cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages...", "To test cross-lingual transfer, we used the XGLUE datasets... a LLM judge fine-tuned on NQ achieved a Kendall’s tau of 0.33..."], "ground_truth_answer": "不，ARES无法很好地泛化到非英语语言，在XGLUE上Kendall’s τ较低（例如0.33）。", "retrieved_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented\nGeneration Systems\nJon Saad-Falcon\nStanford University ∗\njonsaadfalcon@stanford.edu\nOmar Khattab\nStanford University\nokhattab@stanford.edu\nChristopher Potts\nStanford University\ncgpotts@stanford.edu\nMatei Zaharia\nDatabricks and UC Berkeley\nmatei@databricks.com\nAbstract\nEvaluating retrieval-augmented generation\n(RAG) systems traditionally relies on hand\nannotations for input queries, passages to re-\ntrieve, and responses to generate. We intro-\nduce ARES, an Automated RAG Evaluation\nSystem, for evaluating RAG systems along\nthe dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in", "practitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda", "framework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance"], "generated_answer": "根据提供的资料，ARES目前仅在英语环境中进行评估，但未来工作应探索如何将其应用于其他语言。因此，答案是：目前不支持非英语语言的评估，但未来可能扩展。"}
{"question": "SuperGrok订阅的价格是多少？", "ground_truth_contexts": [], "ground_truth_answer": "文档中没有此信息。", "retrieved_contexts": ["preet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\nXiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su,\nand Huan Sun. 2023. Automatic evaluation of attri-\nbution by large language models.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. arXiv\npreprint arXiv:1810.12885.", "uation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and"], "generated_answer": "根据提供的资料无法回答。"}
