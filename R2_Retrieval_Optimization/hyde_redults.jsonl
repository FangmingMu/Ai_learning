{"question": "ARES系统的全称是什么？", "ground_truth_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"], "ground_truth_answer": "ARES代表Automated RAG Evaluation System。", "retrieved_contexts": ["it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES论文的作者是谁？", "ground_truth_contexts": ["Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com"], "ground_truth_answer": "作者是Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。", "retrieved_contexts": ["Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\nXiang Yue, Boshi Wang, Ziru Chen, Kai Zhang, Yu Su,\nand Huan Sun. 2023. Automatic evaluation of attri-\nbution by large language models.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. arXiv\npreprint arXiv:1810.12885.", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,", "the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial costs required to run.\nFor PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation\nset. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nused ARES with human annotation sets ranging\nin size from 25 to 400 and found that 150 is the\nminimum number required (Table 3). Second, we\nexplored whether GPT-4 generations could replace\nhuman annotations entirely, finding that GPT-4 is\nless good than humans in this role, though the idea\narguably has promise (Table 4).\n5.2 ARES Performance on AIS\nWoW CNN / DM\nARES Split Prediction 0.478 0.835\nCorrect Positive/Negative Split 0.458 0.859\nARES Judge Accuracy 62.5% 84.0%", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "language models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit\nIyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.\n2023. LongEval: Guidelines for human evaluation of\nfaithfulness in long-form summarization. In Proceed-\nings of the 17th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 1650–1669, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES报告的三个评估分数是什么？", "ground_truth_contexts": ["ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response generated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question)."], "ground_truth_answer": "三个评估分数是上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "preprint arXiv:2302.04166.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\nElkind, and Idan Szpektor. 2023. Trueteacher: Learn-\ning factual consistency evaluation with large lan-\nguage models.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nJeremy Howard and Sebastian Ruder. 2018. Universal", "compared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the hu-\nman preference validation set with the much larger\nset of datapoints with ML predictions, PPI can de-\nvelop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confi-\ndence bounds for the success and failure rates of the\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (proba-\nbility) for our confidence interval.\nWith the accuracy confidence interval for each"], "generated_answer": "ARES报告的三个评估分数是：\n\n1. **Context Relevance**（上下文相关性）：检索到的信息是否与测试问题相关。\n2. **Answer Faithfulness**（答案忠实度）：语言模型生成的回答是否正确基于检索到的上下文。\n3. **Answer Relevance**（答案相关性）：生成的回答是否与问题相关。"}
{"question": "ARES如何生成用于训练判断器的合成数据？", "ground_truth_contexts": ["Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus.", "We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query–passage–answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers)."], "ground_truth_answer": "ARES使用像FLAN-T5 XXL这样的语言模型从领域内段落生成合成问题-答案对，包括正面和负面示例，并使用检索器过滤低质量查询。", "retrieved_contexts": ["Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-"], "generated_answer": "ARES通过以下步骤生成用于训练判断器的合成数据：\n\n1. **输入语料库**：提供一个领域内的段落集合（in-domain passage set）。\n2. **生成问答对**：利用语言模型（LM），基于语料库中的段落自动生成问题-答案对，构成一个合成的数据集。\n3. **人工验证集**：使用约150个或更多带注释的数据点组成的人类偏好验证集，这些数据点包含正面和负面示例，用于评估判断器的表现。\n4. **少量示例提示**：使用领域内查询和答案的少量示例（例如五个或更多）来引导大语言模型进行合成数据生成。\n\n该过程显著提高了评估阶段的数据效率，减少了对大量人工标注数据的依赖。"}
{"question": "预测驱动推理（PPI）在ARES中的作用是什么？", "ground_truth_contexts": ["Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals for RAG scoring.", "PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our human preference validation set."], "ground_truth_answer": "PPI通过利用一小部分人工标注数据点来提高基于模型的评估准确性，为分数提供置信区间，结合标注和未标注预测。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-"], "generated_answer": "在ARES中，预测驱动推理（PPI）用于支持其评分机制。具体来说，ARES利用PPI来生成评估分数，包括上下文相关性、答案忠实性和答案相关性三个维度的评价。这使得ARES能够通过自动化的方式对检索增强生成（RAG）系统进行有效评估。\n\n注：虽然上下文提到“its scoring by leveraging prediction-powered inference (PPI)”，但未详细说明PPI的具体实现方式或技术细节。以上为基于现有信息的总结。"}
{"question": "ARES在评估过程中通常需要多少人工标注？", "ground_truth_contexts": ["ARES significantly improves data efficiency during evaluation by only requiring three inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more)."], "ground_truth_answer": "ARES需要大约150个或更多人工标注数据点在人类偏好验证集中。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES中使用哪些模型进行合成数据生成和判断器微调？", "ground_truth_contexts": ["For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge."], "ground_truth_answer": "FLAN-T5 XXL用于合成数据生成，DeBERTa-v3-Large用于微调LLM判断器。", "retrieved_contexts": ["the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "总结ARES管道的三个阶段。", "ground_truth_contexts": ["ARES proceeds in three stages (Figure 1). ... First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus. Second, it defines three separate judge models to perform three classification tasks... Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI)."], "ground_truth_answer": "阶段1：从语料段落生成合成查询和答案。阶段2：为上下文相关性、答案忠实度和答案相关性微调LLM判断器。阶段3：使用判断器和PPI为RAG系统评分以获得置信区间。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "preprint arXiv:2302.04166.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\nElkind, and Idan Szpektor. 2023. Trueteacher: Learn-\ning factual consistency evaluation with large lan-\nguage models.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nJeremy Howard and Sebastian Ruder. 2018. Universal", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "比较ARES和RAGAS在上下文相关性和答案相关性方面的性能。", "ground_truth_contexts": ["ARES averages a Kendall’s τ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS. ... For context relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS while for answer relevance, our system is 14.4 percentage points higher than RAGAS."], "ground_truth_answer": "ARES在Kendall’s τ上优于RAGAS（上下文相关性0.065，答案相关性0.132），并在准确性上更高（上下文相关性59.9%点，答案相关性14.4%点）。", "retrieved_contexts": ["it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "language models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit\nIyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo.\n2023. LongEval: Guidelines for human evaluation of\nfaithfulness in long-form summarization. In Proceed-\nings of the 17th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 1650–1669, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-"], "generated_answer": "根据提供的资料，ARES在上下文相关性和答案相关性方面的性能优于RAGAS。具体来说：\n\n- 在上下文相关性（Context Relevance）方面，ARES比RAGAS高出59.3个百分点。\n- 在答案相关性（Answer Relevance）方面，ARES比RAGAS高出14.4个百分点。\n\n这些数据表明，ARES在这两项评估指标上显著优于RAGAS。"}
{"question": "用于评估ARES的KILT和SuperGLUE数据集有哪些？", "ground_truth_contexts": ["From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW)... From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD."], "ground_truth_answer": "KILT数据集：Natural Questions (NQ)、HotpotQA、FEVER、Wizards of Wikipedia (WoW)。SuperGLUE数据集：MultiRC和ReCoRD。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES如何处理合成数据生成中的负面示例？", "ground_truth_contexts": ["To generate negatives for fine-tuning our LLM judges, we rely on two novel strategies... 1. Weak Negative Generation: ... 2. Strong Negative Generation: ..."], "ground_truth_answer": "ARES使用弱负面（随机采样）和强负面（从相似段落采样或生成矛盾答案）用于上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES实验中使用的置信区间概率是多少？", "ground_truth_contexts": ["Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (probability) for our confidence interval."], "ground_truth_answer": "ARES在其实验中使用95%的置信区间概率。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "分析ARES在跨域应用中的优势。", "ground_truth_contexts": ["In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross-domain applications. Across all settings, we found that LLM judges in ARES had strong generalizability... PPI helped mitigate the loss in accuracy and still allow ARES to be successful."], "ground_truth_answer": "ARES判断器在KILT和SuperGLUE的查询和文档类型上显示出强大的泛化能力，PPI缓解准确性损失，但在语言、代码或实体提取等剧烈转变中失败。", "retrieved_contexts": ["XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-"], "generated_answer": "ARES在跨域应用中表现出以下优势：\n\n1. **强泛化能力**：ARES中使用的微调LLM法官在不同领域（如从NQ到FEVER、MultiRC或ReCoRD）展现出良好的泛化性能，即使在训练和测试数据分布不同的情况下也能保持较高准确性。\n\n2. **少量样本适应性**：通过仅使用约300个人类偏好验证集的数据点进行微调，ARES仍能在跨域任务中实现良好表现。\n\n3. **PPI缓解精度损失**：尽管在跨域场景下法官模型的准确率可能下降，但通过PPI（Preference-based Performance Improvement）机制可以有效减轻这种影响，使ARES依然具备成功评估的能力。\n\n4. **持续提升潜力**：随着提供更多领域的示例用于PPI，ARES在后续跨域测试中的表现还能进一步提高。\n\n综上所述，ARES框架具有较强的跨域适用性和鲁棒性，特别适用于难以获取大量标注数据的专业领域部署。"}
{"question": "论文中提到的ARES的局限性是什么？", "ground_truth_contexts": ["ARES relies on a small set of annotations... These annotations often require an annotator familiar with the RAG system’s domain... The LLMs used in ARES benefit substantially from GPU-based hardware... Additionally, all of the datasets used in our evaluation of ARES are in English..."], "ground_truth_answer": "局限性包括需要领域专家标注、需要GPU硬件，并且仅在英文数据集上评估。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that"], "generated_answer": "根据论文中的内容，ARES的局限性主要体现在跨领域应用时的表现。具体来说：\n\n1. **领域适应性差**：当测试集与训练集在查询类型、文档类型或两者都发生变化时（例如从NQ数据集到FEVER、MultiRC或ReCoRD），ARES中使用的微调LLM judge的性能会下降。\n\n2. **需要重新配置**：每次进行跨领域迁移时，都需要提供该领域的段落和少量示例来重新配置ARES judge，这限制了其即插即用的能力。\n\n3. **不同任务表现不一**：在文本到代码（如CodeSearchNet）和抽取式任务（如T-Rex）等其他自然语言处理任务上，ARES judge的Kendall's tau值较低，表明它在这些任务上的泛化能力有限。\n\n综上所述，ARES的主要局限在于对特定领域和任务类型的依赖性强，在面对新的或不同的应用场景时需要额外的工作来进行调整和优化。"}
{"question": "ARES在AIS基准上的答案忠实度表现如何？", "ground_truth_contexts": ["Table 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores."], "ground_truth_answer": "ARES在AIS基准上对答案幻觉的预测在真实平均值的2.5个百分点以内。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "真实RAG系统评估中测试了哪些检索器和生成LLM？", "ground_truth_contexts": ["For our RAG systems, we selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search, and ColBERTv2... and three different generative LLMs (MPT-7b-Instruct, GPT-3.5-Turbo, and GPT-4)."], "ground_truth_answer": "检索器：BM25、OpenAI Ada嵌入、ColBERTv2。生成LLM：MPT-7b-Instruct、GPT-3.5-Turbo、GPT-4。", "retrieved_contexts": ["only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023a. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment, may 2023. arXiv preprint arXiv:2303.16634.\nYuxuan Liu, Tianchi Yang, Shaohan Huang, Zihan\nZhang, Haizhen Huang, Furu Wei, Weiwei Deng,\nFeng Sun, and Qi Zhang. 2023b. Calibrating llm-\nbased evaluator. arXiv preprint arXiv:2309.13308.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, Edouard Grave, Yann LeCun, and\nThomas Scialom. 2023. Augmented language mod-\nels: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "总结ARES在伪RAG系统排名上的结果。", "ground_truth_contexts": ["Table 1: ... Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets."], "ground_truth_answer": "ARES在Kendall’s τ上高于RAGAS（上下文相关性0.065，答案相关性0.132），并在使用78%更少标注的情况下优于采样标注。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES中有效PPI所需的最少人工标注数量是多少？", "ground_truth_contexts": ["We used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3)."], "ground_truth_answer": "ARES中有效PPI至少需要150个人工标注。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES是否支持非英语语言的评估？", "ground_truth_contexts": ["While LLM judges in ARES were successful in cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages...", "To test cross-lingual transfer, we used the XGLUE datasets... a LLM judge fine-tuned on NQ achieved a Kendall’s tau of 0.33..."], "ground_truth_answer": "不，ARES无法很好地泛化到非英语语言，在XGLUE上Kendall’s τ较低（例如0.33）。", "retrieved_contexts": ["it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "SuperGrok订阅的价格是多少？", "ground_truth_contexts": [], "ground_truth_answer": "文档中没有此信息。", "retrieved_contexts": ["its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-"], "generated_answer": "根据提供的资料无法回答。"}
