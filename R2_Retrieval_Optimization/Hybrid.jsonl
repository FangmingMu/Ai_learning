{"question": "ARES系统的全称是什么？", "ground_truth_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"], "ground_truth_answer": "ARES代表Automated RAG Evaluation System。", "retrieved_contexts": ["described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "erations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, Ope-\nnAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad-\nditionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-to-\nsequence model (Lewis et al., 2019). During re-\ntrieval, each RAG system only retrieves one pas-\nsage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES论文的作者是谁？", "ground_truth_contexts": ["Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com"], "ground_truth_answer": "作者是Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。", "retrieved_contexts": ["described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "labeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strat-\negy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information)."], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES报告的三个评估分数是什么？", "ground_truth_contexts": ["ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response generated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question)."], "ground_truth_answer": "三个评估分数是上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "erations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, Ope-\nnAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad-\nditionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-to-\nsequence model (Lewis et al., 2019). During re-\ntrieval, each RAG system only retrieves one pas-\nsage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES如何生成用于训练判断器的合成数据？", "ground_truth_contexts": ["Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus.", "We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query–passage–answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers)."], "ground_truth_answer": "ARES使用像FLAN-T5 XXL这样的语言模型从领域内段落生成合成问题-答案对，包括正面和负面示例，并使用检索器过滤低质量查询。", "retrieved_contexts": ["described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "labeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that\ncompare the candidate systems. Unfortunately,\nboth of these strategies demand high expertise and\nimpose considerable annotation costs.\nModel-based evaluation is an inexpensive strat-\negy to test generative output quality (Zheng et al.,\n2023). For instance, the open-source RAGAS\nframework (James and Es, 2023) prompts an LM\nfor evaluating the relevance of retrieved informa-\ntion and the faithfulness and accuracy of generated\nresponses. Unfortunately, such strategies currently\nrely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information)."], "generated_answer": "根据提供的资料无法回答。"}
{"question": "预测驱动推理（PPI）在ARES中的作用是什么？", "ground_truth_contexts": ["Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals for RAG scoring.", "PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our human preference validation set."], "ground_truth_answer": "PPI通过利用一小部分人工标注数据点来提高基于模型的评估准确性，为分数提供置信区间，结合标注和未标注预测。", "retrieved_contexts": ["and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,"], "generated_answer": "预测驱动推理（PPI）在ARES中的作用是提升其评分的准确性。通过利用PPI，ARES能够借助人类偏好验证集的数据来增强其判断的可靠性，并提高评估RAG系统时的排名准确性。此外，PPI还能缓解在跨领域应用中因领域差异导致的性能下降问题。"}
{"question": "ARES在评估过程中通常需要多少人工标注？", "ground_truth_contexts": ["ARES significantly improves data efficiency during evaluation by only requiring three inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more)."], "ground_truth_answer": "ARES需要大约150个或更多人工标注数据点在人类偏好验证集中。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers"], "generated_answer": "ARES在评估过程中通常需要约150个人工标注的数据点，用于计算其置信区间。此外，在PPI（Prediction-Powered Inference）的应用中，使用了300个人类注释作为人类偏好验证集。\n\n参考：\n- “we designate this annotated set as our human preference validation set, which is composed of approximately 150 annotated datapoints or more”\n- “For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human preference validation set.”"}
{"question": "ARES中使用哪些模型进行合成数据生成和判断器微调？", "ground_truth_contexts": ["For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge."], "ground_truth_answer": "FLAN-T5 XXL用于合成数据生成，DeBERTa-v3-Large用于微调LLM判断器。", "retrieved_contexts": ["described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "erations to be correct if they contained the KILT\nanswer in their output. For our RAG systems,\nwe selected three different retrievers (BM25, Ope-\nnAI Ada embeddings with cosine similarity search,\nand ColBERTv2 (Santhanam et al., 2022)) and\nthree different generative LLMs (MPT-7b-Instruct\n(Team, 2023), GPT-3.5-Turbo, and GPT-4). Ad-\nditionally, we include the Facebook RAG model\n(Lewis et al., 2020), which uses a DPR retriever\n(Karpukhin et al., 2020) and BART sequence-to-\nsequence model (Lewis et al., 2019). During re-\ntrieval, each RAG system only retrieves one pas-\nsage to assist generation.\nIn Table 5, we found that ARES can reliably\nscore and rank RAG systems in real-world applica-\ntions, averaging a Kendall’s tau of 0.91 for context\nrelevance and 0.97 for answer relevance. Com-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "总结ARES管道的三个阶段。", "ground_truth_contexts": ["ARES proceeds in three stages (Figure 1). ... First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus. Second, it defines three separate judge models to perform three classification tasks... Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI)."], "ground_truth_answer": "阶段1：从语料段落生成合成查询和答案。阶段2：为上下文相关性、答案忠实度和答案相关性微调LLM判断器。阶段3：使用判断器和PPI为RAG系统评分以获得置信区间。", "retrieved_contexts": ["described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "ever, while the fine-tuned LLM judge had a higher\nKendall’s tau on average, the GPT-3.5 judge is\nmore readily deployable and does not require any\nadditional fine-tuning. The GPT-3.5 judge does\ncome with its own querying costs, which can vary\nbased on the date of querying as well as the total\ntokens used in evaluation.\nWe also wanted to better understand the impor-\ntance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "compared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the hu-\nman preference validation set with the much larger\nset of datapoints with ML predictions, PPI can de-\nvelop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confi-\ndence bounds for the success and failure rates of the\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (proba-\nbility) for our confidence interval.\nWith the accuracy confidence interval for each", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "after changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction\nRetrieval-augmented generation (RAG) has be-\ncome a prominent approach for building user-\nfacing NLP applications, such as systems for ques-\ntion answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to gener-\nate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "Shyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262.\nOmar Khattab, Christopher Potts, and Matei Zaharia.\n2021. Relevance-guided supervision for openqa with\ncolbert. Transactions of the association for computa-\ntional linguistics, 9:929–944.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization.\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "PPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-\nGLUE, LLM judges are unable to generalize when\nmaking more drastic shifts in domain, such as:\nswitching languages (e.g. English to Spanish, Ger-\nman, and other languages), switching from text to\ncode (e.g. questions + passages to coding functions\n+ documentation), and switching from retrieving\ntext to extraction of entities, webpages, or citations.\nTo test cross-lingual transfer, we used the\nXGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "比较ARES和RAGAS在上下文相关性和答案相关性方面的性能。", "ground_truth_contexts": ["ARES averages a Kendall’s τ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS. ... For context relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS while for answer relevance, our system is 14.4 percentage points higher than RAGAS."], "ground_truth_answer": "ARES在Kendall’s τ上优于RAGAS（上下文相关性0.065，答案相关性0.132），并在准确性上更高（上下文相关性59.9%点，答案相关性14.4%点）。", "retrieved_contexts": ["and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance", "logits in LLM judge prediction to improve PPI\nconfidence intervals, and testing more sophisticated\nLLMs as fine-tuned judges for ARES.\n7 Limitations\nARES relies on a small set of annotations in the\nhuman preference validation set (roughly 150-300\ndatapoints but more is better). These annotations\noften require an annotator familiar with the RAG\nsystem’s domain application. While these annota-\ntions can be easy to generate for general-domain\napplications, more specialized domains, such as\nlaw, medicine, and finance, may require annotators\nwith specialized expertise.\nThe LLMs used in ARES benefit substantially\nfrom GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction"], "generated_answer": "根据提供的资料，ARES 在上下文相关性（Context Relevance）和答案相关性（Answer Relevance）方面的性能优于 RAGAS。\n\n具体来说：\n\n- **上下文相关性**：在多个数据集（如 NQ、FEVER、MultiRC 和 ReCoRD）中，ARES 的 Kendall’s Tau 接近或达到 1.0，显著高于 RAGAS。\n- **答案相关性**：同样地，在这些评估中，ARES 的 Kendall’s Tau 明显更高。例如，在某些情况下，RAGAS 的 Kendall's Tau 为 0.73 或更低，而 ARES 可达 0.96 或更高。\n\n此外，ARES 使用了专门针对每个评估领域微调的轻量级语言模型法官，并利用预测驱动推理（PPI）来提高评分准确性，相较之下，RAGAS 使用的是非针对性的少量示例提示，缺乏适应性和保证质量的能力。\n\n因此，综合来看，ARES 在这两项指标上的表现明显优于 RAGAS。"}
{"question": "用于评估ARES的KILT和SuperGLUE数据集有哪些？", "ground_truth_contexts": ["From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW)... From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD."], "ground_truth_answer": "KILT数据集：Natural Questions (NQ)、HotpotQA、FEVER、Wizards of Wikipedia (WoW)。SuperGLUE数据集：MultiRC和ReCoRD。", "retrieved_contexts": ["Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES如何处理合成数据生成中的负面示例？", "ground_truth_contexts": ["To generate negatives for fine-tuning our LLM judges, we rely on two novel strategies... 1. Weak Negative Generation: ... 2. Strong Negative Generation: ..."], "ground_truth_answer": "ARES使用弱负面（随机采样）和强负面（从相似段落采样或生成矛盾答案）用于上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": ["described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "Udapdr: Unsupervised domain adaptation via llm\nprompting and distillation of rerankers. arXiv\npreprint arXiv:2303.00807.\nDavid P Sander and Laura Dietz. 2021. Exam: How\nto evaluate retrieve-and-generate systems for users\nwho do not (yet) know what they want. In DESIRES,\npages 136–146.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nBERTv2: Effective and efficient retrieval via\nlightweight late interaction. In Proceedings of the\n2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3715–3734, Seat-\ntle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "document type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM\njudges used in ARES proved successful in cross-\ndomain applications. Across all settings, we found\nthat LLM judges in ARES had strong generaliz-\nability, even when only using 300 datapoints in our\nhuman preference validation set for PPI. Further-\nmore, we found that even when the LLM judge’s ac-\ncuracy suffered in cross-domain applications, PPI\nhelped mitigate the loss in accuracy and still allow\nARES to be successful. Additional examples for\nPPI also continued to boost cross-domain ARES\nperformance in subsequent tests.\nWhile LLM judges in ARES were successful\nin cross-domain applications for KILT and Super-", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES实验中使用的置信区间概率是多少？", "ground_truth_contexts": ["Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (probability) for our confidence interval."], "ground_truth_answer": "ARES在其实验中使用95%的置信区间概率。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "分析ARES在跨域应用中的优势。", "ground_truth_contexts": ["In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross-domain applications. Across all settings, we found that LLM judges in ARES had strong generalizability... PPI helped mitigate the loss in accuracy and still allow ARES to be successful."], "ground_truth_answer": "ARES判断器在KILT和SuperGLUE的查询和文档类型上显示出强大的泛化能力，PPI缓解准确性损失，但在语言、代码或实体提取等剧烈转变中失败。", "retrieved_contexts": ["and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "we use a single linear classification layer and ap-\nply a 0.1 dropout to the input, which is the final\nhidden state of the [CLS] token. For our learning\nschedule, we use linear warmup and linear decay\n(Howard and Ruder, 2018) with a 5e-6 learning rate\nand a 32 training batch size across all experimental\nconfigurations.\nA.2 GPT Prompting for Context Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score context relevance:\n• Given the following question and document,\nyou must analyze the provided document and\ndetermine whether it is sufficient for answer-\ning the question. In your evaluation, you\nshould consider the content of the document\nand how it relates to the provided question."], "generated_answer": "根据提供的资料，ARES在跨域应用中的优势主要体现在以下几个方面：\n\n1. **强大的泛化能力**：ARES框架中的微调LLM法官在不同的查询类型（如NQ和FEVER）、文档类型（如NQ和MultiRC）或两者兼而有之的情况下均表现出良好的通用性。\n\n2. **保持高准确性**：即使在跨域场景中，ARES法官仍能维持较高的评估准确性。例如，在某些情况下，尽管法官的准确率可能会下降，但通过预测驱动推断（PPI）机制可以缓解这种性能下降的问题。\n\n3. **提升排名精度**：与不使用PPI的情况相比，加入PPI后进一步提升了法官的排名准确性。\n\n这些特点使得ARES能够在不同领域之间有效切换并保持高效、精准的评估能力。"}
{"question": "论文中提到的ARES的局限性是什么？", "ground_truth_contexts": ["ARES relies on a small set of annotations... These annotations often require an annotator familiar with the RAG system’s domain... The LLMs used in ARES benefit substantially from GPU-based hardware... Additionally, all of the datasets used in our evaluation of ARES are in English..."], "ground_truth_answer": "局限性包括需要领域专家标注、需要GPU硬件，并且仅在英文数据集上评估。", "retrieved_contexts": ["described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "XGLUE datasets (Liang et al., 2020); a LLM judge\nfine-tuned on NQ achieved a Kendall’s tau of 0.33\nover both context relevance and answer relevance\nscoring for XGLUE. To test text-to-code, we used\nCodeSearchNet (Husain et al., 2019); an LLM\njudge fine-tuned on NQ achieved a Kendall’s tau\nof 0.28 over both context relevance and answer\nrelevance scoring for CodeSearchNet. To test ex-\ntraction task generalizability, we used T-Rex from\nKILT (Elsahar et al., 2018; Petroni et al., 2021); an\nLLM judge fine-tuned on NQ achieved a Kendall’s\ntau of 0.38 over both context relevance and answer\nrelevance scoring for T-Rex. Each cross-domain\nshift requires in-domain passages and few-shot\nquery examples for reconfiguring ARES judges.\n6 Conclusion\nIn this work, we present ARES, a novel automated", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "tinguish faithful and hallucinated answers in real-\nworld RAG systems.\n5.3 ARES Ranking of Existing RAG Systems\nWe also wanted to evaluate whether ARES can\nscore and rank existing RAG systems across both\ncontext relevance and answer relevance. For eval-\nuation, we selected the NQ, WoW, and FEVER\ndatasets from KILT. We consider the answer gen-", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "judges. To test the limits of ARES, we need to sim-\nulate the existence of many RAG systems that are\nseparated by small accuracy margins on our eval-\nuation metrics. For this, we create systems using\nartificial query-passage-answer triples, in which\nwe empirically know the positive and negative ex-\namples of the mock RAG system. We generate\nthese mock splits of the given datasets by select-\ning (1) The positive and negative query-passage\nmatches for context relevance, and (2) the positive\nand negative query-passage-answer matches for an-\nswer relevance. We include positive and negative\nexamples from our evaluation sets in Table 7.\nFor our positive triples, we can simply use the\nKILT and SuperGLUE examples without any al-\nteration. For gathering negative query-passage", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "RAG system finds relevant contexts and generates\nanswers that are both faithful and relevant.\nMany existing RAG evaluation frameworks re-\nquire substantial human annotations for scoring.\nARES significantly improves data efficiency dur-\ning evaluation by only requiring three inputs: an in-\ndomain passage set, a human preference validation\nset of approximately 150 annotated datapoints or\nmore, and few-shot examples of in-domain queries\nand answers (e.g. five examples or more), which\nare used for prompting LLMs in synthetic data gen-\neration.\nGiven the corpus of in-domain passages, ARES\nproceeds in three stages. First, it leverages an LM\nto construct a synthetic dataset of question–answer\npairs, derived from the passages in the corpus. Sec-\nond, it defines three separate judge models to per-", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "guarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,\nsuch as translation, summarization, and dialogue\n(Kocmi and Federmann, 2023; Fu et al., 2023; Liu\net al., 2023a; Wang et al., 2023).\nIn the context of knowledge-intensive NLP tasks,\nLLMs have been explored for assessing attribution\nand factuality in LLMs (Min et al., 2023; Gekhman\net al., 2023; Yue et al., 2023). New guidelines\nlike LongEval (Krishna et al., 2023) and datasets\nlike Hagrid and ALCE (Kamalloo et al., 2023;\nGao et al., 2023) provide resources for analyzing\nknowledge-intensive LLM pipelines.\nThe two most-closely related projects to ARES\nare EXAM (Sander and Dietz, 2021) and RA-\nGAS (James and Es, 2023). To evaluate RAG sys-", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES在AIS基准上的答案忠实度表现如何？", "ground_truth_contexts": ["Table 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores."], "ground_truth_answer": "ARES在AIS基准上对答案幻觉的预测在真实平均值的2.5个百分点以内。", "retrieved_contexts": ["Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "ate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how\nto prompt or finetune the LM to use the retrieved\ninformation, to name only a few of the simplest\ndesign decisions.\n∗Project started during research internship at Databricks\nThe best design for a RAG system is not neces-\nsarily universal across data domains, corpus sizes,\nand cost/latency budgets. To tune their own RAG\nsystems, practitioners traditionally need hand an-\nnotations for test questions, passages to retrieve\n(to assess the retriever), and responses to generate,\nlabeled specifically for their target domain. Alter-\nnatively, they may evaluate different approaches in\nproduction by collecting human preferences that", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "annotated datapoints for computing its confidence\nintervals; we designate this annotated set as our hu-\nman preference validation set, which is composed\nof approximately 150 annotated datapoints or more\nthat designate both positive and negative examples\nfor context relevance, answer faithfulness, and an-\nswer relevance.\nWe conduct extensive empirical evaluations,\ndemonstrating that ARES accurately scores\nRAG systems across the six knowledge-intensive\ndatasets in KILT and SuperGLUE, beating exist-\ning automated evaluation approaches like RAGAS\nby 59.3 and 14.4 percentage points on average\nacross context relevance and answer relevance eval-\nuation accuracy, respectively. Additionally, ARES\naccurately calculates answer hallucination occur-", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "真实RAG系统评估中测试了哪些检索器和生成LLM？", "ground_truth_contexts": ["For our RAG systems, we selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search, and ColBERTv2... and three different generative LLMs (MPT-7b-Instruct, GPT-3.5-Turbo, and GPT-4)."], "ground_truth_answer": "检索器：BM25、OpenAI Ada嵌入、ColBERTv2。生成LLM：MPT-7b-Instruct、GPT-3.5-Turbo、GPT-4。", "retrieved_contexts": ["described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "from GPU-based hardware with substantial stor-\nage. In ARES, DeBERTa-v3-Large (304M) and\nFLAN-T5-XXL (11.3B) required GPUs with about\n32GB of memory to run, taking several hours for\nfine-tuning and generation, respectively. While\ncommercial GPUs are widely available, they are\nnot easily accessible to all NLP researchers and\npractitioners due to their costs.\nAdditionally, all of the datasets used in our eval-\nuation of ARES are in English, a well-resourced\nlanguage with abundant annotations. Future work\nshould explore how ARES can be employed in\nother languages by utilizing different LLMs for\nthe ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "ity search over in-domain passages, we use FAISS\nIndexFlatL2 for indexing (Johnson et al., 2019)\nand OpenAI’s text-embedding-ada-002 for gener-\nating embeddings. We use simlarity search over\nin-domain passages to filter our synthetic queries\nthat cannot retrieve the passage from which they\nwere generated. We use version 0.0.18 of RAGAS\nin our experiments (James and Es, 2023).\n4.2 Datasets\nOur core experimental goal is to provide a rich\npicture of where ARES can be applied effectively.\nTo test across multiple types of queries, documents,\nand answers, we selected all the datasets from the\nwidely-used KILT and SuperGLUE benchmarks\nfor which RAG is appropriate.\nFrom KILT (Petroni et al., 2021), we use Natural\nQuestions (NQ), HotpotQA, FEVER, and Wizards", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nSearchNet challenge: Evaluating the state of seman-\ntic code search. arXiv preprint arXiv:1909.09436.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nJithin James and Shahul Es. 2023. Ragas: Evaluation\nframework for your retrieval augmented generation\n(rag) pipelines.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "after changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction\nRetrieval-augmented generation (RAG) has be-\ncome a prominent approach for building user-\nfacing NLP applications, such as systems for ques-\ntion answering (QA), fact-checking, and customer\nsupport (Petroni et al., 2021; Wang et al., 2019).\nTypically, a RAG system consists of a retriever and\na downstream language model (LM). Given a user\nquestion, the retriever finds relevant passages from\na corpus and the LM uses these passages to gener-\nate a response. This formulation admits a multitude\nof choices: what retrieval model to use, how to di-\nvide the documents into retrieval chunks, and how", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "总结ARES在伪RAG系统排名上的结果。", "ground_truth_contexts": ["Table 1: ... Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets."], "ground_truth_answer": "ARES在Kendall’s τ上高于RAGAS（上下文相关性0.065，答案相关性0.132），并在使用78%更少标注的情况下优于采样标注。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023. Benchmarking large language models in\nretrieval-augmented generation. arXiv preprint\narXiv:2309.01431.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "preprint arXiv:2302.04166.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations.\nZorik Gekhman, Jonathan Herzig, Roee Aharoni, Chen\nElkind, and Idan Szpektor. 2023. Trueteacher: Learn-\ning factual consistency evaluation with large lan-\nguage models.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. arXiv preprint arXiv:2111.09543.\nJeremy Howard and Sebastian Ruder. 2018. Universal", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "the dimensions of context relevance, answer\nfaithfulness, and answer relevance. By cre-\nating its own synthetic training data, ARES\nfinetunes lightweight LM judges to assess the\nquality of individual RAG components. To\nmitigate potential prediction errors, ARES uti-\nlizes a small set of human-annotated datapoints\nfor prediction-powered inference (PPI). Across\neight different knowledge-intensive tasks in\nKILT, SuperGLUE, and AIS, ARES accurately\nevaluates RAG systems while using only a few\nhundred human annotations during evaluation.\nFurthermore, ARES judges remain effective\nacross domain shifts, proving accurate even\nafter changing the type of queries and/or docu-\nments used in the evaluated RAG systems. We\nmake our code and datasets publicly available\non Github.\n1 Introduction", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "compared to simply using annotated outputs from\nthe evaluated RAG system. By bolstering the hu-\nman preference validation set with the much larger\nset of datapoints with ML predictions, PPI can de-\nvelop reliable confidence intervals for ML model\nperformance that beat previous classical inference\napproaches.\nThe PPI rectifier function allows us to estimate\nthe errors of the LLM judge and generate confi-\ndence bounds for the success and failure rates of the\nRAG system, estimating context relevance, answer\nfaithfulness, and answer relevance performance.\nAdditionally, PPI allows us to estimate confidence\nintervals with a selected level of probability; for our\nexperiments, we use a standard 95% alpha (proba-\nbility) for our confidence interval.\nWith the accuracy confidence interval for each", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "rely for evaluation on a fixed set of heuristically\nhand-written prompts, offering little adaptability\nto various evaluation contexts and no guarantees\nabout quality.\nTo evaluate RAG systems rapidly and accu-\nrately, we propose ARES, the Automated RAG\nEvaluation System. ARES is the first automated\nRAG evaluation system to generate tailored LLM\njudges for each component of a RAG pipeline, lead-\ning to substantial boosts in evaluation precision and\naccuracy compared to existing approaches like RA-\nGAS. Furthermore, unlike existing RAG evaluation\nsystems, ARES provides confidence intervals for\nits scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES中有效PPI所需的最少人工标注数量是多少？", "ground_truth_contexts": ["We used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3)."], "ground_truth_answer": "ARES中有效PPI至少需要150个人工标注。", "retrieved_contexts": ["and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "quiring RAG-based solutions, we found that ARES\ncan accurately score and rank RAG systems based\non context relevance, answer faithfulness, and an-\nswer relevance scores, beating the existing RAGAS\nautomated evaluation framework.\nARES is a flexible framework, and there may\nbe variants of it that are even more powerful than\nthe ones we explored here. Avenues to explore\ninclude GPT-4 as a replacement for human labeling\n(Table 4), more robust techniques for the synthetic\ndatasets used in fine-tuning LLM judges, utilizing", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "els: a survey.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike\nLewis, Wen tau Yih, Pang Wei Koh, Mohit Iyyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2023.\nFactscore: Fine-grained atomic evaluation of factual\nprecision in long form text generation.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. 2021. KILT: a benchmark for knowledge\nintensive language tasks. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2523–2544, Online.\nAssociation for Computational Linguistics.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,", "ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "ever, while the fine-tuned LLM judge had a higher\nKendall’s tau on average, the GPT-3.5 judge is\nmore readily deployable and does not require any\nadditional fine-tuning. The GPT-3.5 judge does\ncome with its own querying costs, which can vary\nbased on the date of querying as well as the total\ntokens used in evaluation.\nWe also wanted to better understand the impor-\ntance of human annotations for ARES. To this end,\nwe conducted two sets of experiments. First, we"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES是否支持非英语语言的评估？", "ground_truth_contexts": ["While LLM judges in ARES were successful in cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages...", "To test cross-lingual transfer, we used the XGLUE datasets... a LLM judge fine-tuned on NQ achieved a Kendall’s tau of 0.33..."], "ground_truth_answer": "不，ARES无法很好地泛化到非英语语言，在XGLUE上Kendall’s τ较低（例如0.33）。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "offer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-\ntial for rapid deployment in new settings, where it\nis difficult to build a traditional benchmark dataset\nfrom scratch. Early attempts at this use LLMs\nout of the box, as in MT-Bench and Chatbot\nArena (Zheng et al., 2023). AutoCalibrate (Liu\net al., 2023b) seeks to align an LLM-judge with\nhuman preferences, leveraging a self-refinement\nprompt to iteratively improve the LLM judge. How-\never, AutoCalibrate does not offer any statistical\nguarantees for the accuracy of its predictions. Other\nwork has used LLM prompting to evaluate system\nquality across natural language generation tasks,", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "its scoring by leveraging prediction-powered in-\nference (PPI; Angelopoulos et al. 2023). Given a\ncorpus of documents and a RAG system, ARES\nreports three evaluation scores: context relevance\n(is the retrieved information pertinent to the test\nquestion), answer faithfulness (is the response gen-\nerated by the language model properly grounded\nin the retrieved context), and answer relevance (is\nthe response also relevant to the question). A good\narXiv:2311.09476v2  [cs.CL]  31 Mar 2024", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "Jeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nEhsan Kamalloo, Aref Jafari, Xinyu Zhang, Nandan\nThakur, and Jimmy Lin. 2023. Hagrid: A human-\nllm collaborative dataset for generative information-\nseeking with attribution.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-", "the ARES judge and the synthetic data generation.\nThis can help us better understand the strengths and\nweaknesses of the current ARES framework.\nReferences\nMubashara Akhtar, Rami Aly, Christos\nChristodoulopoulos, Oana Cocarascu, Zhijiang Guo,\nArpit Mittal, Michael Schlichtkrull, James Thorne,\nand Andreas Vlachos, editors. 2023. Proceedings of\nthe Sixth Fact Extraction and VERification Workshop\n(FEVER). Association for Computational Linguistics,\nDubrovnik, Croatia.\nAnastasios N. Angelopoulos, Stephen Bates, Clara Fan-\nnjiang, Michael I. Jordan, and Tijana Zrnic. 2023.\nPrediction-powered inference.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,"], "generated_answer": "根据提供的资料无法回答。"}
{"question": "SuperGrok订阅的价格是多少？", "ground_truth_contexts": [], "ground_truth_answer": "文档中没有此信息。", "retrieved_contexts": ["ARES Ranking of Real RAG Systems\nNQ WoW FEVER\nC.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau for\nSampled Annotations 0.73 0.78 0.73 0.73 0.73 0.82\nKendall’s Tau\nfor RAGAS 0.82 0.82 0.73 0.82 0.73 0.87\nKendall’s Tau\nfor GPT-3.5 Judge 0.82 0.87 0.82 0.82 0.64 0.87\nKendall’s Tau\nfor ARES LLM Judge 0.91 0.96 0.91 1.0 0.73 0.87\nKendall’s Tau\nfor ARES 1.0 0.96 0.91 1.0 0.82 1.0\nRAGAS Accuracy 35.9% 68.2% 44.4% 80.1% 21.4% 75.9%\nGPT-3.5 Accuracy 80.5% 91.2% 81.2% 83.5% 61.3% 54.5%\nARES Accuracy 85.6% 93.3% 84.5% 88.2% 70.4% 84.0%\nTable 5: ARES Ranking on Real-World RAG Systems: For scoring context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled", "described Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that\nthe church made of Einstein, standing at the entrance.\nVasily Szaitsev portrayed\nPushkin in the film\nPushkin Returns\n0 0\nTable 7: Positive and Negatives Evaluation Examples", "the following statement and document, you\nmust analyze the provided document and de-\ntermine whether it is sufficient for determining\nthe statement’s factuality. In your evaluation,\nyou should consider the content of the docu-\nment and how it relates to the provided state-\nment’s factuality. Output your final verdict\nby strictly following this format: \"[[Yes]]\" if\nthe document is sufficient and \"[[No]]\" if the\ndocument is not sufficient. Do not provide any\nadditional explanation for your decision.\nStatement: <few-shot example here>\nDocument: <few-shot example here>\nFor WoW, we use the following prompt to score\ncontext relevance:\n• You are an expert dialogue agent. Given the\nfollowing dialogue and document, you must", "and travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,\nand who portrayed Pushkin?\nAfter arriving in New York City, Einstein was taken to various places and\nevents, including Chinatown, a lunch with the editors of the New York\nTimes, and a performance of Carmen at the Metropolitan Opera,\nwhere he was cheered by the audience on his arrival.\nDuring the days following, he was given the keys to the city by Mayor\nJimmy Walker and met the president of Columbia University, who\ndescribed Einstein as \"The ruling monarch of the mind.\" Harry\nEmerson Fosdick, pastor at New York’s Riverside Church, gave\nEinstein a tour of the church and showed him a full-size statue that", "it contain hallucinated or extrapolated state-\nments beyond the passage?\n3. Answer Relevance: Is the answer generated\nrelevant given the query and retrieved pas-\nsage?\nFor each metric, a separate LLM with a binary\nclassifier head is fine-tuned to classify positive and\nnegative examples. For each concatenated query-\ndocument-answer, a single LLM judge must clas-\nsify the triple as positive or negative for that judge’s\nmetric. To fine-tune these judges, we use our hu-\nman preference validation set to evaluate model\nimprovement after each epoch, stopping when we\nhave three epochs with no improvement in loss (see\nsubsection A.1 for more information).", "Assigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a\nblade smuggled out of the workshop . After killing Danvers the stepfather\nbeats a suspicious guard named Ralph Smith to death with his own nightstick\nwith only two strikes and takes his uniform , successfully\nsneaking out of the sanitarium . Checking into a hotel after robbing and\nmurdering a traveling salesman the stepfather alters his appearance ,\ntakes the name Doctor Gene F. Clifford from the newspaper obituaries\nand travels to Palm Meadows , Los Angeles after seeing an ad for it on\nan episode of Dream House .\nLos Angeles 1 0\nWhat was the name of the\n2006 film about Pushkin’s death,", "only a few points apart in ground-truth metrics.\nThis precision enables ARES to guide the develop-\nment and comparison of competitive approaches\nand configurations.\nWe make the ARES code and datasets publicly\navailable on Github.\n2 Related Work\nRAG (Guu et al., 2020; Lewis et al., 2020; Khat-\ntab et al., 2021; Izacard et al., 2022)) is now a\ncommon strategy for bolstering LLMs by combin-\ning them with retrieval systems. Through retrieval,\nRAG helps LM systems gather domain-specific\nknowledge, ground generations in factual informa-\ntion (Shuster et al., 2021; Huo et al., 2023), and\noffer a degree of transparency or interpretability\nvia citing sources (Mialon et al., 2023).\nMultiple LLM-based evaluation techniques have\nemerged for gauging LLM systems. This is essen-", "Bethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law\npractice catering to working-class families - mostly Latino -\nin the shadow of the hulkish remnants of Bethlehem Steel.\nFred Rooney 1 1\nBefore he murder the\ndoctor and Ralph Smith,\nwhere did the stepfather\nreside?\nSurviving being shot and stabbed at the end of the previous film ,\nthe stepfather has been institutionalized in Puget Sound, Washington since ,\nspending his time building model houses in the workshop.\nAssigned a new doctor named Joseph Danvers the stepfather\nbegins confiding in him to gain his trust , ultimately murdering\nthe doctor during a session by stabbing him in the neck with a", "provide any additional explanation for your\ndecision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.4 GPT Prompting for Answer Relevance\nScoring\nFor the NQ, HotpotQA, MultiRC, and ReCoRD\ndatasets, we use 8 few-shot examples with the fol-\nlowing prompt to score answer relevance:\n• Given the following question, document, and\nanswer, you must analyze the provided answer\nand document before determining whether\nthe answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-", "Query Passage Answer Context\nRelevance\nAnswer\nRelevance\nHow can a ball that is not\nmoving possess energy\nof position?\nMechanical energy is a combination of the energy of motion or position.\nThis type of energy describes objects that are moving or could move.\nA moving ball can have energy from motion. An arrow can also have\nthe energy of motion. Both are types of mechanical energy.\nThe ball holds\nmechanical energy 1 1\nWho has a Jimmy\nStewart-like quality\nof quiet trust?\nOne look at Fred Rooney, and you just know he’s the good guy.\nA trace of childish innocence in his face gives the lanky\nBethlehem lawyer a Jimmy Stewart-like quality of quiet trust.\nIn black jeans and button-down shirt, he’s a kind of folk hero\nin the south Bethlehem melting pot where he’s crafted a law", "when changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and\nReCoRD). For PPI, we used 300 labeled examples for our human preference validation set but also found that\nadditional examples further improved the performance of ARES. Furthermore, we found that even in scenarios\nwhere the fine-tuned LLM judge’s accuracy significantly dropped out-of-domain (e.g. answer relevance for NQ\nto FEVER), PPI mitigated the decrease in judge performance. In the table, we define PPI range as the number of\npercentage points from the lower bound to the upper bound of the PPI confidence interval.", "model predictions.\nAs an additional comparison, we also include\nthe Kendall’s τ for RAG ranking with the ARES\nLLM judge without PPI; for all datasets tested, PPI\nimproved the ranking prediction accuracy of the\nfine-tuned LLM judge. Furthermore, we included\na sampled annotations configuration, in which we\nsampled 150-datapoints from each mock RAG sys-\ntem, totalling 1,350 annotations. Even with all\nthese annotations, the Kendall’s τ for ARES is\n0.08 higher on average, across both context and an-\nswer relevance, compared to sampled annotations,\ndespite using 78% less annotations. In sum, ARES\nproves significantly more data-efficient with human\nannotations while being more accurate at scoring\nthan standard sampled annotation methods.\nCompared to the GPT-3.5 judge, ARES provides", "Kendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on\nRAG Evaluation Sets 92.4% 28.4% 85.7% 22.6% 81.5% 92.1% 87.6% 80.2% 29.1% 81.2% 80.1% 92.1%\nTable 6: Cross-Domain Usage of Fine-tuned LLM Judges : We tested the cross-domain application of the\nfine-tuned LLM judge in the ARES framework. We found that for both context relevance and answer relevance\n(C.R. and A.R. in the table, respectively), fine-tuned LLM judges showed strong generalizability across domains\nwhen changing query type (e.g. NQ and FEVER), document type (e.g. NQ and MultiRC), or both (e.g. NQ and", "the answer is relevant for the provided ques-\ntion. In your evaluation, you should consider\nwhether the answer addresses all aspects of\nthe question and provides only correct infor-\nmation from the document for answering the\nquestion. Output your final verdict by strictly\nfollowing this format: \"[[Yes]]\" if the answer\nis relevant for the given question and \"[[No]]\"\nif the answer is not relevant for the given ques-\ntion. Do not provide any additional explana-\ntion for your decision.\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nFor FEVER, we change the word \"question\" in\nthe prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers", "the ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial\ncosts required to run. For PPI in both ARES and the GPT-3.5 judge, we used 300 human annotations for our human\npreference validation set. The prompts used for the GPT-3.5 judges are included in Sections A.2, A.3, and A.4.\nARES Cross-Domain Ranking of Pseudo RAG Systems\nNQ to\nFEVER\nFEVER to\nNQ\nNQ to\nMultiRC\nMultiRC to\nNQ\nNQ to\nReCoRD\nReCoRD to\nNQ\nC.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R. C.R. A.R.\nKendall’s Tau 0.89 0.89 1.0 0.83 0.94 0.89 1.0 0.89 0.78 0.89 0.89 0.94\nKendall’s Tau of\nIn-Domain LLM Judge 0.89 0.78 0.94 1.0 0.94 0.89 0.94 1.0 0.83 0.89 0.94 1.0\nAverage PPI Range 8.7% 7.2% 6.5% 11.5% 10.2% 11.3% 11.9% 11.5% 10.5% 10.1% 9.7% 6.2%\nAccuracy on", "answer relevance (see Figure 2 and Figure 3 for\nARES vs. RAGAS). Among the models tested, the\nbest performing retriever was ColBERTv2 while\nthe best performing generative LLM was GPT-4.\n5.4 Strengths and Limits of Cross-Domain\nApplications\nThe generalizability of the LLM judge used in\nARES is critical for deploying our framework in\nspecialized domains, particularly domains where\nin-domain queries, documents, and answers are dif-\nficult to gather. Therefore, we wanted to test how\nthe LLM judges used in ARES would be affected\nby three domain shifts: change in query type from\ntraining to test (e.g. NQ to FEVER), change in\ndocument type from training to test (e.g. NQ to\nMultiRC), and change in both query and document\ntype (e.g. NQ to ReCoRD).\nIn Table 6, we found that the fine-tuned LLM", "(C.R. and A.R. in the table, respectively), we compare ARES with our fine-tuned LLM judges against sampled\nannotations benchmark, RAGAS, and a few-shot GPT-3.5 judge. For our sampled annotations, we gather 150\nannotated datapoints from each mock RAG system and use those labels to score the system. RAGAS also uses\nGPT-3.5 as its judge but it uses few-shot prompts that are not targeted for each evaluation domain. Overall, we\nfound that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets.\nAdditionally, we include the Kendall’s taus for the ARES LLM Judge without PPI and found that PPI further boosted\nthe ranking accuracy of the judge across the board. We selected GPT-3.5 instead of GPT-4 due to the lower financial", "tle, United States. Association for Computational\nLinguistics.\nKurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela,\nand Jason Weston. 2021. Retrieval augmentation\nreduces hallucination in conversation.\nMosaicML NLP Team. 2023. Introducing mpt-30b:\nRaising the bar for open-source foundation models.\nAccessed: 2023-06-22.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.", "the prompt to \"statement\". For WoW, we change\nthe word \"question\" in the prompt to \"dialogue\".\nA.5 Prompting for Generation of Synthetic\nQueries and Answers\nTo generate synthetic queries and answers using\nFLAN-T5, we use the following prompt and pro-\nvide 5 few-shot examples:\n• Example N\nQuestion: <few-shot example here>\nDocument: <few-shot example here>\nAnswer: <few-shot example here>\nWe use the same prompting structure for gener-\nating incorrect or contradictory answers; we simply\nswap out the few-shot examples to be incorrect or\ncontradictory instead.\nA.6 Synthetic Query and Answer Generation\nFor generating our synthetic questions, we use the\nfollowing prompt for FLAN-T5 XXL:\n• Example #1\nDocument: <few-shot example here>\nQuery: <few-shot example here>\nExample #2"], "generated_answer": "根据提供的资料无法回答。"}
