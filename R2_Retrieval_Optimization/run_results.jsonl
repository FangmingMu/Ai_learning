{"question": "ARES系统的全称是什么？", "ground_truth_contexts": ["ARES: An Automated Evaluation Framework for Retrieval-Augmented Generation Systems"], "ground_truth_answer": "ARES代表Automated RAG Evaluation System。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES论文的作者是谁？", "ground_truth_contexts": ["Jon Saad-Falcon Stanford University ∗ jonsaadfalcon@stanford.edu Omar Khattab Stanford University okhattab@stanford.edu Christopher Potts Stanford University cgpotts@stanford.edu Matei Zaharia Databricks and UC Berkeley matei@databricks.com"], "ground_truth_answer": "作者是Jon Saad-Falcon、Omar Khattab、Christopher Potts和Matei Zaharia。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES报告的三个评估分数是什么？", "ground_truth_contexts": ["ARES reports three evaluation scores: context relevance (is the retrieved information pertinent to the test question), answer faithfulness (is the response generated by the language model properly grounded in the retrieved context), and answer relevance (is the response also relevant to the question)."], "ground_truth_answer": "三个评估分数是上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES如何生成用于训练判断器的合成数据？", "ground_truth_contexts": ["Given the corpus of in-domain passages, ARES proceeds in three stages. First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus.", "We generate synthetic queries and answers from the corpus passages using generative LLMs. The generated data represent both positive and negative examples of query–passage–answer triples (e.g., relevant/irrelevant passages and correct/incorrect answers)."], "ground_truth_answer": "ARES使用像FLAN-T5 XXL这样的语言模型从领域内段落生成合成问题-答案对，包括正面和负面示例，并使用检索器过滤低质量查询。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "预测驱动推理（PPI）在ARES中的作用是什么？", "ground_truth_contexts": ["Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI; Angelopoulos et al. 2023) to improve model-based evaluation accuracy and provide statistical confidence intervals for RAG scoring.", "PPI utilizes a small set of human annotated datapoints for computing its confidence intervals; we designate this annotated set as our human preference validation set."], "ground_truth_answer": "PPI通过利用一小部分人工标注数据点来提高基于模型的评估准确性，为分数提供置信区间，结合标注和未标注预测。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES在评估过程中通常需要多少人工标注？", "ground_truth_contexts": ["ARES significantly improves data efficiency during evaluation by only requiring three inputs: an in-domain passage set, a human preference validation set of approximately 150 annotated datapoints or more, and few-shot examples of in-domain queries and answers (e.g. five examples or more)."], "ground_truth_answer": "ARES需要大约150个或更多人工标注数据点在人类偏好验证集中。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES中使用哪些模型进行合成数据生成和判断器微调？", "ground_truth_contexts": ["For generating our synthetic datasets, we use FLAN-T5 XXL (Chung et al., 2022). We selected DeBERTa-v3-Large (He et al., 2021) for our fine-tuned LLM judge."], "ground_truth_answer": "FLAN-T5 XXL用于合成数据生成，DeBERTa-v3-Large用于微调LLM判断器。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "总结ARES管道的三个阶段。", "ground_truth_contexts": ["ARES proceeds in three stages (Figure 1). ... First, it leverages an LM to construct a synthetic dataset of question–answer pairs, derived from the passages in the corpus. Second, it defines three separate judge models to perform three classification tasks... Third, ARES scores the different RAG systems being assessed using prediction-powered inference (PPI)."], "ground_truth_answer": "阶段1：从语料段落生成合成查询和答案。阶段2：为上下文相关性、答案忠实度和答案相关性微调LLM判断器。阶段3：使用判断器和PPI为RAG系统评分以获得置信区间。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "比较ARES和RAGAS在上下文相关性和答案相关性方面的性能。", "ground_truth_contexts": ["ARES averages a Kendall’s τ 0.065 higher for context relevance and 0.132 higher for answer relevance than RAGAS. ... For context relevance, ARES with a fine-tuned LLM-judge is 59.9 percentage points higher than RAGAS while for answer relevance, our system is 14.4 percentage points higher than RAGAS."], "ground_truth_answer": "ARES在Kendall’s τ上优于RAGAS（上下文相关性0.065，答案相关性0.132），并在准确性上更高（上下文相关性59.9%点，答案相关性14.4%点）。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "用于评估ARES的KILT和SuperGLUE数据集有哪些？", "ground_truth_contexts": ["From KILT (Petroni et al., 2021), we use Natural Questions (NQ), HotpotQA, FEVER, and Wizards of Wikipedia (WoW)... From SuperGLUE (Wang et al., 2019), we use MultiRC and ReCoRD."], "ground_truth_answer": "KILT数据集：Natural Questions (NQ)、HotpotQA、FEVER、Wizards of Wikipedia (WoW)。SuperGLUE数据集：MultiRC和ReCoRD。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES如何处理合成数据生成中的负面示例？", "ground_truth_contexts": ["To generate negatives for fine-tuning our LLM judges, we rely on two novel strategies... 1. Weak Negative Generation: ... 2. Strong Negative Generation: ..."], "ground_truth_answer": "ARES使用弱负面（随机采样）和强负面（从相似段落采样或生成矛盾答案）用于上下文相关性、答案忠实度和答案相关性。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES实验中使用的置信区间概率是多少？", "ground_truth_contexts": ["Additionally, PPI allows us to estimate confidence intervals with a selected level of probability; for our experiments, we use a standard 95% alpha (probability) for our confidence interval."], "ground_truth_answer": "ARES在其实验中使用95%的置信区间概率。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "分析ARES在跨域应用中的优势。", "ground_truth_contexts": ["In Table 6, we found that the fine-tuned LLM judges used in ARES proved successful in cross-domain applications. Across all settings, we found that LLM judges in ARES had strong generalizability... PPI helped mitigate the loss in accuracy and still allow ARES to be successful."], "ground_truth_answer": "ARES判断器在KILT和SuperGLUE的查询和文档类型上显示出强大的泛化能力，PPI缓解准确性损失，但在语言、代码或实体提取等剧烈转变中失败。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "论文中提到的ARES的局限性是什么？", "ground_truth_contexts": ["ARES relies on a small set of annotations... These annotations often require an annotator familiar with the RAG system’s domain... The LLMs used in ARES benefit substantially from GPU-based hardware... Additionally, all of the datasets used in our evaluation of ARES are in English..."], "ground_truth_answer": "局限性包括需要领域专家标注、需要GPU硬件，并且仅在英文数据集上评估。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES在AIS基准上的答案忠实度表现如何？", "ground_truth_contexts": ["Table 2 summarizes our AIS results. We found that ARES can effectively score the AIS datasets, getting within 2.5 accuracy points of the correct scores."], "ground_truth_answer": "ARES在AIS基准上对答案幻觉的预测在真实平均值的2.5个百分点以内。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "真实RAG系统评估中测试了哪些检索器和生成LLM？", "ground_truth_contexts": ["For our RAG systems, we selected three different retrievers (BM25, OpenAI Ada embeddings with cosine similarity search, and ColBERTv2... and three different generative LLMs (MPT-7b-Instruct, GPT-3.5-Turbo, and GPT-4)."], "ground_truth_answer": "检索器：BM25、OpenAI Ada嵌入、ColBERTv2。生成LLM：MPT-7b-Instruct、GPT-3.5-Turbo、GPT-4。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "总结ARES在伪RAG系统排名上的结果。", "ground_truth_contexts": ["Table 1: ... Overall, we found that ARES ranked RAG systems more accurately than RAGAS and GPT-3.5 across all the explored datasets."], "ground_truth_answer": "ARES在Kendall’s τ上高于RAGAS（上下文相关性0.065，答案相关性0.132），并在使用78%更少标注的情况下优于采样标注。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES中有效PPI所需的最少人工标注数量是多少？", "ground_truth_contexts": ["We used ARES with human annotation sets ranging in size from 25 to 400 and found that 150 is the minimum number required (Table 3)."], "ground_truth_answer": "ARES中有效PPI至少需要150个人工标注。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "ARES是否支持非英语语言的评估？", "ground_truth_contexts": ["While LLM judges in ARES were successful in cross-domain applications for KILT and SuperGLUE, LLM judges are unable to generalize when making more drastic shifts in domain, such as: switching languages...", "To test cross-lingual transfer, we used the XGLUE datasets... a LLM judge fine-tuned on NQ achieved a Kendall’s tau of 0.33..."], "ground_truth_answer": "不，ARES无法很好地泛化到非英语语言，在XGLUE上Kendall’s τ较低（例如0.33）。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
{"question": "SuperGrok订阅的价格是多少？", "ground_truth_contexts": [], "ground_truth_answer": "文档中没有此信息。", "retrieved_contexts": [], "generated_answer": "根据提供的资料无法回答。"}
